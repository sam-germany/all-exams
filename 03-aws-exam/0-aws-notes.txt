https://tutorialsdojo.com/           <-- all AWS notes are here


==================================================================================================================
IAM Policy   ( Identity and Access Management policies)
---------

AWS Organizations Service Control Policy (SCP) – 
----------------------------------------------
Use an AWS Organizations Service Control Policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). 
SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.

Permissions boundary - 
-------------------
Permissions boundary is a managed policy that is used for an IAM entity (user or role). The policy defines the maximum permissions that the 
identity-based policies can grant to an entity, but does not grant permissions.


Identity-based policies – 
----------------------
Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity.

Resource-based policies – 
------------------------
Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. 
Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource 
or in other accounts.

Permissions boundaries – 
----------------------
Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based 
policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based
policy can grant to an entity.

Organizations SCPs – 
------------------
Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). 
SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.

Access control lists (ACLs) – 
----------------------------
Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, 
although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant 
permissions to the specified principal. ACLs cannot grant permissions to entities within the same account.

Session policies – 
-----------------
Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that 
the role or user's identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. 
For more information, see Session Policies.


Trust policy - 
--------------
Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role 
is both an identity and a resource that supports resource-based policies. For this reason, you must attach both a trust policy 
and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust
 policy, which is attached to an IAM role.


IAM policy variables
--------------------
Instead of creating individual policies for each user, you can use policy variables and create a single policy that applies to multiple 
users (a group policy). Policy variables act as placeholders. When you make a request to AWS, the placeholder is replaced by a 
alue from the request when the policy is evaluated.

As an example, the following policy gives each of the users in the group full programmatic access to a user-specific object
 (their own "home directory") in Amazon S3.

IAM policy principal - 
---------------------
You can use the Principal element in a policy to specify the principal that is allowed or denied access to a resource 
(In IAM, a principal is a person or application that can make a request for an action or operation on an AWS resource. 
The principal is authenticated as the AWS account root user or an IAM entity to make requests to AWS). You cannot use
 the Principal element in an IAM identity-based policy. You can use it in the trust policies for IAM roles and in resource-based policies.

IAM policy condition - 
--------------------
The Condition element (or Condition block) lets you specify conditions for when a policy is in effect, 
like so - "Condition" : { "StringEquals" : { "aws:username" : "johndoe" }}. This can not be used to address the requirements 
of the given use-case.

Access Advisor feature on IAM console- 
---------------------------------------
To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. 
Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the 
security posture of your AWS environments. Additionally, by removing unused roles, you can simplify your monitoring and auditing
 efforts by focusing only on roles that are in use.


AWS Trusted Advisor - 
---------------------
AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following 
AWS best practices on cost optimization, security, fault tolerance, service limits, and performance improvement.

IAM Access Analyzer - 
---------------------
AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets 
or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, 
which is a security risk.

Amazon Inspector - 
------------------
Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications 
deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from 
best practices.

----------------------------------------------------------------------------------------------------------------
IAM Role
-------
we can create four types of ROLES

1) AWS service              (for EC2, Lambda)
2) Another AWS account      (another account can be belonging to you, or a 3rd party)
3) WEB Identity             (Cognito or any OpenID provider)
4) SAML 2.0 federation      (Your Corperate directory)   


Note: to understand  IAM role, user, policies just check "Niels" AWS associate course video 12, everything is clearly explained



-----------------------------------------------------------------------------------------------------------
AWS Certificate Manager - 
-------------------------
AWS Certificate Manager (ACM) is the preferred tool to provision, manage, and deploy server certificates. With ACM you can request a 
certificate or deploy an existing ACM or external certificate to AWS resources. Certificates provided by ACM are free and 
automatically renew. In a supported Region, you can use ACM to manage server certificates from the console or programmatically.

-------------------------------------------------------------------------------------------------------------
IAM
---
IAM is used as a certificate manager only when you must support HTTPS connections in a Region that is not supported by ACM. 
IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage. IAM supports deploying 
server certificates in all Regions, but you must obtain your certificate from an external provider for use with AWS. 
You cannot upload an ACM certificate to IAM. Additionally, you cannot manage your certificates from the IAM Console.

-------------------------------------------------------------------------------------------------------------
AWS Secrets Manager - 
-------------------
AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables 
you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. 
Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive 
information in plain text. It cannot be used to discover and protect your sensitive data in AWS.

-------------------------------------------------------------------------------------------------------------
AWS Systems Manager - 
------------------
AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so 
you can view operational data from multiple AWS services and allows you to automate operational tasks such as running commands, 
managing patches, and configuring servers across AWS Cloud as well as on-premises infrastructure.

==========================================================================================================================
AWS CloudFormation  ( cloud formation ) - 
==================

Supported "Parameters" used in the CloudFromation Template:
------------------------------------------------------------
String                            –  A literal string
Number                            –  An integer or float
List<Number>                      –  An array of integers or floats
CommaDelimitedList                –  An array of literal strings that are separated by commas
AWS::EC2::KeyPair::KeyName        –  An Amazon EC2 key pair name
AWS::EC2::SecurityGroup::Id       –  A security group ID
AWS::EC2::Subnet::Id              –  A subnet ID
AWS::EC2::VPC::Id                 –  A VPC ID
List<AWS::EC2::VPC::Id>           –  An array of VPC IDs
List<AWS::EC2::SecurityGroup::Id> –  An array of security group IDs
List<AWS::EC2::Subnet::Id>        –  An array of subnet IDs




AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, 
all the resources needed for your applications across all Regions and accounts. Think infrastructure as code; think CloudFormation. 
You cannot use CloudFormation for running commands or managing patches on servers.

"Resources" section in CloudFormation-Template is the only required section and specifies the stack resources and their properties, 
such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in 
the Resources and Outputs sections of the template.

'Parameters' section of the template - This optional section is helpful in passing Values to your template at runtime 
(when you create or update a stack). You can refer to parameters from the Resources and Outputs sections of the template.


Use Drift Detection feature of CloudFormation
---------------------------------------------
Drift detection enables you to detect whether a stack's actual configuration differs, or has drifted, from its expected configuration. 
Use CloudFormation to detect drift on an entire stack, or individual resources within the stack. A resource is considered to have 
drifted if any of its actual property values differ from the expected property values. This includes if the property or resource has 
been deleted. A stack is considered to have drifted if one or more of its resources have drifted.

To determine whether a resource has drifted, CloudFormation determines the expected resource property values, as defined in 
the stack template and any values specified as template parameters. CloudFormation then compares those expected values with the
 actual values of those resource properties as they currently exist in the stack. A resource is considered to have drifted if one 
 or more of its properties have been deleted, or had their value changed.

You can then take corrective action so that your stack resources are again in sync with their definitions in the stack template, 
such as updating the drifted resources directly so that they agree with their template definition. Resolving drift helps to ensure 
configuration consistency and successful stack operations.

Use Change Sets feature of CloudFormation - 
------------------------------------------
When you need to update a stack, understanding how your changes will affect running resources before you implement them can help 
you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running 
resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes 
to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed 
changes or explore other changes by creating another change set. Change sets are not useful for the given use-case.







====================================================================================================================
AWS CodePipeline - ( CI <- it is like a "Continous Integration")
-----------------
AWS CodePipeline is a fully managed "continuous delivery" service that helps you automate your release pipelines 
for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, 
and deploy phases of your release process every time there is a code change, based on the release model 
you define. This enables you to rapidly and reliably deliver features and updates. Whereas CodeDeploy is a 
deployment service, CodePipeline is a continuous delivery service. For our current scenario, CodeDeploy is 
the correct choice.

AWS CodeDeploy - (CD  <-- it is like "Continous Deployment")
---------------
AWS CodeDeploy is a fully managed "deployment" service that automates software deployments to a variety 
of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. 
AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during 
application deployment, and handles the complexity of updating your applications. This is the right 
choice for the current use case.

--------------------------------------------------------------------------------------------------
AWS X-Ray
--------
X-Ray sampling
--------------
By customizing sampling rules, you can control the amount of data that you record, and modify sampling behavior on the fly without modifying 
or redeploying your code. Sampling rules tell the X-Ray SDK how many requests to record for a set of criteria. X-Ray SDK applies a sampling 
algorithm to determine which requests get traced however because our application is failing to send data to X-Ray it does not help in 
determining the cause of failure.

EC2 X-Ray Daemon - 
-----------------
The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API 
The daemon logs could help with figuring out the problem.

EC2 Instance Role - 
-----------------
The X-Ray daemon uses the AWS SDK to upload trace data to X-Ray, and it needs AWS credentials with permission to do that. On Amazon EC2, 
the daemon uses the instance's instance profile role automatically. Eliminates API permission issues (in case the role doesn't have IAM permissions
 to write data to the X-Ray service)

CloudTrail - 
------------
With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. 
You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides event history
 of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. 
 You can check CloudTrail to see if any API call is being denied on X-Ray.

Config:
-------
Config - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, 
you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, 
and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer 
questions such as - “What did my AWS resource look like at xyz point in time?”. Config cannot help determine the source for KMS API calls.

======================================================================================================================
Key Management Store (KMS)
-------------------------
KMS stores the CMK, and receives data from the clients, which it encrypts and sends back

A customer master key (CMK) is a logical representation of a master key. The CMK includes metadata, such as the key ID, creation date, 
description, and key state. The CMK also contains the key material used to encrypt and decrypt data. You can generate CMKs in KMS, 
in an AWS CloudHSM cluster, or import them from your key management infrastructure.

AWS KMS supports symmetric and asymmetric CMKs. A symmetric CMK represents a 256-bit key that is used for encryption and decryption. 
An asymmetric CMK represents an RSA key pair that is used for encryption and decryption or signing and verification (but not both), 
or an elliptic curve (ECC) key pair that is used for signing and verification.


AWS KMS supports three types of CMKs: 
--------------------------------------
customer-managed CMKs, AWS managed CMKs, and AWS owned CMKs.

Server-Side Encryption – 
-----------------------
Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you 
download the objects.

Client-Side Encryption – 
----------------------
Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, 
the encryption keys, and related tools.
When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed CMK, or you can specify 
a customer-managed CMK that you have already created.
Creating your own customer-managed CMK gives you more flexibility and control over the CMK. For example, you can create,
rotate, and disable customer-managed CMKs. You can also define access controls and audit the customer-managed CMKs 
that you use to protect your data.

Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) -
------------------------------------------------------------ 
When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. 
As an additional safeguard, AWS encrypts the key itself with a master key that it regularly rotates. So this option 
is incorrect for the given use-case.

Server-Side Encryption with Customer-Provided Keys (SSE-C) - 
---------------------------------------------------------
With Server-Side Encryption with Customer-Provided Keys (SSE-C), you will need to create the encryption keys as well 
as manage the corresponding process to rotate and remove the encryption keys. Amazon S3 manages the data encryption, 
as it writes to disks, as well as the data decryption when you access your objects. So this option is incorrect for 
the given use-case.

Server-Side Encryption with Secrets Manager - 
-------------------------------------------
AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. 
The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets 
throughout their lifecycle. You cannot combine Server-Side Encryption with Secrets Manager to create, rotate, 
or disable the encryption keys.
===================================================================================================================================
Elastic BeanStack:
----------------
https://www.youtube.com/watch?v=yjHmzo91Rek


AWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. 
Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity 
provisioning, load balancing, auto-scaling, and application health monitoring.

This platform-as-a-service solution is typically for those who want to deploy and manage their applications within 
minutes in the AWS Cloud without worrying about the underlying infrastructure.

AWS Elastic Beanstalk supports the following languages and development stacks:
Apache Tomcat for Java applications
Apache HTTP Server for PHP applications
Apache HTTP Server for Python applications
Nginx or Apache HTTP Server for Node.js applications
Passenger or Puma for Ruby applications
Microsoft IIS for .NET applications
Java SE
Docker
Go

Elastic Beanstalk also supports deployment versioning. It maintains a copy of older deployments so that it is easy for 
the developer to rollback any changes made on the application.


------------
It help us to deploy the application on EC2 instances or deploy other resources.
Their are 4 types of "deployment-policy" types.


"All at once" deployment policy - 
------------
Note: Assume we have 4 instances of an application, it try to updated all the 4 at once with the new code through Elastic-beanstalk, so at the time of updation the application may not 
be able to get requests. so we have downtime

This is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. 
With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might 
need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.


"Rolling" deployment policy - 
----------------------------
Note: Assume we have ASG with 4 instances of an application, it takes two instances and update them with the new git code through Elastic beanstalk, 
once thes two are healthy then it goes to next two and update them also

With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. 
Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service.
The use case states that the application has high traffic and high availability requirements, so full capacity must be maintained during deployments, 
hence rolling with additional batch deployment is a better fit than the rolling deployment.

"Rolling with additional batch" deployment policy - 
------------------------------
Note: Assume we have ASG with 4 instances of an Application, then in the policy AWS create two new instances and update with the new version of code 
that we want to deploy, once these two new instances are healthy then it shut down the two instances from old 4 instances and the process goes like this furhter

With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, 
and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a
cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same 
bandwidth throughout the deployment.


"Immutable" deployment policy - 
-----------
Note: Assume we have ASG with 4 instances of an Application, then in the policy AWS create a new seperate temporary ASG and run 4 new instances and put the new 
uploaded code on it, if the new instances run good then it take these new instances for the new ASG and put it to the old ASG and replace the old instances
with the new instances.

A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. 
It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an 
immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version 
serves traffic alongside the old version until the new instances pass health checks.




===========================================================================================================================
Network ACL: (Active controll list)
-------------
It acts as a  "Stateless firewall", it means we define the incoming rule for src-dest  (port and ip)  and also we have to define the returning traffic
src-dest  (port and ip) 
we can place it on the Subnet-level, through this "list" we can define which traffic can goes inside-outside from the "Subnet"


Security Group:
--------------
It acts as a  "Stateful firewall", it means we define the incoming rule for src-dest  (port and ip)  and the returing traffic will go automatically to src port. 
We can place a Security-group  on a "Instance-level", with this we can define which traffic can go inside-outside the instance


----------------------------------------------------------------------------------------------------------
Amazon EFS volumes
------------------
EFS volumes provide a simple, scalable, and persistent file storage for use with your Amazon ECS tasks. With Amazon EFS, storage capacity is elastic, 
growing and shrinking automatically as you add and remove files. Your applications can have the storage they need, when they need it. 
Amazon EFS volumes are supported for tasks hosted on "Fargate" or "Amazon EC2 instances".

You can use Amazon EFS file systems with Amazon ECS to export file system data across your fleet of container instances. 
That way, your tasks have access to the same persistent storage, no matter the instance on which they land. However, you must configure your 
container instance AMI to mount the Amazon EFS file system before the Docker daemon starts. Also, your task definitions must reference 
volume mounts on the container instance to use the file system.

-----------------------------------------------------------------------------
AWS CodeDeploy ( Code Deploy)
----------------------------
AWS CodeDeploy is a service that coordinates application deployments across EC2 instances and instances running on-premises. 
It makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity 
of updating your applications.

Unlike Elastic Beanstalk, CodeDeploy does not automatically handle capacity provisioning, scaling, and monitoring.

Unlike CloudFormation and OpsWorks, CodeDeploy does not deal with infrastructure configuration and orchestration.

AWS CodeDeploy is a building block service focused on helping developers deploy and update software on any instance, 
including EC2 instances and instances running on-premises. AWS Elastic Beanstalk and AWS OpsWorks are end-to-end application management solutions.

You create a deployment configuration file to specify how deployments proceed.

CodeDeploy complements CloudFormation well when deploying code to infrastructure that is provisioned and managed with CloudFormation. 


Lifecycle Event Hook Availability:
---------------------------------
https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order

ValidateService: 
---------------
ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.


AfterInstall - 
-------------
You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions


ApplicationStart - 
------------------
You typically use this deployment lifecycle event to restart services that were stopped during ApplicationStop


AllowTraffic - 
-------------
During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. 
This event is reserved for the AWS CodeDeploy agent and cannot be used to run scripts

---------------------------------------------------------------------------------
AWS OpsWorks
------------
AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. OpsWorks lets you use Chef and Puppet to 
automate how servers are configured, deployed, and managed across your EC2 instances or on-premises compute environments.

OpsWorks offers three services:

Chef Automate
Puppet Enterprise
OpsWorks Stacks

OpsWorks for Puppet Enterprise lets you use Puppet to automate how nodes are configured, deployed, and managed, whether they 
are EC2 instances or on-premises devices.

OpsWorks for Chef Automate lets you create AWS-managed Chef servers, and use the Chef DK and other Chef tooling to manage them.

OpsWorks Stacks lets you create stacks that help you manage cloud resources in specialized groups called layers. A layer represents a set of EC2 
instances that serve a particular purpose. Layers depend on Chef recipes to handle tasks such as installing packages on instances, deploying apps, 
and running scripts.

Compared to CloudFormation, OpsWorks focuses more on orchestration and software configuration, and less on what and how AWS resources are procured.

------------------------------------------------------------------------------------------------------------------------
SQS Queue
=========
Standard queues 
---------------
It offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that 
messages are processed exactly once, in the exact order that they are sent.

Amazon SQS leverages the AWS cloud to dynamically scale, based on demand. SQS scales elastically with your application so you don't have 
to worry about capacity planning and pre-provisioning. For most standard queues (depending on queue traffic and message backlog), there can 
be a maximum of approximately 120,000 inflight messages

Delay queues 
------------
It let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application 
needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain 
invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. 
The maximum is 15 minutes.


Visibility timeout:   
------------------
-) Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. 

-) The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours               

-) The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using DelaySeconds attribute. 
MessageRetentionPeriod attribute controls the length of time, in seconds, for which Amazon SQS retains a message.               

-) Queue tags are case-sensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag. To be able to tag a queue 
on creation, you must have the sqs:CreateQueue and sqs:TagQueue permissions.

DeleteQueue - 
------------
Deletes the queue specified by the QueueUrl, regardless of the queue's contents. When you delete a queue, any messages in the 
queue are no longer available.

When you delete a queue, the deletion process takes up to 60 seconds. Requests you send involving that queue during the 60 seconds 
might succeed. For example, a SendMessage request might succeed, but after 60 seconds the queue and the message you sent no longer exist.

When you delete a queue, you must wait at least 60 seconds before creating a queue with the same name.


PurgeQueue - 
------------
Deletes the messages in a queue specified by the QueueURL parameter. When you use the PurgeQueue action, you can't retrieve
any messages deleted from a queue. The queue however remains.

RemovePermission - 
---------------------
Revokes any permissions in the queue policy that matches the specified Label parameter.

Increase the VisibilityTimeout -
-------------------------------
When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically 
delete the message. Immediately after a message is received, it remains in the queue. To prevent other consumers from processing 
the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from 
receiving and processing the message.

Reduce the VisibilityTimeout - 
-----------------------------
As explained above, VisibilityTimeout makes sure that the message is not read by any other consumer while it is being processed by one consumer. 


Short Polling:
-------------
Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, 
Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects 
at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only 
if the polling wait time expires.

Long Polling:
------------
Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps 
reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) 
and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action 
is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.








=====================================================================================================================================
AWS Lambda
----------
Lambda allocates CPU power in proportion to the amount of memory configured. Memory is the amount of memory available to your Lambda function at runtime. 
You can increase or decrease the memory and CPU power allocated to your function using the Memory (MB) setting. To configure the memory for your function, 
set a value between 128 MB and 10,240 MB in 1-MB increments. At 1,769 MB, a function has the equivalent of one vCPU (one vCPU-second of credits per second).


Lambda function as container image:
----------------------------------
You can deploy Lambda function as container image with the maximum size of 10GB.

Lambda Reserved/Provisioned concurrency:
---------------------------------------
Use provisioned concurrency to account for the compute-heavy workflows

Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of 
it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request 
is still being processed, another instance is allocated, which increases the function's concurrency.




--------------------------------------------------------------------------------------------------------------
AWS Serverless Application Model (AWS SAM)
================================
https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html

SAM supports the following resource types:

1) AWS::Serverless::Api
2) AWS::Serverless::Application
3) AWS::Serverless::Function
4) AWS::Serverless::HttpApi
5) AWS::Serverless::LayerVersion
6) AWS::Serverless::SimpleTable
7) AWS::Serverless::StateMachine




AWS::Serverless::Function - 
--------------------------
This resource creates a Lambda function, IAM execution role, and event source mappings that trigger the function.

AWS::Serverless::Api - 
--------------------
This creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints. 
It is useful for advanced use cases where you want full control and flexibility when you configure your APIs.

AWS::Serverless::SimpleTable - 
---------------------------
This creates a DynamoDB table with a single attribute primary key. It is useful when data only needs to be accessed via a primary key.




SAM Template:
-------------
Transform: AWS::Serverless-2016-10-31   ( "Transform" and "Resources" this is the required fields)

Globals:     set of globals
Description: String
Metadata:    template metadata
Parameters:  set of parameters
Mappings:    set of mappings
Conditions:  set of conditions
Resources:   set of resources
Outputs:     set of outputs


====================================================================================================================
Kinesis
=======

Amazon Kinesis Data Streams 
---------------------------
It is useful for rapidly moving data off data producers and then continuously processing the data, be it to transform the data 
before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing. 
Kinesis data streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website 
clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.

-) Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams - 
Kinesis Agent works with data producers. 

-) Use Kinesis enhanced fan-out for Kinesis Data Streams - You should use enhanced fan-out if you have, or expect to have, 
multiple consumers retrieving data from a stream in parallel. 



Amazon Kinesis Data Firehose 
----------------------------
It is the easiest way to load streaming data into data stores and analytics tools. Kinesis Firehose cannot be used to process 
and analyze the streaming data in custom applications. It can capture, transform, and load streaming data into Amazon S3, 
Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics.

Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as 
Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk. 
With Kinesis Data Firehose, you don't need to write applications or manage resources. You configure your data producers 
to send data to Kinesis Data Firehose, and it automatically delivers the data to the destination that you specified.


Kinesis Data Streams:
----------------------
Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability 
to read and/or replay records in the same order to multiple Amazon Kinesis Applications. 
Kinesis data streams is highly customizable and best suited for developers building custom applications or streaming data for 
specialized needs. Data Streams also provide greater flexibility in integrating downstream applications than Firehose. 
Data Streams is also a cost-effective option compared to Firehose



Kinesis Client Libraray:
------------------------
The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, 
making it easier to build multiple applications reading from the same Amazon Kinesis data stream 
(for example, to perform counting, aggregation, and filtering).

===========================================================================================================================
RDS
===

Use cross-Region Read Replicas:
------------------------------
In addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your 
production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also 
be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if 
you experience a regional availability issue.

Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region
----------------------------------------------------------------------------------------------------------------
Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several 
different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances 
use Amazon's failover technology.

The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will backup 
your database and transaction logs and store both for a user-specified retention period. If it’s a Multi-AZ configuration, 
backups occur on the standby to reduce I/O impact on the primary. Automated backups are limited to a single AWS Region while
manual snapshots and Read Replicas are supported across multiple Regions.

-) Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and 
   consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a 
   disaster recovery option.



=======================================================================================================================
Aurora DB
=========






=========================================================================================================================
CloudWatch   vs   CloudTrail    vs    Config
----------        ----------          ------

CloudWatch:      <--  Think resource performance monitoring, events, and alerts; think CloudWatch.

CloudTrail:      <--  Think account-specific activity and audit; think CloudTrail.

Config:          <--  Think resource-specific history, audit, and compliance; think Config.


===========================================================================================================================
CloudWatch  (cloud watch)
==========

EC2 Cloudwatch metric
---------------------
You can create a custom CloudWatch metric for your EC2 Linux instance statistics by creating a script through the AWS Command Line 
Interface (AWS CLI). Then, you can monitor that metric by pushing it to CloudWatch.


Standard Resoultuion Metric:
---------------------------



High Resolution Metric:
----------------------
You can publish your own metrics to CloudWatch using the AWS CLI or an API. Metrics produced by AWS services are standard resolution 
by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a 
high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 
5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.

High-resolution metrics can give you more immediate insight into your application's sub-minute activity. But, every PutMetricData call 
for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges.


EC2 "detailed monitoring:
-------------------------
By enabling detailed monitoring you define the frequency at which the metric data has to be sent to CloudWatch, from 5 minutes to 1-minute 
frequency window. But, you still need to create and collect the custom metric you wish to track.



========================================================================================================================
EC2
===

Dedicated Instances - 
---------------------
Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated 
to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, 
even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances 
from the same AWS account that are not Dedicated Instances.

Dedicated Host
--------------
A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and 
control over how instances are placed on the server.

Spot Instances - 
--------------
A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Your Spot Instance runs 
whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present 
with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware 
requirement of the client and hence is not the correct option.


Dedicated Hosts - 
--------------
An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts 
allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control 
over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right 
choice for the current requirement.


On-Demand Instances -
--------------------
With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over 
its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible 
and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.

EC2 Price:
=========

Reserved Instances
-----------------
Reserved Instances offer significant savings on Amazon EC2 costs compared to On-Demand Instance pricing. A Reserved Instance 
can be purchased for a one-year or three-year commitment, with the three-year commitment offering a bigger discount. Reserved 
instances come with two offering classes - Standard or Convertible.

Convertible Reserved instances - 
------------------------------
A Convertible Reserved Instance can be exchanged during the term for another Convertible Reserved Instance with new attributes 
including instance family, instance type, platform, scope, or tenancy. This is the best fit for the current requirement.


Standard Reserved instances - 
----------------------------
With Standard Reserved Instances, some attributes, such as instance size, can be modified during the term; however, 
the instance family cannot be modified. You cannot exchange a Standard Reserved Instance, only modify it


Scheduled Reserved instances - 
------------------------------
Scheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, 
weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in 
advance so that you know it is available when you need it.




gp2 Volume:
----------
The performance of gp2 volumes is tied to volume size, which determines the baseline performance level of the volume 
and how quickly it accumulates I/O credits; larger volumes have higher baseline performance levels and accumulate 
I/O credits faster.

5.3 TiB - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. 
These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of 
time. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), 
baseline performance scales linearly at 3 IOPS per GiB of volume size.


===================================================================================================================
Secret Manager
--------------
AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets 
throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the 
need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration 
for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.

==============================================================================================================
IAM
===

IAM Access Analyzer - 
-------------------
AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, 
that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.
You can set the scope for the analyzer to an organization or an AWS account. This is your zone of trust. The analyzer scans 
all of the supported resources within your zone of trust. When Access Analyzer finds a policy that allows access to a resource 
from outside of your zone of trust, it generates an active finding.


Access Advisor feature on IAM console - 
------------------------------------
To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. 
Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the 
security posture of your AWS environments. This does not provide information about non-IAM entities such as S3, hence it's not a 
correct choice here

===========================================================================================================
Cognito
=======

"Cognito User Pools"
-------------------
After successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users 
access to your own server-side resources, or to the Amazon API Gateway.
Amazon Cognito user pools implement ID, access, and refresh tokens as defined by the OpenID Connect (OIDC) open standard.
The ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, 
and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate 
users against your resource servers or server applications.


"Cognito Identity Pools" -   e.g  Facebook, Google, twitter, ect... are "Identity Pools"
------------------------
You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain 
temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users,
as well as the specific identity providers that you can use to authenticate users for identity pools.

Public providers: 
Login with Amazon (Identity Pools), Facebook (Identity Pools), Google (Identity Pools), Sign in with Apple (Identity Pools).

Amazon Cognito User Pools
Open ID Connect Providers (Identity Pools)
SAML Identity Providers (Identity Pools)
Developer Authenticated Identities (Identity Pools


"Cognito Sync" - 
----------------
Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. 
You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend.
The client libraries cache data locally so your app can read and write data regardless of device connectivity status. 
When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately 
that an update is available.



===========================================================================================================
AWS AppSync - 
-------------
AWS AppSync is a fully managed service that makes it easy to develop GraphQL APIs by handling the heavy lifting of securely 
connecting to data sources like AWS DynamoDB, Lambda, and more. Organizations choose to build APIs with GraphQL because it 
helps them develop applications faster, by giving front-end developers the ability to query multiple databases, microservices, 
and APIs with a single GraphQL endpoint.

AWS Service Catalog - 
--------------------
AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS. 
These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier 
application architectures. AWS Service Catalog allows you to centrally manage deployed IT services and your applications, 
resources, and metadata. This helps you achieve consistent governance and meet your compliance requirements while enabling 
users to quickly deploy only the approved IT services they need.


==================================================================================================================
ECS   Elastic Container Service
--------------------------------
Use ECS service scheduler - 
--------------------------
Amazon ECS provides a service scheduler (for long-running tasks and applications), the ability to run tasks manually 
(for batch jobs or single run tasks), with Amazon ECS placing tasks on your cluster for you. You can specify task 
placement strategies and constraints that allow you to run tasks in the configuration you choose, such as spread 
out across Availability Zones. It is also possible to integrate with custom or third-party schedulers.

Use ECS step scaling policy - 
---------------------------
Although Amazon ECS Service Auto Scaling supports using Application Auto Scaling step scaling policies, 
AWS recommends using target tracking scaling policies instead. For example, if you want to scale your service when 
CPU utilization falls below or rises above a certain level, create a target tracking scaling policy based on the 
CPU utilization metric provided by Amazon ECS.

With step scaling policies, you create and manage the CloudWatch alarms that trigger the scaling process. 
If the target tracking alarms don't work for your use case, you can use step scaling. You can also use target 
tracking scaling with step scaling for an advanced scaling policy configuration. For example, you can configure 
a more aggressive response when utilization reaches a certain level.

Step Scaling scales your cluster on various lengths of steps based on different ranges of thresholds. 
Target tracking on the other hand intelligently picks the smart lengths needed for the given configuration.

==========================================================================================================
keys
----

SSH Keys - 
---------
Are locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.

AWS access keys - 
---------------
You can use these keys with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.

======================================================================================================
AWS Cli Commands
---------------
The AWS CLI has a few general options:

Variable                 Option 	       Config Entry 	        Environment Variable 	   Description

profile 	             --profile 	            N/A 	               AWS_PROFILE 	             Default profile name
region 	                 --region 	           region 	               AWS_DEFAULT_REGION 	     Default AWS Region
output 	                 --output 	           output 	               AWS_DEFAULT_OUTPUT 	     Default output style
cli_timestamp_format 	    N/A 	        cli_timestamp_format 	         N/A 	             Output format of timestamps
cli_follow_urlparam 	    N/A 	        cli_follow_urlparam 	         N/A 	             Fetch URL url parameters
ca_bundle 	             --ca-bundle 	       ca_bundle 	           AWS_CA_BUNDLE             CA Certificate Bundle
parameter_validation 	     N/A 	        parameter_validation 	         N/A 	             Toggles parameter validation
tcp_keepalive  	             N/A 	        tcp_keepalive 	                 N/A 	             Toggles TCP Keep-Alive
max_attempts  	             N/A 	         max_attempts 	           AWS_MAX_ATTEMPTS 	     Number of total requests
retry_mode                	 N/A              retry_mode 	            AWS_RETRY_MODE 	         Type of retries performed



> aws ec2 monitor-instances --instance-ids <instance-id>                     <-- this command will enable monitoring
> aws ec2 run-instances --image-id <image-id>  --monitoring Enabled=true    <--  This syntax is used to enable detailed monitoring 
                                                                                 when launching an instance from AWS CLI
> 


===============================================================================================================
VPC Flow Logs: 
----------------
VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network 
interfaces in your VPC. Flow log data is used to analyze network traces and helps with network security. Flow log data 
can be published to Amazon CloudWatch Logs or Amazon S3. You cannot use VPC Flow Logs to debug and trace data across accounts.

================================================================================================================
Cloud Front (CloudFront)
------------------------

Explanation of the question in test 2.3:  (https://www.youtube.com/watch?v=NTOCzsn7b4A)
----------------------------------------
In the video their he clearly explains how the flow works, After creating a new public-private key
we can attach the pulblic in the cloudfront and after that for creating the neu url we are using the private-key


Signed-URL  vs  Signed-Cookies
------------------------------
https://www.youtube.com/watch?v=JIW_pV3zau8

Signed-URL: 
-----------
It provides limited permission and time to make a request to a "Single File"

Signed-Cookies:
--------------
Note: in case of "Signed-Cookies" the user request for the access, for this we have to develop a application then 
that application will send a "Cookies-header" then that "Cookies-header" will sent back to the user, and when 
user send request for accessing the file then he has to send those cookies in the header, as per the Cookies
present in the request-header, the Cloudfrond authenticate the request and gie the access to the files

It is a cookie that provides limited permission and time to make requests for a set of files "Multiple Files"
e.g: If their are 50 files that has same "cookies header", then user will get access to all those 50-files 
     as those files has same "cookies-header"


Note: If our request url contains these parameters  expires, policy, signature, key-pair-id   then Cloud-Front
     will reject the request.




CloudFront's Field-Level Encryption:
-----------------------------------
CloudFront's Field-Level Encryption to help protect sensitive data - CloudFront's field-level encryption further 
encrypts sensitive data in an HTTPS form using field-specific encryption keys (which you supply) before a POST 
request is forwarded to your origin. This ensures that sensitive data can only be decrypted and viewed by certain 
components or services in your application stack. This feature is not useful for the given use case.




===============================================================================================================
Flow Logs
---------

-) We can create a "Flow logs" for a  "VPC" or a "Subnet" or a "Network Interface"

VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from 
network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. 
After you've created a flow log, you can retrieve and view its data in the chosen destination.

You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a 
subnet or VPC, each network interface in that subnet or VPC is monitored.

Flow log data for a monitored network interface is recorded as flow log records, which are log 
events consisting of fields that describe the traffic flow.

To create a flow log, you specify:
The resource for which to create the flow log
The type of traffic to capture (accepted traffic, rejected traffic, or all traffic)
The destinations to which you want to publish the flow log data

================================================================================================================
Step Function   
-------------
https://www.youtube.com/watch?v=ge6Dm2HYG1E     <-- In this video easily explained

Note:
A Task state ("Type": "Task") represents a single unit of work performed by a state machine.

All work in your state machine is done by tasks. A task performs work by using an activity or an 
AWS Lambda function, or by passing parameters to the API actions of other services.

AWS Step Functions can invoke Lambda functions directly from a task state. A Lambda function 
is a cloud-native task that runs on AWS Lambda. You can write Lambda functions in a variety of 
programming languages, using the AWS Management Console or by uploading code to Lambda.

=========================================================================================================
Amazon Simple Email Service:
---------------------------

Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again - 
A “Throttling – Maximum sending rate exceeded” error is retriable. This error is different than other errors returned 
by Amazon SES. A request rejected with a “Throttling” error can be retried at a later time and is likely to succeed.

Retries are “selfish.” In other words, when a client retries, it spends more of the server's time to get a higher 
chance of success. Where failures are rare or transient, that's not a problem. This is because the overall number
 of retried requests is small, and the tradeoff of increasing apparent availability works well. When failures are 
 caused by overload, retries that increase load can make matters significantly worse. They can even delay recovery 
 by keeping the load high long after the original issue is resolved.

The preferred solution is to use a backoff. Instead of retrying immediately and aggressively, the client waits some 
amount of time between tries. The most common pattern is an exponential backoff, where the wait time is increased 
exponentially after every attempt.

A variety of factors can affect your send rate, e.g. message size, network performance or Amazon SES availability. 
The advantage of the exponential backoff approach is that your application will self-tune and it will call Amazon 
SES at close to the maximum allowed rate.


Configure Timeout mechanism for each request made to the SES service - Requests are configured to timeout if they 
do not complete successfully in a given time. This helps free up the database, application and any other resource 
that could potentially keep on waiting to eventually succeed. But, if errors are caused by load, retries can be
 ineffective if all clients retry at the same time. Throttling error signifies that load is high on SES and it does 
 not make sense to keep retrying.

Raise a service request with Amazon to increase the throttling limit for the SES API - If throttling error is persistent, 
then it indicates a high load on the system consistently and increasing the throttling limit will be the right solution
 for the problem. But, the error is only intermittent here, signifying that decreasing the rate of requests will handle the error.

Implement retry mechanism for all 4xx errors to avoid throttling error - 4xx status codes indicate that there was a
problem with the client request. Common client request errors include providing invalid credentials and omitting 
required parameters. When you get a 4xx error, you need to correct the problem and resubmit a properly formed client 
request. Throttling is a server error and not a client error, hence retry on 4xx errors does not make sense here.


Note:
-----
Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months - 




=======================================================================================================================
ASG  Auto Scaling Group
========================

Target Tracking Scaling Policy:  (here it is clearly explaine how Auto-scaling works internally in AWS side)
-------------------------------
With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates 
and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the 
metric and the target value.

It is important to note that a target tracking scaling policy assumes that it should scale out your Auto Scaling group when 
the specified metric is above the target value. You cannot use a target tracking scaling policy to scale out your Auto Scaling 
group when the specified metric is below the target value.

========================================================================================================================
DynamoDB:
---------
With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing 
TransactWriteItems or TransactGetItems operation.

TransactWriteItems 
------------------
It is a synchronous and idempotent write operation that groups up to 25 write actions in a 
single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables 
within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. 
The actions are completed atomically so that either all of them succeed or none of them succeeds.

You can optionally include a client token when you make a TransactWriteItems call to ensure that the request is idempotent. 
Making your transactions idempotent helps prevent application errors if the same operation is submitted multiple times due to a 
connection time-out or other connectivity issue.

BatchWriteItem API 
------------------
With this we can update multiple tables simultaneously - A TransactWriteItems operation differs from a BatchWriteItem operation 
in that all the actions it contains must be completed successfully, or no changes are made at all. With a BatchWriteItem operation,
it is possible that only some of the actions in the batch succeed while the others do not.

DynamoDB backups
----------------
1)
Use the DynamoDB on-demand backup capability to write to Amazon S3 and download locally - 
This option is not feasible for the given use-case. DynamoDB has two built-in backup methods (On-demand, Point-in-time recovery) 
that write to Amazon S3, but you will not have access to the S3 buckets that are used for these backups.

2)
Use AWS Data Pipeline to export your table to an S3 bucket in the account of your choice and download locally - 
This is the easiest method. This method is used when you want to make a one-time backup using the lowest amount of AWS resources possible. 
Data Pipeline uses Amazon EMR to create the backup, and the scripting is done for you. You don't have to learn Apache Hive or 
Apache Spark to accomplish this task.

3)
Use Hive with Amazon EMR to export your data to an S3 bucket and download locally - Use Hive to export data to an S3 bucket. 
Or, use the open-source emr-dynamodb-connector to manage your own custom backup method in Spark or Hive. These methods are the 
best practice to use if you're an active Amazon EMR user and are comfortable with Hive or Spark. These methods offer more control 
than the Data Pipeline method.

4)
Use AWS Glue to copy your table to Amazon S3 and download locally - Use AWS Glue to copy your table to Amazon S3. This is the best 
practice to use if you want automated, continuous backups that you can also use in another service, such as Amazon Athena.


Eventually Consistent Reads:
-----------------------------
When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. 
The response might include some stale data. If you repeat your read request after a short time, the response should return 
the latest data.


Strongly Consistent Reads:
--------------------------
When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates 
from all prior write operations that were successful.

DynamoDB uses eventually consistent reads by default. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead
parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. As per the given 
use-case, to make sure that only the last updated value of any item is used in the application, you should use strongly 
consistent reads by setting ConsistentRead = true for GetItem operation.



==========================================================================================================================
EBS Volumes:
------------

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html           <-- current list of EBS values

-) 1 GB can have up-to 50-IOPS         <-- Note: 1-GB  can have 20 or 30 IOPS but maximum 50-IOPS  
 
200 * 50  = 10,000 IOPS        <-- it means 200-GB  10,000 IOPS


EBS volumes are by-default AZ locked:     (it means in which AZ you create and only be re-attach to another EC2 in the same AZ)
-------------------------------------
An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an 
instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached 
to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume 
type on live production volumes.

When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss due to the failure 
of any single hardware component. You can attach an EBS volume to an EC2 instance in the same Availability Zone.

==========================================================================================================================
EC2 Spot Instance
-----------------

It is always possible that Spot Instances might be interrupted. Therefore, you must ensure that your application 
is prepared for a Spot Instance interruption.

These three actions can be done when we are using a "Spot Instance" and it will be taken back from AWS

1) Stop the Spot Instance - This is a valid option. Amazon EC2 can be configured to stop the instance when an 
                             interruption occurs on Spot instances.

2) Hibernate the Spot Instance - This is a valid option. Amazon EC2 can be configured to hibernate the instance 
                             when an interruption occurs on Spot instances.

3) Terminate the Spot Instance - This is a valid option. Amazon EC2 can be configured to hibernate the instance 
                              when an interruption occurs on Spot instances. The default behavior is to terminate 
                              Spot Instances when they are interrupted.

===========================================================================================================================
RDS  Relation Database Service:
==============================

IAM Database Authentication:
----------------------------
You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. 
With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an 
authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request.
Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication 
is managed externally using IAM.

=======================================================================================================
S3 ( Simple storage service )
-----------------------------
Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies, 
bucket policies, Access Control Lists (ACLs), and Query String Authentication.

IAM policy for S3:
-------------------
IAM enables organizations with multiple employees to create and manage multiple users under a single AWS account. With IAM policies, 
customers can grant IAM users fine-grained control to their Amazon S3 bucket or objects while also retaining full control over 
everything the users do.



EC2 access S3:
--------------
IAM roles have been incorporated so that your applications can securely make API requests from your instances, without 
requiring you to manage the security credentials that the applications use. Instead of creating and distributing your 
AWS credentials, you can delegate permission to make API requests using IAM roles.

Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, 
the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.

This is the most secure option as the role assigned to EC2 can be used to access S3 without storing any credentials 
onto the EC2 instance.


Bucket Policies:
---------------
With bucket policies, customers can define rules which apply broadly across all requests to their Amazon S3 resources, such as 
granting write privileges to a subset of Amazon S3 resources. Customers can also restrict access based on an aspect of the request,
such as HTTP referrer and IP address.


ACL:
----
With ACLs, customers can grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users for an individual bucket or object.
Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to
it as a subresource. A bucket ACLs allow you to control access at bucket level.

Query String Authentication:
----------------------------
With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time. 
Using query parameters to authenticate requests is useful when you want to express a request entirely in a URL. This method is 
also referred as presigning a URL

S3 Cross-Origin Resource Sharing (CORS) - 
----------------------------------------
Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a 
different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access
to your Amazon S3 resources.

To configure your bucket to allow cross-origin requests, you create a CORS configuration. The CORS configuration is a document with 
rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support each origin, 
and other operation-specific information. You can add up to 100 rules to the configuration. You can add the CORS configuration as the 
cors subresource to the bucket.

If you are configuring CORS in the S3 console, you must use JSON to create a CORS configuration. The new S3 console only supports JSON CORS configurations.


S3 Transfer Acceleration - 
------------------------
Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. 
S3 Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location, 
data is routed to your Amazon S3 bucket over an optimized network path.

S3 Access Analyzer - 
-------------------
Access Analyzer for S3 is a feature that monitors your access policies, ensuring that the policies provide only the intended access to your S3 resources. 
Access Analyzer for S3 evaluates your bucket access policies and enables you to discover and swiftly remediate buckets with potentially unintended access.

Access Analyzer for S3 helps review all buckets that have bucket access control lists (ACLs), bucket policies, or access point policies that 
grant public or shared access. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or 
other AWS accounts, including AWS accounts outside of your organization.


S3 Access Points - 
-----------------
Amazon S3 Access Points simplifies managing data access at scale for applications using shared data sets on S3. With S3 Access Points, 
you can now easily create hundreds of access points per bucket, representing a new way of provisioning access to shared data sets. 
Access Points provide a customized path into a bucket, with a unique hostname and access policy that enforces the specific permissions 
and network controls for any request made through the access point.

S3 Object Ownership -  bucket-owner-full-control
------------------------------------------------
S3 Object Ownership is an Amazon S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets. 
By default, when other AWS accounts upload objects to your bucket, the objects remain owned by the uploading account. With S3 Object Ownership, 
any new objects that are written by other accounts with the bucket-owner-full-control canned access control list (ACL) automatically become 
owned by the bucket owner, who then has full control of the objects.

S3 Object Ownership has two settings: 1. Object writer – The uploading account will own the object. 2. Bucket owner preferred – 
The bucket owner will own the object if the object is uploaded with the bucket-owner-full-control canned ACL. Without this setting 
and canned ACL, the object is uploaded and remains owned by the uploading account.


Server side Encryption are of three types:     ( SSE-S3, SSE-KMS, SSE-C)
=========================================

1) SSE-S3: (SSE with AWS S3-Managed Keys )
-------------------------------------------
In this mode of SSE, AWS S3 manages and handles the encryption keys. It uses a unique key to encrypt each object on the server side using AES-256. 
The client doesn’t directly access the encryption key or use it to encrypt and decrypt your data manually. AWS S3 also encrypts that unique key 
using a root or master key, adding an extra layer of security. The root key is also rotated regularly, making it difficult for attackers to 
find the unique key. There is no additional charge for this encryption other than requests for default encryption configuration.

You can use a bucket policy to enforce this encryption. For instance, you can add a check if the data upload request header contains 
the x-and-server-side-encryption and denies the upload operation if it doesn’t have it.
However, you cannot enforce SSE-S3 encryption when you upload files using pre-signed URLs.

2) SSE-KMS: (SSE with AWS Key Managed Service)
---------------
In contrast to SSE-S3, SSE-KMS includes additional layers of security and charges. S3 uses the AWS Key Management Service (AWS KMS) keys 
for encrypting bucket objects. AWS KMS is a specifically designed service for managing encryption keys at scale in the cloud. Therefore, 
users benefit from the additional security and features provided by KMS.

SSE-KMS allows you to create keys centrally and define their usage through policies. Also, you can find out when and who used those keys 
to ensure the keys are being used properly. When using SSE-KMS keys with S3, both the bucket and the keys need to locate in the same region.

Furthermore, you need separate permissions to use SSE-KMS. For example, use the kms:GenerateDataKey permission to use KMS keys when uploading 
an object. If you need to download an object with SSE-KMS encryption, you must define kms: Decrypt as permission.

3) SSE-C  (SSE with Customer-Provided Keys)
-------------------------------------------
If you are not ready to hand over the encryption key management to AWS services but want to take that responsibility yourself, SSE-C is an 
ideal option. In SSE-C encryption, you manage the encryption keys on your own, while S3 manages the encryption and decryption processes. 
When uploading an object into your S3 bucket, S3 uses your encryption key to encrypt them.

You must provide the same key to download the objects. Then it checks if the customer-provided key matches the correct key. S3 will only 
proceed with the encryption if the provided key is valid. Once the encryption finishes, S3 removes it from its memory.


GenerateDataKey operation
=========================
Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt 
the data - GenerateDataKey API, generates a unique symmetric data key for client-side encryption. This operation returns a plaintext 
copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to 
encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.


GenerateDataKey returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK.


Steps to encrypt data outside of AWS KMS:
----------------------------------------
1) Use the GenerateDataKey operation to get a data key.
2) Use the plaintext data key (in the Plaintext field of the response) to encrypt your data outside of AWS KMS. Then erase the plaintext data key from memory.
3) Store the encrypted data key (in the CiphertextBlob field of the response) with the encrypted data.


Steps to decrypt data outside of AWS KMS:
-----------------------------------------
1) Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.
2) Use the plaintext data key to decrypt data outside of AWS KMS, then erase the plaintext data key from memory.



================================================================================================================================================
ALB  Application Load Balancer:
-------------------------------


Use the header X-Forwarded-For - 
------------------------------
The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. 
Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address 
of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing 
stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server

Sticky sessions are enabled for the load balancer - 
-------------------------------------------------
This can be the reason for potential unequal traffic routing by the load balancer. Sticky sessions are a mechanism to route requests to 
the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience 
to clients. To use sticky sessions, the clients must support cookies.

When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that 
encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client 
should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from 
a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, 
the load balancer detects the cookie and routes the request to the same target.

If you use duration-based session stickiness, configure an appropriate cookie expiration time for your specific use case. If you set 
session stickiness from individual applications, use session cookies instead of persistent cookies where possible.


Instances of a specific capacity type aren’t equally distributed across Availability Zones - 
-------------------------------------------------------------------------------------------
A Classic Load Balancer with HTTP or HTTPS listeners might route more traffic to higher-capacity instance types. This distribution aims to prevent 
lower-capacity instance types from having too many outstanding requests. It’s a best practice to use similar instance types and configurations to 
reduce the likelihood of capacity gaps and traffic imbalances.

A traffic imbalance might also occur if you have instances of similar capacities running on different Amazon Machine Images (AMIs). In this scenario, 
the imbalance of the traffic in favor of higher-capacity instance types is desirable.

Note:
-)  With Application Load Balancers, cross-zone load balancing is always enabled.


===============================================================================================================================================
Elasticache Redis:
-----------------

Implement Amazon ElastiCache Redis in Cluster-Mode - 
--------------------------------------------------
One can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change 
to your existing workload. Cluster mode comes with the primary benefit of horizontal scaling of your Redis cluster, with 
almost zero impact on the performance of the cluster.

When building production workloads, you should consider using a configuration with replication, unless you can easily 
recreate your data. Enabling Cluster-Mode provides a number of additional benefits in scaling your cluster. In short, 
it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node 
type (vertical scaling). This means that Cluster-Mode can scale to very large amounts of storage (potentially 100s of terabytes) 
across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.

Implement Amazon ElastiCache Memcached - 
--------------------------------------
Redis and Memcached are popular, open-source, in-memory data stores. Although they are both easy to use and offer high 
performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity
while Redis offers a rich set of features that make it effective for a wide range of use cases. Redis offers snapshots 
facility, replication, and supports transactions, which Memcached cannot and hence ElastiCache Redis is the right 
choice for our use case.

====================================================================================================
API Gateway:
-----------
Restrict access by using CORS - 
-------------------------------
Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a 
different domain. When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing 
these requests, you must disable cross-origin resource sharing (CORS) for selected methods on the resource.


Use Account-level throttling - 
-----------------------------
To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API. By default, API Gateway 
limits the steady-state request rate to 10,000 requests per second (rps). It limits the burst (that is, the maximum bucket size) to 5,000 
requests across all APIs within an AWS account. This is Account-level throttling. As you see, this is about limit on the number of 
requests and is not a suitable answer for the current scenario.

Use Mapping Templates - 
-----------------------
A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. 
Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be 
easy to ready. Mapping Templates have nothing to do with access and are not useful for the current scenario.

Assign a Security Group to your API Gateway - 
---------------------------------------------
API Gateway does not use security groups but uses resource policies, which are JSON policy documents that you attach to an API to 
control whether a specified principal (typically an IAM user or role) can invoke the API. You can restrict IP address using this, 
the downside being, an IP address can be changed by the accessing user. So, this is not an optimal solution for the current use case.