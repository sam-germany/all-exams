Questions
-----------------
pages open first 

Volumes 
env with secret 
env with configmap 
cronjob 
job 
ingress 
------------------------
Topics: Ingress, config, 
Labs: 66,70,80,102,114,11, 141  (164 till 170 ) (150- till - 163)

Labs: 141,152 

------------------------------------------------------------------
How we can enable "alpha" or " beta" versions for any Resources.
1) 
-) rabc.authorization.k8s.io    <-- enable "/v1alpha1" version 

> vi /etc/kubernetes/mainifests/kube-apiserver.yaml 
> under command:   - --runtime-config=rbac.authorization.k8s.io/v1alpha1              <-- like this we can update the config.


1-a) Api versions

> kube-apiserver.yaml      ->  command:   --runtime-config=batch/v2alpha1       <-- like this we can enable "batch/v2alpha1" version.

-------------------------------------------------------------
2) How to get the preferred version of a resource.

> k proxy 8001&        -> curl localhost:8001/apis/authorization.k8s.io       <-- "authoriazation.k8s.io  <-- like this we can find the 
                                                                                                              "preferredversion"


------------------------------
2) How to create a "tls" Secret   &  "docker-registry" secret 

> k create secret tls --key=<put-the-given-key>  --cret=<put-the-given-cret> -n my-namespace 


> k create secret docker-registry my-sec --docker-username=aa --docker-password=123 
                                         --docker-server=my-registry.com --docker-email=abc@gmail.com   <-- like this we can create a "Secret" that can be for "imagePullSecret" 
                                                                                                            in a pod with this we can pull the image 
                                                                                 
-----------------------------------------------------
In Docker ui - Find the base OS of the image "python:3.4" 

docker run python:3.6 cat /etc/*release*



--------------------------------------------
Pod as Root-User:

spec:
  securityContext:
    runAsUser: 0 

















Imp Points:
---------------------------------------------------------------
-)  Secrets are only "encoded" they are not "encrypted"
    Encoded:        <-- it means base64 encod-decode 
    Encrypted:      <-- it means we need public key to encrypt and private key to decrypt
    
--------------------------
Imp Points:

-) Kube-scheduler is responsible for scheduling the pod, it means it will decide on which Node the pod 
should be placed, if a "Pod" is in a pending state, then first check "kube-scheduler" pod is running or not in "kube-system" namespace 

-) Kube-Controller-Manager  is responsible for deployment, like how many replicaSet should be their, if after increasing the replica
  of a "Deployment" no new replica is created, then first check the "kube-controller-manager" pod in the "kube-system" namespace 


-) "kubectl api-resources"         <-- with this command we can get the calue name for  "--resource="  that we have to put in "Role" or "ClusterRole"

-) k get pods --kubeconfig /root/CKS/super.kubeconfig               <-- Normally by default kubernetes by default look for config file under  ".kube/config" file 
                                                                      but with the "--kubeconfig <file-path>" we can use a seperate configuration file also for testing.

> kubectl config get-contexts  > /config.txt       <-- to get all the contexts 

> kubectl config current-context                 <-- to get the current context 

> kubectl config use-context <context-name>       <-- Command to switch to a specific context 


-) k label pod my-pod env=prod --overwrite     <-- before their was a label "env=dev" and we want to change to "env=prod" then just use this "--overwrite" command

------------------------------------------------------------------------------------------
Tesing Pod:
----------

-) k run test-pod alpine/curl --rm -it  -- sh                 <-- like this we can create a test pod fastly just for executing curl commands, "-rm" it will remove the pod 
                                                                  after executin this command, or after exit.

-) k run pod --image=busybox -it --rm --restart=Never -- sh -c 'echo aaaaaa vvvv'              <-- Main point is when using "busybox" then must use "sh -c" or "/bin/sh -c"
                                                                                                "--restart-Never" we must use this command otherwise it will not work, and 
                                                                                                it struct in some never ending loop 

-) k run mypod --image=ubuntu:18.4 --command sleep 1000        <-- check the  difference between "--"  using it or not ? 

OR 

-) k run mypod --image=ubuntu:18.4 --command -- sleep 1000            

-) k run mypod --image=ubuntu:18.4 --dry-run=client --command -- sleep 1000 

-) k run tech-pod --image=busybox -it --rm --restart=Never -- /bin/sh -c 'echo goggggg'   <-- After executing this command in the output it should 
                                                                                            print the message and also a seperate message, 
                                                                                            that pod is deleted     

-) k run test-yellow-tech --image=busybox:1.28  --it --restart=Never -- nslookup yellow-tech-service


-) k create job --image=busybox:1.28 $do > aa.yaml -- sh -c "sleep 2 && echo done"        <-- like this we can create a direct job 




------------------
1) k run my-pod --image=busybox:1.28 --command sleep 3600         <--here we are creating a new pod, just for running "nslookup" command as pre given in the question 

2) k exec -it my-pod -- nslookup green-service               <-- first check if the output is correct 
-------------------

-) k run test-yellow-tech --image=busybox:1.28  --it --restart=Never -- nslookup yellow-tech-service


-) > k get events -o jsonpath='{range.items[?(@.reason=="Killing")]}{.involvedObject.name}{"\n"}{end}'         <-- get the last deleted objects

-) k get --raw "/version" | jq -f '.gitVersion'     <-- Get the version of the Cluster.

-) k cluster-info              <-- to get where is the "CoreDNS" and "control-plane" running at 


How To test a service in another namespace from a pod in another namespace:
--------------------------------------------------------------------------
Step-1) 
> k run test --image=busybox -it -- /bin/sh

Step-2)
# nc -v -z -w 2 <service-name>   <port-number>    <-- "service-name" means the service to whom we want to connect from this pod, we have to put a space in 
                                                  between pod and port-number. port-number is the port of the service. it should return 
                                                  Answer it returns:    <service-name> (ip-address) "open" or "blocked"

OR

Step-2)
# nslookup <service-name>.<namespace>.svc.cluster.local                





######################################################################################################
-) systemctl status kubelet   

-) systemctl start kubelet 

-) systemctl enable kubelet 

-) systemctl status kubelet                       <-- here it should show the "active (running)

OR

-) journalctl -u kubelet 

or

7) systemctl daemon-reload 

8) systemctl restart kubelet 

--------------------------------------------------------------------

-) systemctl restart etcd 




---------------------------------------
  containers:
  - name: busybox
    command:
    - "/bin/sh"
    - "-c"
    - "echo 'This is my best pod' && sleep 1000" 

--------------------------------------------------------
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:                <-- "httpGet" <-- this example i did not saw in the kubernetes-documentation so i have to remember it 
            path: /ready
            port: 80
          successThreshold: 3  

-----------------------------------------------------------
spec:
  containers:
  - name: nginx 
    image: nginx
    livenessProbe:                           <-- From the kubernetes-documentation page i have taken this "livenessProbe" option 
      httpGet:
        path: /healthz
        port: 80
      initialDelaySeconds: 300  


##################################################################################################

<service-name>                                            <-- like this we can call a service directy within the same namesapce 
> my-service                                     

<service-name>.<namespace-name>.svc.cluster.local         <-- like this we can call a Service from another namesapce 
> my-service.tech.svc.cluster.local 

<stateful-pod-name>.<headless-service-name>.<namespace-name>.svc.cluster.local       <-- like this we can call the Headless-pod through headless-service 
> my-sa-0.myheadless-service.tech.svc.cluster.local 

<pod-internal-ip>.<namespace>.pod 
10-50-192-2.default.pod                                    <-- like this we can call the pod directly 

<pod-internal-ip>.<namespace>.pod.cluster.local  
10-50-192-2.default.pod.cluster.local                      <-- in the ip add address use "-" insted of "."



#####################################################################################################



-----------------------------------------------------
Important-Question 1-100

Search: 13,33,36,48,49,52,55,57,69,71,72,73,76,79,83,85,86,98,99,100


pod-commands: 34,40,78

Error: 38,63
Network-policy: 12,45,77

liveness-probe: 54
Readiness-probe: 97

3,9(see-copy-method)

----------------------------------------------------
In Master-Node
---------------
Kube-api-server            6443   Port    (But somewhere i saw 8443 also)   "https://controlplane:6443/......
coredns
etcd                       2379   Ports 
kube-proxy
kube-scheduler             10259  Port 
Kube-Controller-Manager    10257  Port 
kube-flannel-ds
Kubelet                    10250  Port          ( Kubelet can also be deployed on Master-node)

In-Nodes
-------
Kubelet                    10250  Port 
Kube-proxy                                      (watch in lecture 223  "Service Networking")
Services                   30000 till 32767 Ports  ( "> ps aux | grep kube-api-server" in this process or directly in the "kube-api-server.yaml  file we can see 
                                                                                       a attribute "--service-cluster-ip-range=x.x.x.x/x" from here we can find that 
                                                                                       under which range our cluster is alloweed to provide automatically a ip address 
                                                                                       to the "Service" when it is created 
----------------------------------------------------------


> /etc/kubernetes/manifests/             <-- on the "controlplane" or "master" node we can get all the controlplane resources .yaml files 



ETCD
=====

ETCD listen on default port  "2379"


CRI:       <-- "Container-Runtime-Interface"   e.g, Docker, RKT, CRI-O 
CNI:       <-- "Container-Network-Interface" 
CSI:       <-- "Container-Storage-Interface"   e.g AWS-EBS, AWS-EFS, Azure-Disks


Commands:
---------
> k config use-context "user.name"            <-- like this we can change the config, "user.name" come form the  
                                                "kubectl config view"  -> config.yaml -> users: -> name 

> k config set-context --current --namespace=<namespace-name>   <-- like this we can set a default namesapce and every query goes to that namesapce.

> alias k=kubectl                 <-- it will set k as alias for "Kubectl" command

> export ETCDCTL_API=3
> etcdctl snapshot save 
> etcdctl endpoint health
> etcdctl get
> etcdctl put

> cat /etc/kubernetes/manifests/kube-apiserver.yaml       <-- to view "api-server" options where cluster is created with "kubeadm" tool     
> ps -aus | grep kube-apiserver 

> kubectl -n <pod-name> exec -it <sidecar-container-name> -- cat /log/app.log 

> kubectl get nodes              <-- it also return the "Kubernetes version" which is installed.

> kubectl replace --force -f /tmp/kubectl-edit-2345685343.yaml

Imp Points:
-----------
-) Kubernetes Pod Network Ip-assign   10.32.0.0/12    it means   10.32.0.1  till 10.47.255.254  



Declarative commands   <-- when we use .yaml files, it means we are declearing resources in a .yaml file
Imperative commands  <-- wehn we try to create a resource directly through command line


Difference between "kubectl apply" and "kubectl create" and "kubectl replace"
----------------------------------------------------------------------------
> "Kubectl apply" creates kubernetes objects through "Declarative commands"  <- with this the last .yaml file will be stored under "metadata.annotations"
> "kubectl create" creates kubernetes objects through "Imperative commands"  <- with this the last .yaml file will not be stored under "metadata.annotations"
> "kubectl replace" creates kubernetes objects through "Imperative commands"  <- with this the last .yaml file will not be stored under "metadata.annotations"


Difference between  "kubectl replace" and "kubectl replace --force"  and "kubectl edit" commands
------------------------------------------------------------------
> "kubectl replace -f aby.yaml" replace kubernetes objects through "Declaratice commands", we first make change manually on .yaml file
   after that with "kubectl replace -f abc.yaml" it will just change the changes that we make, it will not delete and recreate the object

> "kubectl replace --force -f abc.yaml":   Wtih "--force" we are saying please delete the old object and then create the new object with the
                                          new configuration that we made

> "Kubectl edit" edits kubernetes objects through "Imperative commands", we can directly make the changes in live manifest, 
   But changes will not be recorded.


Note: 
Secrets are only encoded
Secrets are not encrypted in ETCD db of Kubernetes
Certificates are Encrypted.

Pod-Eviction-Timeout: 
---------------------
Kubernetes will wait for 5min by-default for a Pod if it goes offline, As the whole node goes down, if after 5min the pod does not 
comes up then kubernetes declare it dead and create new pod on the other available node only if it is a part of a "ReplicaSet" if it is not
a part of any "ReplicaSet" then the pod is completely deleted.
By-default  in the "kube-controller-manager" it is set to   "--pod-eviction-timeout=5m0s" 

If the Pod with the Node comes back inbetween 5min then it will be taken as live again and no process will be done further.

=============================================================================================================
Basic commands:
---------------

--dry-run=client  <-- this will not create the resource, instead, it will tell us weather the command is correct or not

--dry-run=client -oyaml  <-- it will check if the command is correct and generate a .yaml file also

> kubectl replace --force -f abc.yaml         <-- this command will delete the existing pod and create a new with this file
                                              normally we need to write first "delete" then "apply" but this command do both
                                              of the things in one time.



> kubectl explain pods   or   replicase  or deployment
> kubectl describe pods
> kubectl create pods --help
> kubectl create secret generic --help
> kubectl explain secret 
> kubectl run --help
> k taint --help 

> kubectl replace --force -f abc.pod          <-- like this it will forcefully delete and recreate the pod

> kubectl replace --force -f /tmp/kubectl-edit-3434343.yaml     <-- like this we can forcefully delete and recreate the pod

----------------------------------------
Pods :
------
> kubectl  run  mypod --image nginx -n my-namespace         <-- it will directly create a pod in "my-namespace"
> kubectl run mypod --image redis:alpine --labels="tier=db"
> kubectl run mypod --image redis:alpine --labels env=dec,ab=bc
> kubectl run my-pod --image nginx --port=8080
> kubectl run my-pod --image httpd:alpine --port 80 --expose true    <-- it create a "pod" and also a "Service" same time
> kubectl run my-pod --image busy-box --command sleep 3200 --dry-run=client -oyaml      
> k run mypod --image=ubuntu:18.4 --command -- sleep 1000            
> k run mypod --image=ubuntu:18.4 --dry-run=client --command -- sleep 1000              
> kubectl get pods 
> kubectl get pods -owide              <-- it also shows that on which node this pod is placed
> kubectl get pod -n my-ns --show-labels         <-- it will show the pods with their labels also
> kubectl describe pods 
> kubectl get pods --all-namespaces
> kubectl get pods -A
> kubectl get pods --selector app-labele=myvlaue          <-- like this we can search for pods that has a lable "app-labele=myvlaue"
> kubectl get pods --selector env=dev | wc -l             <-- the return the number of lines in the result, their is a HEADER in the result also
                                                              this is the reason we have to do -1 from the resutle, if result is 8, then 8-1= 7 is answer
> kubectl get pods --selector env=dev --no-headers | wc -l     <-- this will return a result with a number, in the result no HEADER is included 
                                                                  so as the last obove command we can directly get 7 as result.

> kubectl get all --selector env=prod --no-headers | wc -l   <-- return all the objects that has lable "env=prod"

> kubectl get all --selector  env=prod,tier=db,app=p-app 

> kubectl get pods --watch

> kubect set image   my-deployment my-container=nginx:1.9.1     <--like this we can set a new "image" in the Deployment 

> k api-resources                           <-- get all the resources 

> k api-resources --namespaced=true         <-- it will return a list of all the resources that we can create inside a "Namespace"

> k api-resources --namespaced=false        <-- it will return a list of all the resources that we can not create inside a "Namespace"
                                              those must be cluster scope 

######################################################################################################################################
Command from practise course:
----------------------------
> kubectl get pod -A --sort-by=.metadata.creationTimestamp           <-- sort all the pods as per Descending order as per creation-time
                                                                        by-default it output in Descending order

> kubectl get pod -A --sort-by=.metadata.creationTimestamp | tac      <-- with "tac" it will reverse the flow and make it to Ascending order


########################################################################################################################
Cluster Upgrade:
----------------
Q-3)  Upgrade the cluster ?
                    -kubeadm: 1.19.0
                    -kubelet: 1.19.0
                    -kubectl: 1.19.0


-) kubeadm upgrade plan 

-) cat /etc/*release*            <-- this command shows us which operating systemm i am currently working, 

1) k drain <node-name>
2) k cordon <node-name>
3) apt update 
4) apt install kubeadm=1.19.0-00
5) kubeadm upgrade apply v1.19.0
6) apt install kubelet=1.19.0-00
7) apt install kubectl=1.19.0-00
8) systemctl restart kubelet 
9) kubectl uncordon <node-name> 



Q-15)(check) Upgrade the cluster (Master and worker Node) from  1.18.0   to   1.19.0 

1) k get nodes              <-- we have "controlplane" (master)  and node01 (worker) nodes 

2) k drain controlplane --ignore-daemonsets          <-- Drain the master node first, it will also "cordon" the node 
3) ssh controlplane
4) apt update 
5) apt install kubeadm=1.19.0-00
6) kubeadm upgrade apply v1.19.0 
7) apt install kubelet=19.19.0-00
8) systemctl restart kubelet 
9) k uncordon controlplane 
10) k get nodes                               <-- at this point we can see the "master" node upgrade is finished 
                                              In the output recheck the verion for master node 

Note: we run same commands for "worker node" again 

1) k drain node01 --ignore-daemonsets          <-- Drain the worker node now , it will also "cordon" the node 

2) ssh node01
3) apt update 
4) apt install kubeadm=1.19.0-00
5) kubeadm upgrade node 
6) apt install kubelet=19.19.0-00
7) systemctl restart kubelet 
8) k uncordon node01
9) k get nodes                               <-- at this point we can see the "worker" node upgrade is finished 
                                              In the output recheck the verion for worker  node 


#####################################################################################################################################
ReplicaSet: 
-------------
> kubectl explain replicaset
> kubectl edit rs my-set
> kubectl scale rs my-set --replicase=5  

Deployment: 
-----------

> kubectl create deployment my-dep --image nginx --replicas=3

> kubectl create deployment --image nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml 

> kubectl create deploy

> kubectl expose deployment my-dep --port 80

> kubectl scale deloyment my-dep --replicas=5

> kubectl set image deployment my-deploy nginx=nginx:1.18

> kubectl get service 
> kubectl get svc
> kubectl describe svc my-service
######################################################################################################################
Rollout:
-------

Note: Rolling-update is the default deployment strategy. if we do not define any strategy, then it select by-default "Rolling-update" 
      as a strategy.

Rolling-update Strategy:
---------------
In Rolling-update if we have 5 pods are already their, and we want to update them with new version, then while doing
deployment, it delete one pod and bring one new, in this way it goes.

Recreate Strategy:
------------------
In this case it first delete all the pods and after that it create all new pods at once, this strategy can have downtime 
because at the time of first delete all the pods are deleted.

> kubectl create -f abc.yaml  --record          <-- like this we can create a new Deployment and also recording it for the "k rollout" directly from first time.
                                          Note: As at the creation time we put the flag "--record" now on every update of this deployment it will be automatically 
                                                saved versions for "k rollout" we do not need everytime to user this "--record" flag on any updation.
                                          Note: if we do at the time of update a Deployment "--record" , if we do update after this one then we do not 
                                                need to put this "--record" flag again.     


> kubectl get deployments 

> kubectl create -f abc.yaml       <-- with this command we can create a deployment first time

> kubectl apply -f abc.yaml           <-- As a deployment is already create, with this command it will update the Deployment
                                         If their is no deployment then with this command we can also deploy it first time

> kubect set image   my-deployment my-container=nginx:1.9.1    <-- like this we can change the image of current running deployment   
                                                                 my-deploment is the name of the Deployment
                                                                 my-container is the name of the Container                                      


> k rollout status deployment/my-deploy      <-- it will log the status of the deployment

> k rollout history deployment/my-deploy     <-- it will print the history of the deployemnt

> kubectl rollout status my-deployment      <-- with this we will get the status of the deplyoment

> kubectl rollout history my-deployment   

> kubectl rollout undo my-deployment          <-- with this we can rollback the changes to last deployed deployment.
 
> k rollout undo deployment my-deployment --to-revision=3           <-- like this we can rollback to any version    

Create a new Service
-------------------
> kubectl expose pod my-redis --port 6379 --name my-serice --dry-run=client -oyaml   <-- it will create a Serive and automatically exposes the port
                                                                                   or generate lable connection with the Pod name "my-redis"
                                                                                   it automatically uses the Pod lable and add in Service
                                                                                   By-default it creates a "clusterip" service
                                                                                   with this command we can not use --tcp=80:80 --node-port=30080 

> kubectl create service clusterip my-redis --tcp=6379:6379 --dry-run=client -oyaml     <-- the main point is that we can not add any selector with this 
                                                                                      command, it automatically only generate "app=my-redis" as tag
                                                                                      we can not explicitly define any selector with "kubectl create" command
                                                                                 
> kubectl expose pod my-pod --type NodePort --port=80 --name my-service --dry-run=client -oyaml    <-- it will create a new service and as we have used
                                                                                                    "kubectl expose" it will automatecally take the 
                                                                                                    Pod tags and put in Service-Selector
                                                                                                    with this command we can not use --tcp=80:80 --node-port=30080 

> kubectl create service nodeport my-service --tcp=80:80 --node-port 30080 --dry-run=client -oyaml    <-- the main point is that we can not add any selector with this 
                                                                                                       command, it automatically only generate "app=my-redis" as tag
                                                                                                      we can not explicitly define any selector with "kubectl create" command                                                                                                                 

---------------------------------------------------------------------------------
Service:
-------
Assume we have two namespaces  Namespace "A"   and Namespace  "B"

mysql.connect("db-service")          <-- if a web-app in Namespace "A" call a service in same namespace "A" then it directly use the service-name

mysql-connect("db-service.dev.svc.cluster.local")    <-- if a web-app in Namespace "A" call a service in different Namespace "B" this it should use 
                                                        this way, When we create a service a DNS name is created with this whole name "db-service.dev.svc.cluster.local"
                                                        in the Kubernetes cluster, As Service is a cluster specific object so when we call this DNS it will route 
                                                        request to this service

db-service         <-- name of the Service
dec                <-- name of the Namespace
svc                <-- It is a subdomain added in DNS, as we are calling a Service
cluster.local      <-- It is a subdomain for the cluster,As both of the Namespaces are in same cluster
--------------------------------------------------------------------------------

Labels and Selectors:
---------------------
> k get pods --selector app=App1

> k get pods --selector env=App | wc -l 
> k get pods --selector env=App --no-headers | wc -l 

> k label pod mypod env=dev,app=dev
> k label node node01 env=dev



> kubectl config set-context $(kubectl config current-context) --namespace=dev          <-- like this we can set the config to "dev" namesapce
                                                                                      it means when we do "k get pods" it will fetch pods details from "dev" namespace


ResourceQuota for Namespace
----------------------------
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec: 
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi  


#####################################################################################################################
Readiness & Liveness 
--------------------


Readiness-Probe:     
----------------
At the time of starting a pod, kubelet check under this "readinessProbe" weather the pod is ready or not.
Once the pod is ready after that kubelet will not call this endpoint anymore.

Liveness-probe:
--------------





Their are three type of "readinessProbe"
1) httpGet:

2) tcpoSocket:

3) exec:
    command                 <-- with this we can define our own command 


-) httpget: 

apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: registry.k8s.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      httpGet:
        path: /api/any/ready            <--if we have any endpoint in the application, then we can  automatically a request to that endpoint,
        port: 8080                         that path here and at the time of starting the containter kubelet send it if return 200 then it is ready 
      initialDelaySeconds: 15           <-- assume i have a JAVA service and it takes 10-15sec to start then we can put that starting time here                                                             
      periodSeconds: 10                 <-- Default value is 10-sec:  As kubelet make a "readiness" check that this pod is ready or not, here we can define that time 
                                            after how many seconds kubelet should make a call to check the status of the pod. Deafult value is 10sec 
      failureThreashold: 4              <-- Default value is 3   if some how after 15 second as above defined the service is not ready then, kubernetes will try 3 
                                            times to check if the service is ready or not, if after that it is not ready then it will mark pod as faulty. 



-) tcpSocket:

apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: registry.k8s.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:                                           <-- if we have a TCP-Socket connection then use this 
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10


-) exec: 
     command: 

apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: registry.k8s.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:                                      
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10








#######################################################################################################################
Jobs:
-----


apiVersion: batch/v1
kind: Job
metadata:
  name: my-job
spec:
  completions: 3                             <-- it will create 3 replicas of this pod    
  template:
    spec:
      containers:
      - name: my-job
        image: ubuntu
        command: ['expr','3','+','2']
      restartPolicy: Never   


apiVersion: batch/v1
kind: Job
metadata:
  name: my-job
spec:
  completions: 3            
  parallelism: 3                       <-- it will create 3 pods that run parallaly, it means one job is done by three pods    
  template:
    spec:
      containers:
      - name: my-job
        image: ubuntu
        command: ['expr','3','+','2']
      restartPolicy: Never  

apiVersion: batch/v1
kind: Job
metadata:
  name: my-job
spec:
  completions: 3                             <-- it will create 3 replicas of this pod    
  template:
    spec:
      containers:
      - name: my-job
        image: kodekloud/throw-dice
      backoffLimit: 25                         <-- it will run 25 times this "Job" as a job starts and if it does not complete then it tries for "25" times
      completions: 3                          <-- their is some logic written in the image, that the result should be "6" then only the task is completed 
                                               "completions" means that task should be completed, if the result be 4 or 5 then task is not completed 
                                                like this we can apply a completions


##########################################################################################
CronJob:
--------


kubectl create cronjob my-cronjob --image=your-image:latest --schedule="*/1 * * * *" --restart=OnFailure --dry-run=client -o yaml > cronjob.yaml


apiVersion: batch/v1
kind: CronJob
metadata:
  name: my-job
spec:                                   <-- this "spec" section is for "Cronjob" itself 
  schedule: "* * * * *"
  successfulJobsHistoryLimit: 2             <-- This field means keep the history of "2" Successful Jobs 
  failedJobsHistoryLimit: 4                 <-- This field means keep the history of "4" failed jobs.
  jobTemplate:
    spec:                               <-- As "CronJob" run "Job" internally, so this "spec" if for "Job"
      completions: 3            
      parallelism: 3
      activeDeadlineSeconds: 10           <--This time is set for the "Job" no matters how many pods are running this job, once it reach "10" sec and 
      template:                              if the job does not finish then it will terminate all the pods.
        spec:                            <-- this "spec" section is for "container" itself
          containers:
          - name: my-job
            image: ubuntu
            command: ['expr','3','+','2']
          restartPolicy: Never
          terminationGracePeriodSeconds: 8        <-- it will terminate the pod after "8" seconds


###################################################################################################################
Admission Controllers:
-----------------------

> kube-apiserver -h | grep  enable-admission-plugins        <-- it will show the list of all the "enabled Admission Controllers"

> kubectl exec kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep  enable-admission-plugins       <-- if cluster is deployed with "kubeadm" 
                                                                                                                      then use this command. 

> ps -ef | grep kube-apiserver | grep admission-plugins          <-- to get the list of all the "enabled" and "disabled"  Admission-controllers 

kube-apiserver.yaml         
------------------
spec:
  containers:
  - command:
    - --enable-admission-plugins=<Any-Admission-controller-name>            <-- to enable any new "Admission controller"
    - ---disable-admission-plugins=<Any-Admission-controller-name>       <-- to disable any existing "Admission controller"


    


######################################################################################################################
Taints:
-------
> kubectl describe node kubemaster | grep Taint         <-- this will show us the which "Taint" is applied on the Master-node

> kubectl taint node <any-node-name> key=value:Taint-Type

> kubectl taint node mynode  mykey22=myValue22:NoSchedule


To remove the Taint from a node
-----------------------------
> kubectl taint node mynode  node-role.kubernetes.io/master:NoSchedule-     <-- first go check the whole value of the Taint from the "describe node" command
                                                                            then put that value as above and at the end put "-" minus sign

Note: 
key       <-- it can be a string upto 253 characters
value     <-- it can be a string upto 63 characters


q) How to Taint a Node and define same Taint for the Pod ? 

Taint    
------

their are 3 tyes of "Taint" values we can use in "effect" key
1) NoSchedule                         <-- new pods that do not match the taint are not scheduled onto this node, But existing pods on the node remain running.
2) PreferNoSchedule                   <-- new pods that do not match the taint might be scheduled onto the node,
                                          but the existing pods on the node remain running.
3) NoExecute                          <-- in this case only same "Tainted" pods will be placed on this Node, and also if any old another "Tainted" pods
                                          are their then they will be removed from this Node

Note: we can use two types of"opertaor" 
1) Equal            <-- it means all   key,value,effect  all must be matched. (Imp:  this is a default parameter)
2) Exist            <-- it means  key,effect parameters must match, you must leave a blank value parameter, which matches any


> kubectl taint nodes node1 anyKeyName=blue:NoSchedule             <-- here we are "Tainting" a node, with  "key=app"   and "value= blue : NoSchedule" 
                                                            Note: this command we use to "Taint" a node

--------------------------------------------------
Defining a pod with the same "Taint" as we define above for the "Node"

apiVersion: v1                                                      
kind: Pod
metadata: 
  name: myPod-color
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  tolerations:
  - key: "mykey22"                    <-- this "anyKeyName" is the key defined above in the node command, we can put any name
    operator: "Equal"                 <-- as in the above Node command we use  "="   so here we have to put "Equal"
    value: "myvalue22"                <-- as in the above Node command we use Value = "blue" so here we are using blue
    effect: "NoSchedule"              <-- their are 3 types of values we can use as above defined, so here we are using one of them           
             
 Note: all the 4 values must be put inside the double quotes            

> kubectl describe node my-node 

> kubectl label node my-node my-lable=my-value

apiVersion: v1                                                      
kind: Pod
metadata: 
  name: myPod-color
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  nodeSelector:
    my-lable: my-value  

#######################################################################################################################
-----------------------------------------------------------------------------------------------------------
Node Affinity  
------------

q) Assume we have two Nodes, one with larger capacity means 5-cpu and 20GB ram and one Node has 2-cpu and 10-gb ram,
   how we can lable these Nodes and how we can Tell a Pod to go to a Larger Node

> kubectl label nodes node-1 size22=Large22                    <-- we can  put a Lable for the Node  "size=Large"  and if we put same lable in the 
                                                             Pod defination .yaml file, it means we are saying that that Pod should go to this
                                                             Node only   

options
1) requiredDuringSchedulingIgnoredDuringExecution      ( Must take this )
2) preferredDuringSchedulingIgnoredDuringExecution     ( it is just a advice )


Note: 
1) "requiredDuringSchedulingIgnoredDuringExecution" in this case  as per the condition if no "node" is found with matched label defined
   in the pod.yaml file then kubernetes will not create any pod and it will go in "Pending" stage, this "Pending" status shows us when we 
   do "kubectl get pods"
2) "preferredDuringSchedulingIgnoredDuringExecution" in this case as per the condition if no "node" is found with the matched label
   kubernetes will not stop the pod , it will just go to another node and create the pod 



apiVersion: v1
kind: Pod
metadata: 
  name: myapp-pod
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size22
            operator: In                        <-- using  "In"  operator
            values:
            - Large22                            <-- like this way we can define that this Pod can be placed on a Node which has a Lable
            - Medium22                               of   "size22=Large22"   or "size22=Medium22" 




"NotIn" example
---------------
apiVersion: v1
kind: Pod
metadata: 
  name: myapp-pod
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution
        nodeSelectorTerms:
        - matchExpressions:
          - key: size22
            operator: NotIn                          <-- using "NotIn"   <-- it means this Pod can go to any Node which do not have a Lable "size22=Small22"            
            values:
            - Small22

OR 
---
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution
        nodeSelectorTerms:
        - matchExpressions:
          - key: size22
            operator: Exists      <-- it checks only if the "key" with the name "size22" exists in the Node, then it directly place the pod in that Node


---------------------------------------------------------------
apiVersion: v1        
kind: Pod             
metadata:                 
  name: myapp 
  labels:              
    app: mypod22
    env: Prod-2                    
spec:             
  containers:                     
    - name: myapp-frontend             
      image: mydockerimage/1.0.0
      resources:
        requests:                         <--  "request" it means this pod must have minimum memory  
          memory: "64Mi"
          cpu: "0.5"          
        limits:                           <-- "limits"  it means this pods can take only memory upto  "4GB"
          memory: "4Gi"                      no more memory will be allocated to it.
          cpu: "1"    

Note: As laptops have 8-cores so it means 8 CPUs              

e.g
----
limits:
  cpu: 1             <- it means "1 CPU"  or "1 core" 

----
limits:
  cpu: 500m   or  100m  or 0.1          <- these all  means half CPU


e.g
---
limits:
  memory: 1Gi             <- it means the memory of "1 GB"  or "1024 MegaByte"   or "1024 Mi"


---
limits:
  memory: 512Mi             <- it means the memory of "512 MegaByte"    so we can say half of 1 GB

---------------------------------------------------------------------
ResourceQuota
-------------
apiVersion: v1
kind: ResourceQuota
metadata: 
  name: compute-quota                     
  namespace: dev                   
spec: 
  hard: 
    pods: "10" 
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi  


Note: we can set "ResourceQuota" at Namespace-level, every Namespace can have its own "Resource-Quota"

########################################################################################################################
-------------------------------------------------------------------------------
DaemonSet
--------

apiVersion: apps/v1   
kind: DaemonSet
metadata:
  name: monitoring-daemon 
spec:
  selector:
    matchLabels: 
      app: monitoring-agent-a            
  template:    
    metadata:                     
      labels:              
        app: monitoring-agent-a         
    spec:             
      containers:                  
        - name: monitoring-agent
          image: mydockerimage/2.0.0   

> kubectl get daemonsets

> kubectl describe daemonsets my-daemonset



How to create the path for the "Static-pod" in a Node ?

Kubelet.service file
---------------------

Way-1) 
------
ExecStart=/usr/local/bin/kubelet  \\
  --pod-manifest-path=/etc/Kubernetes/manifests \\           <-- here we are defining path for the Static-pod that is created in the Node


Way-2) (when we create cluster with "cubeadmin tool" then it usese the "way-2")
------
ExecStart=/usr/local/bin/kubelet  \\
  --config=kubeconfig.yaml \\                   <-- we can define the above path in a "kubeconfig.yaml" file and put the file name here 

Note: in the "kubeconfig.yaml" file we can find this path "staticPodPath: /etc/kubernetes/manifests"

######################################################################################################################
Static Pod
-------------
> cat var/lib/kubelet/                  <-- All 4 files i can find under it.

> cat var/lib/kubelet/config.yaml             <-- in this file "staticPodPath:  ...."  <-- see this key hier the path is defined
                                                  It is the kubelet on the Node who is creating the pod 
> cat var/lib/kubelet/kube-apiserver.yaml                                                  

> cat var/lib/kubelet/kube-controller-manager.yaml

> cat var/lib/kubelet/kube-scheduler.yaml 

> cat /etc/kubernetes/manifests/           <-- under this folder we can place all the "Static-pod"  yaml files.



#########################################################################################################################
Multi Scheduler:
================

> kubectl get events -o wide          <-- under it we can see if the new Pod is running with a custom-scheduler or not

> kubectl logs my-scheduler -n kube-system         <-- we can view logs of out scheduler

> k get events -o wide                    <-- like this we can see all the events, e.g  when a pod is created then 
                                          which "scheduler" has placed this pod in the node, we can se scheduler name in event

> k logs my-scheduler -n kube-system            <-- like this we can also see the logs of the "Scheduler" 


kube-scheduler.service    (Default Scheduler)
----------------------
ExecStart=/usr/local/bin/kube-scheduler  \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml


my-scheduler.service
--------------------
ExecStart=/usr/local/bin/kube-scheduler  \\
  --config=/etc/kubernetes/config/my-scheduler.yaml


----------------------------------------------------
How to define a Custom Scheduler ?

Step-1)
--------
my-scheduler.yaml
--------
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: true
  resourceNamespace: kube-system
  resourceName: lock-object-my-scheduler  


Step-2) 
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
  namespace: kube-system
spec:
  containers: 
    - command:
      - kube-scheduler
      - --address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --config=/etc/kubernetes/my-scheduler.yaml
      image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
      name: kube-scheduler    

Step-3)
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
  namespace: kube-system
spec:
  containers:
    - image: nginx
      name: nginx
  schedulerName: my-scheduler    


--------------------------------------------------------------------------------------------
PriorityClass:
-------------

How to create a PriorityClass ?

Step-1)  
Create a priorityClass file

abc.file
--------
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: my-high-priority
value: 100000
globalDefault: false


Step-2)

pod.yaml
-------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  priorityClassName: my-high-priority
  containers:
    - name: nginx
      image: nginx
      resources: 
        request: 
          memory: "1Gi"
          cpu: 10

########################################################################################################################
Monitoring
----------

> kubectl top node            <-- to get the metrics of all the Nodes
> kubectl top pod             <-- to get the metrics of all the pods

> kubectl logs -f my-pod           <-- it will return the logs of my-pod


> kubectl logs -f my-pod container-1       <-- if we have two container running in a pod, e.g one service and other sidecar contianer
                                               then we have to give the name in the command
> kubectl logs -f my-pod sidecar-container                               





###############################################################################
Docker Commands:
---------------

Note:  ENTRYPOINT ["sleep"]   or  CMD ["sleep"]     <-- in the both commands the first argument must be a executable
                                                        it means we must put ["sleep"]   if we put ["10"]  then we will get error
                                                        but if we use ENTRYPOINT and CMD then in the second place CMD can have 
                                                        CMD ["10"]  as argument.

Case-1) 
-------

dockerfile-a.yaml
-----------------
FROM Ubuntu
CMD sleep 5

> docker run dockerfile-a                     <-- in this case the Ubuntu pod execute for 5sec after that it destroys



Case-2) 
-------

dockerfile-a.yaml
-----------------
FROM Ubuntu
CMD sleep 5

> docker run dockerfile-a    sleep 10           <-- As we have used "CMD", then in this case which argument we pass while doing
                                                 "docker run dockerfile-a"  the whole argument will be replaced
                                                 "CMD sleep 5"   will be replaced to "CMD 10"   at the run time
                                                 if we just pass  "docker run dockerfile-a 10"  then we will get error 
                                                 as with the CMD the whole argumes will be replaced


Case-3) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         

> docker run dockerfile-a 10           <-- as we use "ENTRYPOINT" then any argument what we put in the command will be added at 
                                         the execution time. at the end as we pass "10" it will be "ENTRYPOINT sleep 10"


Case-4) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         

> docker run dockerfile-a         <-- in this case we forget to write the argument in the command so in this case 
                                 it throws error as at the end it rund "ENTRYPOINT sleep" only



Case-5) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         
CMD["10"]

> docker run dockerfile-a            <--in this case as we forget to put an argument then by-default it take 10 from CMD






Case-6) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         
CMD["10"]

> docker run dockerfile-a   20              <-- AS which argument we put at the run time it totally override the "CMD"
                                               so  "20" will override with CMD["20"] 

Case-7) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         
CMD["10"]

> docker run --entrypoint sleepAA dockerfile-a 20           <-- like this we can change the "ENTRYPOINT" argument from
                                                            "ENTRYPOINT ["sleep"]"   to "ENTRPOINT ["sleepA"]"
                                                            At the runtime it become  "sleepA 20"
                                                            Note: Their is not "sleepA" command, it is just for example i am showing


Case-8)

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command: ["sleep"]             <-- as in the above example "command" is same as "ENTRYPOINT"
      args: ["10"]                   <-- as in the above example "args" is same as "CMD"
 



Case-9)

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command: ["sleep" ,"10"] 


or


pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command:
      - "sleep"
      - "10"

or 

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command: ["sleep"]                    <-- command is same as "Entrypoint"
      args: ["10"]


> kubectl run my-pod --image-nginx --command -- sleepA     <-- like this we can override the "command: ["sleepA"]" argumen, 
                                                              as we do not define anything else so it take "10" from "args"

> kubectl run my-pod --image-nginx --command -- sleepA  2000   <-- like this we can override the "command: ["sleepA","2000]"  

> kubectl run my-pod --image=nginx -- 15                   <-- like this we can override the "args: ["15"]" argument
or
> kubectl apply -f pod.yaml -- 15                       <-- this is just my try example, but not sure if it works

> kubectl run my-pod --image=nginx -- sleep 15         <-- like this we can override the "args: ["sleep","15"]" argument


###################################################################################################################
Environment Variables  with ConfigMap and secret
-------------------------------------

Case-1) 
Environment varibale directly in the Pod

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
      env:                                      <-- like this we can define an "Environment variable", it will be then placed in the container 
        - name: my-key-a                            so the service can call directly from code the Environment variable
          value: my-val-a   
        - name: my-key-b
          value: my-val-b   

--------------------------------------------------------------------------------
Case-2) 
ConfigMap  & Secret
-------------------

Environemnt variable adding in pod through configMap

> kubectl get configmap
> kubectl describe configmap

> kubectl create configmap my-configmap --from-literal=my_key_AA=my_value_AA --from-literal=my_key_BB=my_value_BB 

> kubectl create configmap my-configmap --from-env-file=abc.properties                   <-- write the key-value in a file and put the file name here

> kubectl get secret 
> kubectl create secret my-secret --from-literal=my_key_AA=my_value_AA --from-literal=my_key_BB=my_value_BB 

> kubectl create secret my-secret --from-env-file=abc.properties                    <-- write the key-value in a file and put the file name here


> echo -n "my-pass | base64
> echo -n "cm9cdA==" | base64 --decode


Step-1)
configMap.yaml
--------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap-A
data:
  APP_COLOR: blue
  APP_MODE: prod

Step-2)
secret.yaml
--------
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret
data:
  my-username: dxNlcm5hbWU=
  my-password: cGFzc3dvcmQ=

Step-3)

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080     
      envFrom:
        - configMapRef:
            name: my-configmap-A                   <-- here we are putting the above define configmap
      envFrom:
        - secretRef:                               <--here we are refering the whole Secret to the pod 
            name: my-secret                           it means all the key-value will be inserted in as evn-variable

--------------------------------------------------------------------------
Case-3) 
Environment variable in ConfigMap or Secret in a Volume for Pod 
---------------------------------------------


Step-1)
configMap.yaml
--------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap-A
data:
  db-host: my-db-pass


Step-2)
secret.yaml
--------
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret
data:
  my-username: dxNlcm5hbWU=
  my-password: cGFzc3dvcmQ=


Step-3)
pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080     
      env:
        - name: MY_DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: MY_USERNAME                  <-- here we are inserting only this key from the secret, as env variable 
        - name: MY_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: my-password
        - name: MY_CONFIGMAP
          valueFrom:
            configMapKeyRef:
              name: my-configmap-A
              key: db-host

---------------------------------------------------------------------------------------------------
ConfigMap and Secret as a Volume
-----------------------------

Step-1) 
configMap.yaml
--------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap-B
data:
  mydata.conf: |               <-- this is a type of a file
    data_ aaaa
    data_ bbbb
    data_ cccc


Step-2)
secret.yaml
--------
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret-B
type: Opaque
data:
  secret.file |                                          <-- like this it is used as a file
    dfdjdkfjekERDF435454DFEjkgjkgRDLOIGME=    


Step-3)
pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
      volumeMounts:
        - name: my-confg-map-volume-DD 
          mountPath: /tmp/configmap/new-folder-in-container         <--with this we are creating a new container in the pod and the file in the configMap 
                                                                       "mydata.conf" will be placed in this newely created folder
        - name: my-secret-volume-DD
          mountPath: /tmp/secret/new-folder-in-container
  volumes:
    - name: my-confg-map-volume-DD                  <--- here we can give any name, it will just show in the container
      configMap:
        name: my-configmap-B                     <-- here we need to correct name for the mapping the above create config-map
    - name: my-secret-volume-DD
        secret:
          secretName: my-secret-B



-------------------------------------
Create a configmap from file:
-------------------------

abc.txt 
------
user=my-user
pass=1234


> k create cm  test-cm --from-env-file=abc.txt          <-- we must use "--from-env-file"

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080     
      env:
        - name: MY_DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: test-cm
              key: user 

-------------------------------------------------



##########################################################################################################
Multicontainer pods,  Sidecar
-----------------------------
> kubectl logs <pod-name> -c <container-name>       <--like this we can give the continer-name and see the logs


pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
    - name: my-sidecar-pod
      image: nginx    


----------------------------------------------------------------------
Attach only a "Volume" to containers in a Pod
---------------------------------------------

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
      volumeMounts:
        - mountPath: /var/log/event-simulator/
          name: my-ubunut-volume
    - name: my-sidecar-pod
      image: nginx   
      volumeMounts:
        - mountPath: /log
          name: my-sidecar-volume

####################################################################################################
InitContainers:
-------------

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
  initContainers:
    - name: my-init-container
      image: busybox
    - name: second-init-container
      image: nginx  

> k logs <pod-name> -c <init-contaier-name>               <-- like this we can see the logs coming from "InitContainer"      


######################################################################################################
Node:
----

-> ssh <node-name>    <-- like this we can switch to or we can say go inside the "node" that the name is defined 
-> exit               <-- to come out from the node back. 

Note: 
-) when a node goes offline the master-node wait for 5min before considering it dead. it is defined in the
   "kube-controller-manager"  --pod-eviction-timeout=5m0s", After this when a new Node come in place of old one 
  then it comes blank with no pods scheduled on it. 




> kubectl drain my-node-1          <-- if we want to delete all the pods on the node, after this command no pod will be available on the node
                                       and the "kube-scheduler" will not place new node on this node, Internally it do "kubectl cordon" that 
                                       so that no new node will be placed on the Node

> kubectl drain my-node-1 --ignore-daemonsets

> kubectl drain my-node-01 --force                                        

> kubectl uncordon my-node-1      <-- Only after doing "uncordon" the Node come online again and the "kube-scheduler" will start placing 
                                      Pods on this Node again.

> kubectl cordon my-node-1        <-- It marks the Node as unscheudable, After this command no new Pods will be place on this node,
                                      But the old Pods will still remain running on this Node, Only after "kubectl uncordon" the 
                                      Kube-scheduler is alloweed to place the pods again on this Node.



#############################################################################################################
Cluster Upgrade
----------------
if i am haveing kubernetes version v1.13.4

kube-apiserver      v1.13.4
Controller-manager  v1.13.4
Kube-scheduler      v1.13.4
Kubelet             v1.13.4
kube-proxy          v1.13.4 
kubectl             v1.13.4

ETCD Cluster        v3.2.18 
CoreDNS             v1.1.3         <-- these both of them have their own release versions, they are not same as kubernetes-version


Imp-points:
----------
-) As "Kube-apiserver" is the main component and all the other components are taking through "kube-apiserver" so in this case 
  all other component must be lower or equal to the version of "kube-apiserver".

-) If "kube-apiserver" has "v1.13.4" then  "Controller-manager" and "kube-scheduler" can be either has same version or can be one version lower then "kube-apiserver" version
   It means they can be on "v1.12"  version but not below the -1 version 

-) If "kube-apiserver" has "v1.13.4" then "kubelet" and "kube-proxy" can be either has same version or can be two version lower then "kube-apiserver" version   
   It means then can be on "v1.12" or "v1.11"

-) If "kube-apiserver" has "v1.13.4" then "kubectl" (Kubectl that we have installed on our computers), "Kubectl" can be one version higher or one version lower then
   the "kube-apiserver" version. it means "kubectl" can be "v1.12" or "v1.14"

Note:
-) Upgrading Cluster is always first we Upgrade "Master-node" after we upgrade "Worker-nodes"

-) If we upgrade the cluster with "kubeadm" tool then we should upgrade "kubelet" manually on every Node extra.
-) We must upgrade the kubeadm-tool itself before upgrading the cluster













> kubeadm upgrade plan

> kubeadm upgrade apply

Upgrading process with kubeadm tool

Step-1)  
> apt-get upgrade -y kubeadm=1.12.0-00            <-- First upgrade the "kubeadm" itself

Step.2) 
> kubeadm upgrade apply v1.12.0                   <-- After that upgrade the cluster

Step-3) 
Upgrade "Kubelet" on the Master-Node, Note: we do not need to upgrade the "Kubeadm" on the Master-node

> apt-get upgrade -y kubelet=1.12.0-00            <-- After upgrading the Control-plane on Master node, we have to upgrade the "Kubelet" first on the "Master" only
> systemctl restart kubelet                       <-- After upgrading the "Kubelet on the node, we have to restart the "Kubelet" then only the upgrade effects

Step-4) 
Upgrade "Kubeadm" and "Kubelet" on Worker Node

> kubectl drain my-node-1
> apt-get upgrade -y kubeadm=1.12.0-00
> apt-get upgrade -y kubelet=1.12.0-00
> kubeadm upgrade node config --kubelet-version v1.12.0
> systemctl restart kubelet 

> kubectl uncorden my-node-1

########################################################################################################################################
ETDC Backup
-----------

> ps -ef | grep -i etcd   <-- we can also get all the endpoint info from a "process", like this we are getting info from a "etcd" process 
                                      

> k get pods -n kube-system   
> k describe pod etcd-controlplane   <- under etcd.Image  <-- here in the image-name we can find the deployed version of "ETCD"



> export ETCDCTL_API=3    <-- like this we can set the ETCDCTL version to "3", normally we need this "ETCDCTL_API=3 command before 
                              every "etcdctl" command, so just "export" it then we do not need to put it in front of every command.
> etcdctl version          

-) ETDC cluster is placed on the master-node

-) "etcd.service"   <-- under this file we can find "--data-dir=/var/lib/etcd" <-- here by-default kubernetes store all the data for "ETCD"

Steps for Backup using snapshot:
---------------------
> etcdctl snapshot save snapshot.db 
> etcdctl snapshot status snapshot.db 

Steps for Restore ETCD db from Backup 
------------------------------------
> service kube-apiserver stop                                                     <-- first stop the "kube-apiserver"
> etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup       <-- here we put the path where the backup is stored

> change the new data directry path in teh "etcd.service" path. directory name specified in the above command "etcd-from-backup"

> systemctl daemon-reload
> service etcd restart 
> service kube-apiserver start 

Imp: 
-) With all the "etcdctl" we shoud add these 4 commands also 
--endpoints=https://127.0.0.1:2379
--cacert=/etc/etcd/ca.crt 
--cert=/etc/etcd/etcd-server-crt 
--key=/etc/etcd/etcd-server.key 







##########################################################################################################################################
Certificates
-------------

Public key extensions:
------------------------
*.pem 
*.crt             <--  Those end with these are "Public Key" or "Public Certificates"


Private key extensions:
----------------------
*.key
*-key.pem            <-- Those have in the name ".key" or "-key" are "Private Key"


> cat aaa.csr | base64             <-- it will encode the Certificate but print the certificate in multiple-lines 
> cat aaa.csr | bswe64 -w 0        <-- it will endcode the certificate and print the certificate as a Single-line




Note: Their are three types of Certificates:

Root certificates:     <-- These are configured on the "CA" (Certificate Authority)
Server Certificates:   <-- These are configured on the Servers 
Client Certificates:   <-- These are configured on the Clints  


---------------------------------------------------------------------
"Servers Components" in Kubernetes Cluster
-------------------------------------------
These below are three server and should have Server-Certificates

1) Kube-API Server 
2) ETCD     Server 
3) Kubelet  Server 


"Client Components" in Kubernetes Cluster
-----------------------------------------
These below are clients and should have Client-Certificates 

1) Kube-Scheduler           <-- It is a Client for Kube-api server, so we generate a Client-Certificate for it 
2) Kube-Controller-Manager  <-- It is a Client for "kube-api" server, so we generate a Client-Certificate for it 
3) kube-proxy               <-- It is a client for "kube-api" server, so we generate a Client-certificate for it. 






CERTIFICATE AUTHORITY ( CA )
----------------------------

> openssl genrsa --out ca.key 2048                                              <-- ca.key     it Generatey-key
> openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA"  -out ca.csr           <-- ca.csr     It generate "Certificate-Signing-Request"
> openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt                      <-- ca.crt     It generate "Sign-Certificates"


ADMIN USER:
-------------
> openssl genrsa -out admin.key 2048                                                      <-- admin.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin/O=system:masters" -out admin.csr   <-- admin.csr  It generate "Certificate-Signing-Request"
                                                                                           "O=system:masters"  <-- with this we are saying that this is not 
                                                                                                                  a normal user, it is a admin-user
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out ca.key                          <-- ca.crt     It generate "Sign-Certificates"

KUBE SCHEDULER
-------------
> openssl genrsa -out scheduler.key 2048                                            <-- scheduler.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin" -out scheduler.csr          <-- scheduler.csr  It generate "Certificate-Signing-Request"
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out scheduler.key             <-- scheduler.crt  It generate "Sign-Certificates"


KUBE CONTROLLER MANAGER
-----------------------
> openssl genrsa -out controller-manager.key 2048                                      <-- controller-manager.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin" -out controller-manager.csr    <-- controller-manager.csr  It generate "Certificate-Signing-Request"
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out controller-manager.key       <-- controller-manager.crt  It generate "Sign-Certificates"

KUBE PROXY
----------
> openssl genrsa -out kube-proxy.key 2048                                      <-- kube-proxy.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin" -out kube-proxy.csr    <-- kube-proxy.csr  It generate "Certificate-Signing-Request"
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out kube-proxy.key       <-- kube-proxy.crt  It generate "Sign-Certificates"


---------------------------------------------------------------------------------------------------------
> cat  /etc/kubernetes/manifests/kube-apiserver.yaml 

> openssl x509 -in <path to xxx.crt>  -text -noout                        
> openssl x509 -in /etc/kubernetes/oki/apiserver.crt -text -noout                <-- like this we can get all the info about the cretificate.
                                                                                   With "-noout" it will not create a new file with the Certificate 
                                                                                   it will just print the Certificate info on the console  
  
> k get csr                                         <--- to see all the "Certificate Signing Requests"
> k certificate approve <name-of-Certificate>       <-- with this we can approve the Certificate
> k get csr <name-of-certificate> -oyaml            <-- like this we can see the details inside the certificate 

> k certificate deny <name-of-certificate>          <-- like this we can deny a "Certificate Signing Request" 

> k delete csr <name-of-certificate>                <- in last step first we "deny" the request and after that we delete the CSR request. 










How we can make a REST call to "KUBE-API" server with these admin Certificates and keys ? 

> curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt    <-- This is the example of REST-Api call that we make 
                                                                                                       to the "KUBE-API" server.
  






#######################################################################################################
kubeconfig
----------

./kube/config            <-- be default "kubectl" look for the "config-file" under this folder 


> kubectl config -h                                   <-- like this we can list all the commands  for "config"

> kubectl config view                                 <-- with this we can view the config 

>> kubectl config use-context abc@production          <-- like this we are setting the default config to "abc@production" 

> kubectl config --kubeconfig=my-base-config use-contexts dev-frontend   <-- if we have two "./kube/config" files then we can add 
                                                                            the file name like this "--kubeconfig=my-base-config"

> k config use-context research --kubeconfig <path-to-file>       <-- "--kubeconfig" this attribute is used when we put the .config file 
                                                                     in anyother location rather then ./kube/config

> mv /root/my-kube-config   /root/.kube/config           <-- like this we can move from other place to default folder  the .config file

Steps to create Context file
----------------------------

Step-1) Add Cluster details
> kubectl config --kubeconfig=base-config set-cluster development --server=https://1.2.3.4           <--"my-base-config" is the file name for config

Step-2) Add user details
------------------------
> kubectl config --kubeconfig=base-config set-credentials my-user-a --username=dev --password=my-pass       <-- "my-user-a" is the name of the user for this config file.

Step-3) Setting Context 
> kubectl config --kubeconfig=base-config set-context dev-frontend --cluster=development --namespace=frontend --user=my-user-a




###################################################################################################################
RBAC    Role, Rolebinding, ClusterRole, ClusterRolebinding
----

Core API Group:  
---------------
Pods, Nodes, Namespaces, Services, Configmaps, Secrets, Events, Endpoints, PV, PVC 

Named API Groups:
-----------------
apps:           Deployments, ReplicaSets, StatefulSets, DaemonSets, DeploymentsRollback
extensions:     Ingress, NetworkPolicies, PodSecurityPolicies
batch:          Jobs, CronJobs
storage.k8s.io: StorageClass, VolumeAttachment
policy:         PodDisruptionBudgets



Role:
-----

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]                       <--"" indicates the core API group, Not the "Named Api Group" (imp)
  resources: ["pods"]
  verbs: ["get", "watch", "list"]



apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane                                  <-- "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role                                    <-- this must be Role or ClusterRole
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

> cat /etc/kubernetes/manifests/kube-apiserver.yaml       <-- to view "api-server" options where cluster is created with "kubeadm" tool     
> ps -aux | grep authorization 





> k auth can-i --help          <-- it will give the example of all the command that the user that i logged-in can execute 

> k auth can-i '*' '*'


> kubectl auth can-i  create deployments             <-- like this we can check if the current user is alloweed to "Create Deployments"

> kubectl auth can-i create pods                     <-- like this we can check if the current user is alloweed to "Create Pod"       

> kubectl auth can-i create pods --as dev-user       <-- like this we can check the permission for "dev-user" if he is alloweed or not 

> kubectl auth can-i create pods --as dev-user --namespace  my-ns-a  <-- like this we can check it "dev-user" has permission to create pod in the
                                                                             "my-ns-a" Namespace

> kubectl create role my-role --verb=list,create,delete --resource=pods 

> kubectl create role my-role --verb=list,create,delete --resource=pods --resourceNames=pod-a,pod-b   <-- this role is applied only for "pod-a" and "pod-b"

> kubectl create rolebinding my-rolebinding --role=my-role --user=dev-user




--------------------------------------------------------------------------------------
ServiceAccount 
--------------

serviceAccountName:      <-- this is the name of attribute that we should put under Deplyoment -> template -> spec -> serviceAccountName: xxx


spec:
  automountServiceAccountToken: false     <-- normally kubernetes automatically add the "default" serviceaccount to the pod, but 
                                             with this field we can define that the kubernetes should not add any "service-account" 
                                            token by itself. 



imagePullSecrets:       <-- this we have to put under Deployment -> template -> spec -> imagePullSecrets: -> -name: xxxx    

Note: 
-) if we create new attribute in a Deployment for serviceAccount then it automatically recreate the pods with new SA 
-) But if we directly change the SA inside a pod then, we have to recreate the pod, then only it effects, so better to put 
inside the pod.yaml file 


> kubectl create serviceaccount my-sashboard-sa

> kubectl create token <service-account-name>            <-- like this we can create a Token for a existing "service-account"

> k create secret docker-registry -help        <-- "docker-registry" is  a type of special secret that we should create to pull images from DockerHub  
                                                  like this we can get help for creating a secret for pulling Image from DockerHub



------------------------------------------------------------------------------------------
Security contexts:
------------------

> whoami                                 <-- i return that i am a "root" user or any other "user" on ubuntu 
 
> k exec my-pod -- whoami                <-- here we are checking which user permission i have inside the pod "my-pod" 



Pod level Security context
---------------------------
Pod -> spec -> securityContext -> runAsUser: 1010     <-- Pod level "securityContext"
                                                         like this we can forcefully say the pod to run all the process not as root-user 
                                                        it will run all the process as normal "user" with Id "1010"


Container level Security context
--------------------------------
Pod  -> spec -> containers -> securityContext -> runAsUser: 1010     <-- As we are defining the attribute "securityContext" under "containers"
                                                                        it is then a "container level" security context

SYS_TIME capability:
-------------------
pod -> spec -> containers -> securityContext -> capabilities -> add -> ["SYS_TIME"]        <-- note that the pod must run as "root user" then only it works
 

NET_ADMIN capability:
-------------------
pod -> spec -> containers -> securityContext -> capabilities -> add -> ["NET_ADMIN"]        <-- note that the pod must run as "root user" then only it works

-------------------------------------------------------------------------------------
Network Policies:
----------------
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db                                     <-- this is the pod to which the other Pods trying to access, 
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock: 
            cidr: 172.17.0.0/16                    <-- IP of the pod that from which we trying to access the main-pod 
            except:
              - 172.17.1.0/24
        - namespaceSelector:                      <-- Namespace in which all the pods that can access the main-pod
            matchLabels:
              project: myproject
        - podSelector:                             <-- the pod form which we want to  access the main-pod. This Pod should be in the same namespace that 
            matchLabels:                               the main-pod is having
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:                           <-- this is the IP that the main-pod can send request to 
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978
    - to:          
        - namespaceSelector:                      <-- Namespace to which the main-pod can send request to 
            matchLabels:
              project: myproject
      ports:
        - protocol: TCP
          port: 5978              
    - to:                        
        - podSelector:                             <-- Pod that the main-pod can send the request
            matchLabels:                          
              role: fronten
      ports:
        - protocol: TCP
          port: 5978

> k get networkpolicies 
> k get netpol my-network-policy 


######################################################################################################################
Storage:
-------



====================================================================================================
Volume Examples:
----------------

1) Storing Data directly from Pod in the 

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - image: alpine
      name: alpine
      command: ["/bin/sh", "-c"]
      args: ["shuf -i 0-100 -n l >> /my-storage-path/endnumber.out;"]
      volumeMounts:
        - mountPath: /my-storage-path        <-- (1) we are creating a "Volume" with path "/my-storage-path" in the container. 
          name: my-data-volume                       The service running on the pod will send logs to /my-storage-path folder.
  volumes:                                           Note: we can have two different path names but the "Volume" name must be same
    - name: my-data-volume
      hotstPath:
        path: /data                <--(2) Any data that comes from Pod in the "/my-storage-path" that will be stored at the end in the 
        type: Directory                    "/data" folder on the Node, as we defined "type: Director" so it create a folder on the Node



2) Create a Storage with AWS-EBS

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - image: alpine
      name: alpine
      command: ["/bin/sh", "-c"]
      args: ["shuf -i 0-100 -n l >> /my-storage-path/endnumber.out;"]
      volumeMounts:
        - mountPath: /my-storage-path        <-- (1) we are creating a "Volume" with path "/my-storage-path" in the container. 
          name: my-data-volume                       The service running on the pod will send logs to /my-storage-path folder.
  volumes:                                           Note: we can have two different path names but the "Volume" name must be same
    - name: my-data-volume
      awsElasticBlockStore:
        volumeID: <ID-Nummber>                <--(2) Any data that comes from Pod in the "/my-storage-path" that will be stored at the end in the 
        fsType: ext4                          "/data" folder on the Node, as we defined "type: Director" so it create a folder on the Node


=====================================================================================================================
Persistent Volume:
-----------------

Commands:

> kubectl get pv
> kubectl get pvc
> kubectl delete pvc my-claim-name
> 


-) Their are three types of "PersistentVolume" modes
   - RWO--ReadWriteOnce        <-- the volume can be mounted as read-write from pods placed in a single node. ReadWriteOnce access mode still can    
                                    allow multiple pods to access the volume when the pods are running on the same node.
   - ROX--ReadOnlyMany         <-- the volume can be mounted as read-only from pods placed in many nodes.
   - RWX--ReadWriteMany        <-- the volume can be mounted as read-write from pods placed in many nodes.
     RWOP--ReadWriteOncePod    <-- 

-) The "accessModes" of both PV and PVC should be match, then only PVC can claim a PV 

>) persistentVolumeReclaimPolicy: Retain   <-- After we delete a PVC, the attached PV will be remain their till we manually delete it, 
                                               It can not be re-used by any other Claims.
>) persistentVolumeReclaimPolicy: Delete   <-- when we delete the PVC the attached PV will also be delete at the same time

>) persistentVolumeReclaimPolicy: Recycle   <-- In this case when a PVC is deleted, the data stored in the PV will also be deleted,
                                              But the PV remain their, and after this recycle process the PV can be claimed by any other PVC 
                                               

Step-1) Create "Persistent Volume"

pv.yaml
--------
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: my-per-volume
  labels:
    name: my-pv
spec:
  accessModes:
    - ReadWriteOnce        
  capacity:
    storage: 1Gi
  persistentVolumeReclaimPolicy: Retain  
  hostPath:
    path: /data     
    type: Directory                    <-- "type: Directory" it is optional, if we use "hostPath" then it is automatically taken



pv.yaml
--------
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: my-per-volume
  labels:
    name: my-pv                    <-- here we can define a lable in the PV
spec:
  accessModes:
    - ReadWriteOnce        
  capacity:
    storage: 1Gi
  persistentVolumeReclaimPolicy: Delete  
  awsElasticBlockStore:
    volumeID: <ID-Nummber>  
    fsType: ext4 


Step-2) Create "Presistent Volume Claim"

pvc.yaml
--------
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: my-claim-aaaa
spec:
  selector:
    matchLabels:
    name: my-pv                       <-- As we have define the "Selector" then it search for "PV" with this lable and attach to it
  accessModes:
    - ReadWriteOnce        
  resources:
    request:
      storage: 500Mi



Step-3) Attach the above created "PVC" in the Pod.yaml below

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim-aaaa

----------------------------------------------------------------------------------------------------------
StorageClass:
------------

> k get storageclass 
> k get sc 




StorageClass can use three type of Disks that we define under "Parameters"

Silver StorageClass:               Gold StorageClass:          Platinum StorageClass:
-------------------                -----------------           ----------------------
type: pd-standard                  type: pd-standard           type: pd-ssd
replication-type: none             replication-type: none      replication-type: none 


Note:
volumeBindingMode: WaitForFirstConsumer  or  Immediate

Step-1) Create a "StorageClass"

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: my-storage-class-bb
provisioner: ebs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer 
parameters:
  type: pd-standard 
  replication-type: none  


Step-2) Create PVC 

pvc.yaml
--------
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: my-claim-aaaa
spec:
  storageClassName: my-storage-class-bb
  accessModes:
    - ReadWriteOnce        
  resources:
    request:
      storage: 500Mi


Step-3) Attach the above created "PVC" in the Pod.yaml below

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim-bb


---------------------------------------------------------------------------------------
gp2
---


apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp2
parameters:
  fsType: ext4
  type: gp2
provisioner: kubernetes.io/aws-ebs
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer



gp2-pv.yaml
-----------
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    kubernetes.io/createdby: aws-ebs-dynamic-provisioner
    pv.kubernetes.io/bound-by-controller: "yes"
    pv.kubernetes.io/migrated-to: ebs.csi.aws.com
    pv.kubernetes.io/provisioned-by: kubernetes.io/aws-ebs
  creationTimestamp: "2023-02-02T12:37:29Z"
  finalizers:
  - kubernetes.io/pv-protection
  labels:
    topology.kubernetes.io/region: eu-west-1
    topology.kubernetes.io/zone: eu-west-1b
  name: pvc-7da6e25e-3727-4647-93a3
spec:
  accessModes:
  - ReadWriteOnce
  awsElasticBlockStore:
    fsType: ext4
    volumeID: aws://eu-west-1b/vol-091a7c498
  capacity:
    storage: 10Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: kong-data-stolon-stolon-keeper-0
    namespace: rwip-api-gateway
    resourceVersion: "1518093054"
    uid: 7da6e25e-3727-4647-93
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - eu-west-1b
        - key: topology.kubernetes.io/region
          operator: In
          values:
          - eu-west-1
  persistentVolumeReclaimPolicy: Delete
  storageClassName: gp2
  volumeMode: Filesystem



gp2-pvc.yaml
------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    pv.kubernetes.io/migrated-to: ebs.csi.aws.com
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    component: stolon-keeper
    stolon-cluster: kube-stolon
  name: kong-data-stolon-stolon-keeper-0
  namespace: rwip-api-gateway
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: gp2
  volumeMode: Filesystem
  volumeName: pvc-7da6e25e-3727-4647-93a3


----------------------------------------------------------------------------
gp3
---

gp3-strageclass.yaml
--------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3
parameters:
  type: gp3
provisioner: ebs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer



gp3-pv.yaml
-----------
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: ebs.csi.aws.com
    volume.kubernetes.io/provisioner-deletion-secret-name: ""
    volume.kubernetes.io/provisioner-deletion-secret-namespace: ""
  creationTimestamp: "2023-07-17T11:15:33Z"
  finalizers:
  - kubernetes.io/pv-protection
  - external-attacher/ebs-csi-aws-com
  name: pvc-12a2cb92-17bd-4b89-8d61-40c385f195f1
  resourceVersion: "1825656404"
  uid: a695fe60-d71c-4465-8d4b-050293dde96b
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 32Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: kubecost-cost-analyzer
    namespace: kubecost
    resourceVersion: "1825656273"
    uid: 12a2cb92-17bd-4b89-8d61-40c385f195f1
  csi:
    driver: ebs.csi.aws.com
    fsType: ext4
    volumeAttributes:
      storage.kubernetes.io/csiProvisionerIdentity: 1689254055136-8081-ebs.csi.aws.com
    volumeHandle: vol-0d0ae1e54589b092c
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: topology.ebs.csi.aws.com/zone
          operator: In
          values:
          - eu-west-1a
  persistentVolumeReclaimPolicy: Delete
  storageClassName: gp3
  volumeMode: Filesystem
status:
  phase: Bound


gp3-pvc.yaml
-----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    volume.beta.kubernetes.io/storage-provisioner: ebs.csi.aws.com
    volume.kubernetes.io/selected-node: ip-11-240-144-149.eu-west-1.compute.internal
    volume.kubernetes.io/storage-provisioner: ebs.csi.aws.com
  creationTimestamp: "2023-07-17T11:15:29Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app: cost-analyzer
    app.kubernetes.io/instance: kubecost
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: cost-analyzer
    helm.sh/chart: cost-analyzer-1.104.1
  name: kubecost-cost-analyzer
  namespace: kubecost
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 32Gi
  storageClassName: gp3
  volumeMode: Filesystem
  volumeName: pvc-12a2cb92-17bd-4b89-8d61-40c385f195f1


----------------------------------------------------------------------------
EFS
---

efs-storageclass.yaml
---------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: Immediate


efs-pv.yaml
-----------
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    volumeName: efs-grafana
  name: efs-grafana
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 10Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: kube-prometheus-stack-grafana
    namespace: monitoring
    resourceVersion: "2090018606"
    uid: 2f3fce13-35d0-4b2e-a597-5b9da361ff6d
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-0ef27c45cfe3f5b9e
  mountOptions:
  - tls
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  volumeMode: Filesystem


efs-pvc.yaml 
------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 10.0.3
    helm.sh/chart: grafana-6.50.2
  name: kube-prometheus-stack-grafana
  namespace: monitoring
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: efs-sc
  volumeMode: Filesystem
  volumeName: efs-grafana




#################################################################################################################################
Networking:
----------

----------------------------------
> route  
> ip route           <-- both of these command we use to see the "routing-table" entries 
-----------------------------------

> ip link    <-- to list and modify interface on the host.

> ip addr    <-- to see the ip-addresses link to above interfaces.

> ip addr add 192.168.1.10/24 dev eth0         <-- like this we can set ip-address on a Interface. This command we have to execute on linux machine 
                                                  Note: all the changes made above are valid only till a new re-start. All will be deleted.
                                                  If we want to presist the changes, then we have to put then in the Network-interface file.

> i route add 192.168.2.0/24  via  192.168.1.1

> ip route add default via  192.168.2.1

> pin <any ip>                         <-- if the ip address is reachable then it return result,

Node-A             Node-B           Node-C      
Network-A  -->   Network-B  -->   Network-C          <-- these are the networks created on every Node.

> echo 1 > /proc/sys/net/ipv4/ip_forward        <-- Execute this command on the Node-B, after doing this we can send a packet from 
                                                    Node-A to Node-c  through the Node-B, As we have done the "ip_forward" 
                                                    this command will make in the file "/etc/sysctl.conf = 1" file 

> cat /proc/sys/net/ipv4/ip_forward             <-- if it return  "0" it means no ip_forward is set for this Node or linux machine.



-------------------------------------------------------------------------------------
DNS Server:
----------

> cat >> /etc/hosts    <-- in this file add   ->   192.168.1.11  db      <-- this is file is placed on every linux machine.


Assume we have a DNS service with ip - 192.168.1.100

> /etc/resolv.conf     <-- on every Linux machine have a file "resolv.conf" in it just add the "DNS" server ip, then when we do a ping then 
                           it just check the ip in DNS server and return the result.

> cat /etc/nsswitch.conf             <-- here we can define, which DNS server, the host first look, it should search in local-file with DNS entry 
                                       or the call first go to any extern DNS server 
--------------------------------------------------------------------------------------------
Domain Names 
-------------

below is example of "www.google.com" 

.                             <-- Root 
.com, .net, .edu, .de         <-- these are Top Level Domain Name
google                        <-- This is domain name assign to google
www                           <-- Subdomain



-----------------------------------------------------------------------------------------------
Record Types  in Route 53:
-------------------------

A-Records           <-- mapping ipv4  
AAAA                <-- mapping ipv6
CNAME               <-- mapping  food.web  to  my-web.com, It is name-to-name mapping


--------------------------------------------------------------------------------
> nslookup                        <-- if we put a entry in local "/etc/resolv.conf" file, "nslookup" does not check this file
                                     It directly check and send request to DNS server 

> dig                            <-- if we put a entry in local "/etc/resolv.conf" file, "nslookup" does not check this file
                                     It directly check and send request to DNS server 

> ps aux                                        <-- when we we run this command on the pod, then it will show the process running on the pod
> ps aus                                        <-- when we run this command on the node, then it show all the process running on the whole node.


> ip netns add my-interface-a                   <-- with this we can create a new "Network-Namespace" on a Linux-host or we can say linux-machine 
> ip netns                                      <-- like this we can list all the Network-namespace available on the linux-host 

> ip link                                       <-- to like all the interfaces on the linux-host.

> ip netns exec <network-namespace-name> ip link         <-- like this we can see which Network interface that is defined inside the "Network-namespace"
OR
> ip -n <namesapce-name> link                        <--both of these commands are same 

> arp                                           <-- we can see entried in the linux-host, this show the list of other hosts that we have made connection 

> ip netns exec <namespace-name> arp             <-- this command runs now inside the given "namespace-name"

> route                                        <-- like this we can get the entries from the routing-table on the linux-host 

> ip netns exec <namespace-name> route          <-- like this we will get the routing-table entrie from the given namespace 

> ip address                                     <-- we can get the "Network-interface" and "MAC-address" that is attached to pod or Node

> ip address show <interface-name>             <-- e.g  eth0  <-- it can be a interface name, then it will give us the info about it.

> ip address show type bridge                  <-- it can show us all the "Bridge-network-interface" on the Machine, Node or Pod.

> ip route                                      <-- it will give info about all the routes that are defined.

> ip route add 192.168.1.0/24  via 192.168.2.2

> netstat -plnt 

> route 

> netstat  --help 

> ip link add <any-cable-endpoint-name> type veth peer name <any-cable-endpoint-name>        <-- like this we create two endpoints with a attachment 

> ip link add <cable-endpoint-name> netns <namespace-name>                       <-- like this we can attach one side of the cable-endpoint to the namespace 

> ip link add <cable-endpoint-name> netns <namespace-name>                       <-- like this we can attach other side of the cable-endpoint to the other namespace 

> ip -n <namespace-name> addr add 192.168.15.1 dev <cable-endpoint-name>         <-- like this we can define a ip-address to a namespace 

> ip -n <namespace-name> link set <cable-endpoint-name> up                       <-- like this we are activating the links connection 

> ip link add <any-bridge-name> type bridge                                      <-- like this we can create a network-bridge on a linux-machine 

> ip netns                                                                   <-- to get all the namesapces from a "Docker-host"

 
> ip address show type bridge                                <-- it will show all the "bridge" interface on the machine 



#####################################################################################################################
CNI    (Container network interface)
----
-) The main purpose of the CNI is when a new "Pod" is created then it should assign a "IP" address and also create connections to "Network-bridge"
   that a pod can call any other po or service.

-) kubelet.service          <-- in this file we can see the 
                                "--network-plugin=cni"            <-- which type of newtork-plugin we have defined 
                                "--cni-bin-dir=/opt/cni/bin"      <-- in this directory we see all the supported cni-plugins as executable files e.g  "bridge" "dhcp" "flannel"
                                "--cni-conf-dir=/etc/cni/net.d"   <-- in this location we can find a "configuration" file, this files is used by "kubelet" to know which plugin to be used. 

> ps -aux | grep kubelet       <-- like this we can also see all the configuration about the "CNI" defined in the "kubelet"


> cd /etc/cni/net.d/<name-of-file>               <-- here is the file that is executed by the CNI every time when a new container is created. 


> kubectl logs <wave-net-ere> -n kube-system       <-- As in this example we have "wave" as CNI installed, in the logs we can see 
                                                      a attribute "ipalloc-range:10.x.x.x/x"  <-- from here we can findout the defined 
                                                      range for the  ip-address that will be allocated to every pod. 


--------------------------------------------------------------------------------------------------------------------
Kube-proxy: 
-----------
-) When we create a new "Service" then automatically "kube-proxy" will be executed and it prove a ip address to the newely created "Service" 

> ps aux | grep kube-api-server       <--    in this process or directly in the "kube-apiserver.yaml  file we can see 
                                             a attribute "--service-cluster-ip-range=x.x.x.x/x" from here we can find that 
                                             under which range our cluster is alloweed to provide automatically a ip address 
                                             to the "Service" when it is created 

> iptables -L -t nat | grep <service-name>        <-- like this we can find the "Ip address" that is assigned to the service 

Logs:
----
> /var/log/kube-proxy.log                    <-- from here we can see all the logs that are produced by the "kube-proxy" 





##########################################################################################################
Services & kube-api-server & Kube-proxy   (Vid  223: Service-Networking)
----------------------------------------

kube-api-server --service-cluste-ip-range ipNet (Default: 10.0.0.0/24)

> ps aux | grep kube-api-server               <-- with this we get a output that from-till which range "kube-proxy" alloweed
                                                 to set ip ranges for Services.
                                        Output: kube-apiserver --authorization-mode=Node,RBAC --service-cluster-ip-range=10.96.0.0/12   

> iptables -L -t nat | grep my-db-service        <-- this commmand outputs the ip-address defined for the service and it also shows
                                                    the ip address that is for the pod that is mapped to this service 

> cat /var/log/kube-proxy.log                 <-- it will give us all the logs, how the kube-proxy assign all the ip to service and to pod 
                                                  and mapping them.


########################################################################################################
CoreDNS
-------

-) When a Service is created, then a ip-address is assigned to it, After Service is created then a entry in the CoreDNS table
   is made with the Name of the "service-map" to ip of the service, so when any pod send a request to the Name of the service then
   the request is forwarded to the ip of the service.
   

##############################################################################################################
Ingress:
---------
> k create ingress --help

#################################################################################################################

Check on the Master Node:
------------------------
> service kube-apiserver status 
> service kube-controller-manager status
> service kube-scheduler status 

Logs on Master:
--------------
> k logs kube-apiserver-master -n kube-system 

> sudo journalctl -u kube-apiserver 

Check on the Worker Node:
-----------------------
> service kubelet status 
> service kube-proxy status 




#####################################################################################################################
Json Path queries
-----------------

> k get pods -o json                                                   <-- to get the json output of the pods.

> k get nodes -o=jsonpath='{.items[*].metadata.name}'                                         <-- it use "-o=jsonpath="

> k get nodes -o=custom-columns=ANY-NAME:.metadata.name                                       <-- like this we can create output with custom "Columna-name" 

> k get nodes -o=custom-columns=ANY-NAME:.metadata.name, ANY-NAME:.status.capacity.cpu 

> k get nodes --sort-by=.metadata.name 

> k get nodes --sort-by=.status.capacity.cpu  

> k get nodes -o=jsonpath='{.items[*].metadata.name}{.items[*].status.capacity.cpu}'   <-- master node 4 4        Assume we have two nodes 

> k get nodes -o=jsonpath='{.items[*].metadata.name}{"\n"}{.items[*].status.capacity.cpu}'       <-- "\n"  for "New-line"

> k get nodes -o=jsonpath='{.items[*].metadata.name}{"\t"}{.items[*].status.capacity.cpu}'       <-- "\t"  for one "tab" space 

> k get nodes -o=jsonpath='{range .items[*]}{.metadata,name}{"\t"}{.status.capacity.cpu}{"\n"}{"end"}'    

> k get nodes -o=custom-columns=NODE-A:.metadata.name

> k get nodes -o=custom-columns=NODE-A:.metadata.name,CPU-A:.status.capacity.cpu 


############################################################################################################
Helm
----

> cat /etc/*release*                                            <-- to get the information about the Operating-system installed on the machine e.g "ubuntu" 

> helm --help                                                   <-- it will print all the sub-commands that we can use with "helm"

> helm version 

> helm --debug                                                   <-- to enable "verbose" option 


------------------------------------------------------
> helm install  <my-chart-name> <chart-name-from-provider>       <-- it will pull the "helm-chart" and "install" it also 



> helm pull --untar <chart-name-from-provider>                    <-- it will only pull the "helm-cart" and store it locally.

> ls <chart-name-from-provider>

> helm install <my-chart-name>  ./path-to-chart                    <-- like this we can install the above "pulled" chart           
> helm uninstall <my-char-name>                                 <-- to uninstall the package 

OR

> helm repo add <my-name>  <link-for-repo> 
> helm repo add bitnami  https://charts.bitnami.com/bitnami 
 


> helm repo list                           <-- like this we can find all the manually "added" repos                                
---------------------------------------------------------

                "Check the difference between these two commands" 

> helm list                                <-- to list all the available packages. 



> helm search repo <name> 
> helm search repo bitnami                   <-- if we have added "repo" manually then like this we can search them. 
                               App-version   <-- with this "helm search repo" command we can check the "APP VERSION"
                               CHART VERSION <-- with this "helm search repo" command we can check the chart-version 


Search in the Artifact-hub:
----------------------------
> helm search hub <chart-name-from-provider>                         <-- to search the chart with provider-name  

> helm search hub <my-chart-name>                                   <-- to search the chart with <my-name> 




















###################################################################################################################
config    
------------------------
1) Write all the contexts added in you .kube/config file, with "kubectl" command /config.txt 

> k config view                             <-- to view the config that are deifined in the .kube/config file 

> k config view --raw                       <-- sometime if we do not saw any Certificates in the config file then use this "--raw" command to see the content 

> kubectl config get-contexts  > /config.txt       <-- to get all the contexts 

> kubectl config current-context                 <-- to get the current context 

> kubectl config use-context <context-name>       <-- Command to switch to a specific context 

> cat ./kube/config | grep current-context         <-- like this we can print current-context without using kuebctl 

> kubectl --context=<context-name> <command>     <-- like this we can also run a command by using a special context 

> k config user-context <any-name> --kubeconfig /abc/config                               <-- if the  "config" file is placed at any other location.

###########################################################################################################################
CKS Exam:
---------


--------------------------------------------------------------------------------------
Question-1) 

a) 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-1
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress                                     <-- this NP will block all the incoming-outgoing traffic from all the pods in the "default" namespace 



b) 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-2
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: pod-1
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          run: pod-2                          <-- it will allow traffic from "pod-1" to "pod-2"


c) 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-3
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: pod-2
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          run: pod-1                      <-- it allows "pod-2" to get incoming traffic from "pod-1"


d) 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-4
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP                     <-- It allows all the pods in default namespae to send traffic to "Port-53" for creating   
                                           DNS Resolution.


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-5
  namespace: prod
spec:
  podSelector:
    matchLabels:
      run: pod-3
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: default      <-- As pod-1 is in the "default" namespace so we have to define also the label that is defined in the prod namespace 
    - podSelector:
        matchLabels:
          run: pod-1 


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-6
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: pod-1
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          ns: prod                               <-- As pod-3 is in the "prod" namespace so we have to define also the label that is defined in the prod namespace 
    - podSelector:
        matchLabels:
          run: pod-3          



> k run pod-1 --image=nginx 
> k run pod-2 --image=nginx 

> k expose pod-1 --name=svc-pod-1 --port=80           <-- as we are testing with "curl" command so here we must use "--port=80"

> k expose pod-2 --name=svc-pod-2 --port=80

> k exec pod-1 -- curl svc-pod-2                       <-- like this we can test 

> k exec pod-1 -- curl svc-pod-3.prod.svc.cluster.local

-----------------------------------------------------------------------------------------------------------------
Kubernetes Dashboard:
---------------------

Question:  Make the kubernetes Dashboard publicily available.

Step-1)
> k edit deploy kubernetes-dashboard -n kubernetes-dashboard 

spec:
  containers:
  - args:
    - --auto-generate-certificates          <-- remove this argument, Normally it create ssl certificate automaticalls, As we want to make the 
                                                dashboard unsecure, and available it publicly then we have to remove this.
    - --insecure-port=9090                  <-- add this argument, it will open the insecure port for the Dashboard.
    livenessProbe:
      httpGet:
        path: /
        port: 9090  # 8443                 <-- Either we have to remove the "livenessProbe" completely, or we can just change the "port"
        scheme: HTTP                           with both of these changes it will work.

Step-2) 
> k edit svc kubernetes-dashboard -n kubernetes-dashboard 

spec:
  ports:
  - port: 9090    # 443                    <-- change the "port" from  443  to 9090 
  - targetPort: 9090  # 8443               <-- change the "targetPort" from 8443  to  9090
  
  type: NodePort   # ClusterIp             <-- change the "type" from ClusterIp to "NodePort"


Step-3)
> get clusterrole -A | grep view         <-- their is a predefined clusterrole "view" , This clusterrole allows us to view all the see all the resources in the cluster 

> k create rolebinding my-name --serviceaccount=kubernetes-dashboard:kubernetes-dashboard --clusterrole=view -n kubernetes-dashbaord  <-- it will attach a new crb with the view role
                                                                                                                                        now we will have access to all the resources in the kubernetes-dashboard namespace 

> k create clusterrolebinding my-name --serviceaccount=kubernetes-dashboard:kubernetes-dashboard --clusterrole=view -n kubernetes-dashbaor  <-- here we are creating a "ClusterRoleBinding" with this it can 
                                                                                                                                             view all the resources in all the namespaces.   

Note: We can attach a "ClusterRole" with a "RoleBinding" but the "User" or "ServiceAccount" will have only the acces to the resources only that the defined in the Namespace 
      om which the "RoleBinding" is created 
      But When we create a "ClusterRole" with the "ClusterRoleBinding" in this case  "User" or "ServiceAccount"  will have access to all the resources in any namespace.

      

###########################################################################################################################      
Ingress:
-------

Step-1)
> https://kubernetes.github.io/ingress-nginx/deploy/          <-- from this website take the below .yaml deployment 


> kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml


Step-2)
> k edit svc ingress-nginx-controller -n ingress-nginx                <-- chnage the "type: LoadBalancer"  to  "type: NodePort"

> k get nodes -owide            <-- take the INTERNAL-IP  of the "Controlplane" Node 

> k get svc -n ingress-nginx    <-- take the "Port" of "NodePort Service the "ingress-nginx-controller"

> curl Node-IP:NodePort-port    
> curl 172.30.1.2:32411              <-- this call will return    404         <-- it means connection is created successfully 

> k run pod-1 --image=nginx
> k run pod-2 --image=httpd

> k expose pod pod-2 --port=80 --name=service1
> k expose pod pod-2 --port=80 --name=service2



##################################################################################################################################
Their is a IP 169.254.169.254  of the Node, and a user can from a pod call this IP address and get the internal info through this IP
How to block a Pod that it can not make a "curl" command call from the pod.

Step-1)
> curl http://169.254.169.254/v1/metadata/public-ipv4
> curl http://169.254.169.254/v1/metadata/hostname                  <-- like this we can make a "curl" command from a pod to get the internal 
                                                                        info from Hardware or server.

Step-2)

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:{}
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0                       <-- here we are allowing all the IP, that our pod can call any ip address 
        except:
        - 169.254.169.254/32              <-- here we are overwriding the above, that only this ip will be blocked, rest are alloweed.


###############################################################################################
kube-bench:
----------

How to check the security-loops in the kubernets cluster ?

Step-1)
> https://github.com/aquasecurity/kube-bench
> https://github.com/aquasecurity/kube-bench/blob/main/job.yaml        <-- open this file.


Step-2)
> vi job.yaml                  <-- copy paste this in a new .yaml file and create a new pod 

Step-3)
> k logs <name-of-above-pod>        <-- like this we get the logs of the above create pod, and one-by-one complete all the veneruability findings.


###################################################################################################
Download the newer version of the "kubernetes-binary" and install it 

Step-1)
> k version              <-- first check which version we are using. and take the binary for the next version 


Step-2) 
> kubernetes docs search for  "kubernetes releases" -> click on left side "Release Notes"  ->  click on "ChangeLog" 
                                                    -> then go to which version we need and click on "Source Code" or direct on "Server Binaries" or any other


Step-3)  Right click on the Binary name link and open in a another tab, after that copy that link from browser 
> wget https://dl.k8s.io/v1.29.1/kubernetes-server-linux-amd64.tar.gz   <-- download that link with the "wget" command.
                                                                          it download kubernetes-server-linux-amd64.tar.gz a file like this on node.

Step-4)
> sha512sum kubernetes-server-linux-amd64.tar.gz

The above command give you output as below 
b01f8ef7160045363b017e79f1b3c2c5b54a67aa3426c0e5c45562be88bd72a3cb5e72043764684e10e255ecbb19d697765755031aa3c1b49c4e4c12122b3077  kubernetes-server-linux-amd64.tar.gz 

Step-4) 
In step 3 we have copy the download link, just directly right side of the link their is a binary and it should be same as this above long number 
just compare them. it must be same. like this we can compare both.


Step-5) 
> docker ps | grep kube-apiserver         <-- it prints many folder name copy the first name e.g  "4696401a780b"
> docker cp 4696401a780b:/ container      <-- it copy all the files in a folder name "container" we can give any other name also 
                                              "container" or  "my-container" 
> cd container                            <-- like this we can check all the contentes in this directory 
> cd ..
> find container/ | grep kube-apiserver      <-- "container/usr/local/bin/kube-apiserver"  <-- this is the output and we know the location of it 

> sha512sum container/usr/local/bin/kube-apiserver         <-- like this we will get the "binary address" of the kube-apiserver that is install on the server 
                                                             After that we can match it with our downloaded binary then we can confirm that exact version is installed or not 


#############################################################################################################
RBAC:
----
                                        
> k api-resource --namespaced=true          <-- it return all the resources that can be created under a "Namespace"


> k auth can-i -help                        <-- like this we can get all the command that we can execute with "k auth can-i" 









############################################################################################################
ServiceAccount:
---------------

Step-1)
> k create sa my-sa 

Step-2)
> vi pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: my-pod
  name: my-pod
spec:
  serviceAccount: my-sa 
  containers:
  - image: nginx
    name: my-pod

Step-3)
> k exec my-pod -it -- sh 
# mount | grep secret 
tmpfs on /run/secrets/kubernetes.io/serviceaccount type tmpfs (ro,relatime,size=1928540k)        <-- this is the output of the above command 
# cd /run/secrets/kubernetes.io/serviceaccount         <-- under this folder we will see the token that is generated by the "ServiceAccount" 
# ls 
ca.crt namespace token                    <-- under this "/run/secrets/kubernetes.io/serviceaccount" we can see these three files 


Note: This token we can use when we want to make a call to "Kube-api" server 


Q) When we create a "serviceAccount" and attach it to a pod, then it generate automatically a "token" in the pod at "/run/secrets/kubernetes.io/serviceaccount" 
   With this "token" we can make a call to the "API-Server" and get the secret infos, that we should not have normally, how to disable this "token" 

Their are two ways. 

Way-1) 
--------
apiVersion: v1 
kind: ServiceAccount 
metadata:
  name: my-sa 
automountServiceAccountToken: false            <-- like this "ServiceAccount" will not generate any token, when it attacht to a pod 


Way-2)
apiVersion: v1 
kind: Pod 
metadata: 
  name: my-pod 
spec: 
  serviceAccountName: my-sa 
  automountServiceAccountToken: false           <-- like this "ServiceAccount" will not generate any token, when it attacht to a pod 

Note: With "automountServiceAccountToken: false"   it will not mount the token as a secret to the pod, it will not mount it. so a pod can  
      not use token to make a call to "API-Server" 
      
 
-------------------------------------------
ClusterRole   "edit" 
------------------

Q) How to limit the permissions of a "ClusterRole"

Step-1) 
> k create sa my-sa 

Step-1) 
> k auth can-i delete secret --as system:serviceaccount:prod:my-sa ---namespace=prod    <-- like this we can check if this serviceaccount has permission to delete a secret or not .
                                                                                           by-default it can not delete any secret.
Step-2)
edit         <-- their is a predefine "ClusterRole" that can "edit" every resource in kubernetes 

> k create clusterrolebinding my-crb --clusterrole=edit --serviceaccount=prod:my-sa           <-- i have to define the "namespacename" also 

> k auth can-i delete secret --as system:serviceaccount:prod:my-sa ---namespace=prod         <-- if now we check this after adding "edit" role to serviceaccount, it should have permission to delete 


#########################################################################################################
Cluster Hardening - Restrict API Access
--------------------------------------

Q) How to disable the "Anonymous-auth" that User can not make a call to "Kubeapi-server" 


Step-1)
> curl https://localhost:6443 -k        <-- normally when we make this "curl" then we are able to make a call to "Apiserver" 

Response:
--------
{
  "kind": "Status"
  "apiVersion": "vi",
  "metadata":  {

  }
  "status": "Failure"                                                         <-- eventhough we are becoming "Failure" but still it means that we are able to make call to "ApiServer" 
  "message": "forbidden: User \"system:anonymous\" cannot get path \" /\"",       but we are "Forbidden" it means we have to attach Authorization token with the call. 
  "reason": "Forbidden",
  "code": 403
}


Step-2) 
> vi /etc/kubernetes/manifests/kube-apiserver.yaml 

spec:
  containers:
  - command:
    - --anonymous-auth=false                      <--- when we pass this argument then we are not able to make a call 


> curl https://localhost:6443 -k            <-- if we call now then it will show that we are not able to make any call 

Response
---------
{                
  "kind": "Status"                                   <-- this is the response come from the above call
  "apiVersion": "vi",
  "metadata":  {

  }
  "status": "Failure"
  "message": "Unauthorized"
  "reason": "Unauthorized"
  "code": 401
} 

Note: "--anonymous-auth=true"       <-- by-default it is on "true" mode, so we do not need to add as "true" explicitly, 
                                  But if we make it "--anonymous-auth=false"  then after testing make it true, As ApiServer need for its livenessProbe 
                                   this "anonymous" acces, so either we remove this annotation as we have put it or make "true"


-------------------------------------------------------------------------
Q) How to make a call to "Api-Server" with "curl" command ?

Step-1)
> k config view                       <-- like this we can view the config 
> k config view --raw                 <-- if the certificates are not shown then use this "--raw" command 

Step-2) 
Now create three files from the .kube/config file from 
- certificate-authority-data 
- client-certificate-data 
-

-> form the output of the above command first copy the "certificate-authority-data" it is base64 encoded so first decode like this all three of them 

> echo <paste-base64- certificate-authority-data> | base64 --decode > ca        <-- we can directly create "ca" file without any extension
> echo <paste-base64- client-certificate-data> | base64 --decode > crt 
> echo <paste-base64- client-key-data> | base64 --decode > key 

Note: like this we have create the three files ca,crt,key 


Step-3)
> ip a                           <-- in the result check Nr:2  "eth0" or enp1s0" or any else 
                                    their is an ip address is written "172.30.1.2" or any other-
                                    copy that ip address 
OR 
In the .kube/config  file also  "server: https://172.30.1.2:6443"   <-- from here we can also take the ip address of "Api-Server" 


> curl https://172.30.1.2:6443               <--like this we can make a call to "Api-Server" 

> curl https://172.30.1.2:6443 --cacert ca --cert crt --key key            <-- put all the above created files and execute, we are 
                                                                            able to connect to "ApiServer" 
                                                                      Note: we must use "--cacert ca"  this will give error "--cacert=ca"

----------------------------------------------------------------------------
Q) How to make a "External-access" to the   Kube  "Api-Server" ?

Step-1)
> k get svc        <-- in the beginning by-default a service is create by kuberntes with the name  "kubernetes" as a "ClusterIP"
                      To open the Cluster for external calls, just make this service type from "ClusterIP" to "NodePort
                  Note: copy the port nr of the "NodePort" service 

Step-2)
- Copy the actual  ".kube/config" file, and past in your local machine. And create file with any-name "my-config"   
- in the config file their is "server: https://95.216.172.143:6443"   <-- change the port-nr "6443" to the port nr of the nodeport
  service, it is normally "32278" something like this.

Step-3)
> k get pods --kubeconfig my-config           <-- as we have copy-paste and create the "my-config" file then use it in this command.
                                             Note: As above written we must change the port number of the server to NodePort service.       


----------------------------------------------------------------------------
NodeRestriction AdmissionController:
-----------------------------------

vi etc/kubernetes/manifests/kube-apiserver.yaml 

spec:
  containers:
  - command:
    - --enable-admission-plugins=NodeRestriction    <--normally this plugin is by-default enabled, but if it is not their then put it in the "Api-Server" .yaml file 

Note: 



#######################################################################################################
ETCD:
----


Q) How to read a Secret directly from  "ETCD" Server ?

Step-1)
> k create secret generic my-secret --from-literal=username=sunny 
> k create secret generic my-secret-2 --from-literal=pass=1234 -n prod           <-- i have created two different secrets 

Step-2) Search for "etcd" values
> cat /etc/kubernetes/manifests/etcd.yaml | grep etcd

Output:                                                                        <-- something like this comes in output 
- --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
- --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
- --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key



Step-3) checking if the "etcd" is healthy" optional step 
> ETCDCTL_API=3 etcdctl endpoint health --cert <put-file-end-with-server.crt>            <-- "endpoint health" with this we are checking the health of ETCD 
                                      --key <put-file-end-with-server.key> 
                                      --cacert <put-file-end-with-ca.crt>          <-- put all in one line, just for understanding i do it like this.

Step-4) getting "default namespace  my-secret"
> ETCDCTL_API=3 etcdctl get /registry/secrets/default/my-secret  --cert <put-file-end-with-server.crt>            <-- "/default/my-secret" with this we are getting the secret from "default" namespace 
                                                                 --key <put-file-end-with-server.key> 
                                                                 --cacert <put-file-end-with-ca.crt>


Step-5) getting "prod namespace my-secret-2"
> ETCDCTL_API=3 etcdctl get /registry/secrets/prod/my-secret-2  --cert <put-file-end-with-server.crt>            <-- "/prod/my-secret-2" with this we are getting the secret from "prod" namespace 
                                                                --key <put-file-end-with-server.key> 
                                                                --cacert <put-file-end-with-ca.crt>


----------------------------------------------------------------------------------------------
Q) How to encrypt the existing secret using "aescbc" and a password of our choice ?

Step-1) Search in the documentation "encrypt etcd"

Step-2)
> echo "aaaaaaaaaaaaaaa" | base64           <-- here we are creating a key we are going to encrypt the Secrets in the ETCD 
                                              Note: this key must be of 16 or 32 characters, otherwise it will not work 

Step-2)
> cd /etc/kubernetes/etcd-test          <-- normally we can make a folder in any place but it is recommend to crearte here folder 
                                           we are creating with name "etcd-test 

Step-3)
vi /etc/kubernetes/etcd-test/my-file.yaml                                 <-- creat this file under the above created dir and put this file under it  
                                                                       Note: main point is we have to just to put this file here, not need to do "k apply -f "
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
      - configmaps
      - pandas.awesome.bears.example
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: <BASE 64 ENCODED SECRET>            <--  Put the above encoded key here 
      - identity: {}

Step-4)
> vi /etc/kubernetes/manifests/kube-apiserver.yaml  

spec:
  containers:
  - command:
    - kube-apiserver
    - --encryption-provider-config=/etc/kubernetes/etcd-test/my-file.yaml       <-- put this command at this place, this command is 
                                                                                 also defined in the documation  
                                                                                here we have to put the path of the above dir and file           
    volumeMounts:
    - mountPath: /etc/kubernetes/etcd-test
      name: etcd-testing                                                               <-- we have to create a volumeMount 
      readOnly: true
  volumes:
  - hostPath:
      path: /etc/kubernetes/etcd-test 
      type: DirectoryOrCreate 
    name: etcd-testing                                     <-- here we are creating a volume and define the path 



Note: after making these changes, when ever a new Secret will be created it then that data will not be stored as plain test in the etcd db

#####################################################################################################
Runtimeclass:
-------------

Note: search in the docs "Runtimeclass"

Step-1)

vi rn.yaml 
---------
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: gvisor                   <-- we can  put any name here 
handler: runsc 

Step-2)
vi pod.yaml 
--------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test
spec:
  runtimeClassName: myclass            <-- like this we can add the above defined "Runtimeclass"
  containers:
  - image: nginx
    name: test

#######################################################################################################
Q) Run a pod with a "root user"  after that make changes and run it as "non root" user.


Step-1) 
Create a pod and check if it run as "root" user or not 

> k run test-pod --image=nginx -it -- sh 
# id                                               <-- execute this command  
uid=0(root) gid=0(root) groups=0(root)             <-- if "uid-0" comes in output it means we are running as "root" user 

Step-2)
Create a pod as non-root user 

vi pod.yaml 
-----------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test
spec:
  securityContext:
    runAsUser: 1000                   <--  here we have defined the user=1000, so it is not a root user, root user is "0"
    runAsGroup: 2000
  containers:
  - command: 
    - sh 
    - -c 
    - sleep 1d                      <-- we must use "sleep 1d" when we use "securityContext" otherwise we will get error 
    image: nginx
    name: test

> k exec test -it -- sh       
# id                                               <-- execute this command  
uid=1000(root) gid=2000(root) groups=2000(root)    <-- if "uid-2000" comes in output it means we are running as "non-root" user 


-------------------------------------------------------------------------------------------
Q) How to run a "Container" in a pod as a non-root 

vi pod.yaml 
-----------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test
spec:
  containers:
  - command: 
    - sh 
    - -c 
    - sleep 1d      
    image: busybox                <-- in this case we will get a error, As by-default the "busybox-image" run as "root"
    name: test                            but we defined the container as "non-root" so we get error, in this case we must   
    securityConext:                       specify that the image should also run as "non-root"
      runAsNonRoot: true          <-- like this we are forcing pod to run as "non-root" user 


-------------------------------------------------------------------------------------------------
Q) How to run a container as  "Privileged Container"

> k exec -it test-pod -- sh 
# id 
uid=0(root) gid=0(root) groups=0(root)         <-- as it shows it run as a "root" user 
# sysctl kernel.hostname=sunny                 <-- like this we can change the host name of the kernel 
error                                        <-- but as we do not run it a "Privileged" so it give us error 


vi pod.yaml 
-----------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test-pod
spec:
  containers:
  - command: 
    - sh 
    - -c 
    - sleep 1d      
    image: busybox               
    name: test                         
    securityConext:                     
      privileged: true             <-- it will now run as "Privileged" 


> k exec -it test-pod -- sh 
# id 
uid=0(root) gid=0(root) groups=0(root)         <-- as it shows it run as a "root" user 
# sysctl kernel.hostname=sunny                 <-- like this we can change the host name of the kernel 
changed                                        <-- no we can change the privileged.


--------------------------------------------------------------------------------------------
AllowPrivilegeEscalation
------------------------



vi pod.yaml 
-----------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test-pod
spec:
  containers:
  - command: 
    - sh 
    - -c 
    - sleep 1d      
    image: busybox               
    name: test                         
    securityConext:                     
      allowPrivilegeEscalation: false      <-- like this we can explicitly define to not to allow extra permissions.

----------------------------------------------------------------------------------------------------------
PodSecurityPolicy:  ( It is depricated, so no need to read it )
-----------------

Note: We can create a "Pod-Securty-Policy" that a pod is not alloweed to get more premission then its root.
      e.g  that it will not get more permissions and "allowPrivilegeEscalation: false"  is only alloweed. 
           And if we put "allowPrivilegeEscalation: true" then with this policy the pod will not be created. 

Step-1)
> vim /etc/kubernetes/manifests/kube-apiserver.yaml 

spec:
  containers:
  - command:
    - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy 

#################################################################################################
Microservice Vulnerabilites: mTLS 
---------------------------------

mTSL means two way authentication, Two parties can authenticate with each other at same time and by this create a secre 
two way communication.

Q) Create a Service-Mesch sidecar proxy like "istio" that the traffic from a pod goes out and come in through a sidecar proxy container 

vi pod.yaml 
-----------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test
  name: test-pod
spec:
  containers:
  - image: busybox     
    name: test  
    command: 
    - sh 
    - -c 
    - "ping google.com"
    image: busybox     
    name: test
  - name: sidecar-proxy 
    image: ubuntu          
    command: 
    - sh 
    - -c 
    - 'apt-get update && apt-get install iptables -y && iptables -L && sleep 1d'      <--to enable route traffic through sidecar we need 
    securityConext:                                                                    to instal "iptables" 
      capabilities:
        add: ["NET_ADMIN"]        <-- we need also to add "NET_ADMIN" capabilites, only after that it allows to route traffic through sidecar container 


########################################################################################################
Multistage build:
-----------------

Note: in this usecase we are having a golang application and we are building a multistage build with Docker 

Step-1)

my-app.go 
---------
package main

import {
     "fmt"
     "time"
     "os/user"
}

func main() {
  user, err := user.Current()
  if err != nil {
               panic(err)
  }

  for {
    fmt.Println("user: " + user.Username + " id: " + user.Uid)
    time.Sleep(1 * time.Second)
  }
}

Step-2) 
> go mod init myapp          <-- like this we are initializing a go project, we must have "go.mod" file 

-----------------------------------------------------------------------------------------------
Example-1
-----------

Dockerfile
----------
FROM golang:1.21-alpine AS builder
WORKDIR /test-a
COPY go.mod .
COPY myapp.go .
# COPY . .                         <-- or we can directly copy all in one command 
RUN go mod download
RUN go build -o ./my-api
CMD ["/test-a/my-api"]             <-- like this we create a normal Docker image it is near about "250 MB" 
                             Note: As we have not defined any "User" so when we run this image then it will run as "root"
                                             


-----------------------------------------------------------------------------------------------
Example-2
---------

Dockerfile
----------
FROM golang:1.21-alpine AS builder
WORKDIR /test-a
COPY go.mod .
COPY myapp.go .
# COPY . .                                  <-- or we can directly copy all in one command 
RUN go mod download
RUN go build -o ./my-api

FROM alpine 
WORKDIR /test-b
COPY --from=builder /test-a/my-api ./my-api         <--"--from=0" it means just copy "build binary" or "build artifact" from the previous stage to new stage 
CMD ["/test-b/my-api"]                         Note: when we build this then it is only "5 MB" of size.


---------------------------------------------------------------------------------------------
Example-3    ( Securing and Hardining the image) <-- it means reduce veneruabilities.
---------

Dockerfile
----------
FROM golang:1.21-alpine AS builder
WORKDIR /test-a
COPY go.mod .
COPY myapp.go .
# COPY . .                                  <-- or we can directly copy all in one command   
RUN go mod download
RUN go build -o ./my-api


FROM alpine:3.14.2                                                            As all the files are stored for user in the "/etc" folder 
RUN chmod a-w /etc                                                        <-- as below explaine with this command  we can make the "etc" folder readonly 
RUN addgroup -S sunny-group && adduser -S sunny-user -G sunny-group      <-- like this we can create a new "user" and "Group" so that the image will not use "root" user 
RUN rm -rf /bin/*                                                         <-- with this we are restricting the "shell" below it is explained.
WORKDIR /test-b                                                               means we can not go inside the container with "docker exec -it <container-id> sh"
COPY --from=builder /test-a/my-api ./my-api
USER sunny-user                                                          <-- here we are switching form "root" user to newely created "user"    
CMD ["/test-b/my-api"]


Testing-1
--------
> docker run 

> docker run -d <image-name>                     <-- it will create a continer and return a id 
dsdsdsdsdsdfg4rr54423                           <-- an output comes like this

> docker exec -it  dsdsdsdsdsdfg4rr54423 sh     <-- we have to put the id that come in output, like this we can go inside 
                                                    the container with shell command 

> ls -la                   <-- if in the result come "etc" and other folder then it is ok, otherwise do a "cd .." and then "ls -la"
OR
> cd ..
> ls -la 

drwxr-xr-x    1 root     root          4096 Jan 30 15:13 etc       <-- "drw" as we can see that "ohner" has "Write" access also 


RUN chmod a-w /etc                      <-- Add this in the second build process in the "Dockerfile"

> ls -la 
dr-xr-xr-x    1 root     root          4096 Jan 30 15:32 etc      <-- Now we can see "dr-" is showing it means no "w"  Write premission is their 



Testing-2
--------
> docker run -d <image-name>                   <-- it will create a continer and return a id 
dsdsdsdsdsdfg4rr54423                          <-- an output comes like this

> docker exec -it  dsdsdsdsdsdfg4rr54423 sh         <-- like this we can exec in the docker image
AND 
> docker run test-1
> docker exec -it <container-id> sh            <-- like this we can directly exec in the running contianer 





Testing-2
------
> docker run test-1                         <-- first we execute a container from a image 
> docker exec -it aaasad3eee sh             <-- after that we can directly exec in that container, but some time we are not able to run the contianer 
                                                 so we have to use the above type Testing-1

RUN rm -rf /bin/*                         <-- But if we put this in the second-stage of the Dockerfile then we are not able to "sh" in 
                                              image and also not in the  runnging container.


#########################################################################################################
Kubesec:
-------

Scan Type-1)
------------
Note: we can directly go to https://kubesec.io and put their the .yaml files and check if their is some veneruabilities or not.


Scan Type-2)
-------------

> docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin < kubesec-test.yaml        <-- this is a example command that we can get from the "kubesec" website 


kubesec/kubesec:512c5e0             <-- this we nned to use, we need "docker" to run this command 


Step-1)
> k run my-pod --image=nginx --dry-run=client -oyaml > my-pod.yaml 

Step-2)
> docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin < my-pod.yaml 

Output:                                                                       <-- like this it will give us a output, we can modify the "my-pod.yaml" file 
[                                                                                and run this scan again, 
  {
    "object": "Pod/test.default",
    "valid": true,
    "message": "Passed with a score of 0 points",
    "score": 0,                                                               <-- As we solve below some veneruabilities then this scome become 1 or 2 or 3 
    "scoring": {
      "advise": [
        {
          "selector": "containers[] .securityContext .runAsUser -gt 10000",
          "reason": "Run as a high-UID user to avoid conflicts with the host's user table"
        },
        {
          "selector": ".spec .serviceAccountName",
          "reason": "Service accounts restrict Kubernetes API access and should be configured with least privilege"
        },
        {
          "selector": "containers[] .securityContext .runAsNonRoot == true",
          "reason": "Force the running image to run as a non-root user to ensure least privilege"
        },
        {
          "selector": "containers[] .securityContext .capabilities .drop",
          "reason": "Reducing kernel capabilities available to a container limits its attack surface"
        },
        {
          "selector": ".metadata .annotations .\"container.seccomp.security.alpha.kubernetes.io/pod\"",
          "reason": "Seccomp profiles set minimum privilege and secure against unknown threats"
        },
        {
          "selector": "containers[] .securityContext .capabilities .drop | index(\"ALL\")",
          "reason": "Drop all capabilities and add only those required to reduce syscall attack surface"
        },
        {
          "selector": ".metadata .annotations .\"container.apparmor.security.beta.kubernetes.io/nginx\"",
          "reason": "Well defined AppArmor policies may provide greater protection from unknown threats. WARNING: NOT PRODUCTION READY"
        },
        {
          "selector": "containers[] .securityContext .readOnlyRootFilesystem == true",
          "reason": "An immutable root filesystem can prevent malicious binaries being added to PATH and increase attack cost"
        },
        {
          "selector": "containers[] .resources .limits .memory",
          "reason": "Enforcing memory limits prevents DOS via resource exhaustion"
        },
        {
          "selector": "containers[] .resources .requests .cpu",
          "reason": "Enforcing CPU requests aids a fair balancing of resources across the cluster"
        },
        {
          "selector": "containers[] .resources .limits .cpu",
          "reason": "Enforcing CPU limits prevents DOS via resource exhaustion"
        },
        {
          "selector": "containers[] .resources .requests .memory",
          "reason": "Enforcing memory requests aids a fair balancing of resources across the cluster"
        }
      ]
    }
  }
]


########################################################################################################################
OPA Conftest:        ( With this we can write test for our kubernetes manifest files )
-------------

www.conftest.dev 

Installing option:

Option-a)
------------
> brew install conftest 
> conftest test deploy.yaml         <-- put hier your deployment or service or any other resource file for testing 


Option-b) 
----------
> docker run --rm -v $(pwd):/project openpolicyagent/conftest test deployment.yaml        <-- in this case we are directly running the test in a 
                                                                                              docker container, so no need to install on ubuntu 




------------------------------------------------
Testing policy for manifest files
--------------------------------

Step-1) 
policy/my-dep-policy.rego            <-- create a "policy" folder and put below test file in it. test file name should be "deployment.rego"    
----------------------                 it should have .rego  extnstion, name can pbe different, but it mus be inside a "policy" folder 
package main

deny[msg] {
  input.kind == "Deployment"                                            <-- here we have to define the "kind: Deployment"
  not input.spec.template.spec.securityContext.runAsNonRoot

  msg := "Containers must not run as root"
}

deny[msg] {
  input.kind == "Deployment"
  not input.spec.selector.matchLabels.app

  msg := "Containers must provide app label for pod selectors"
}

---------------------------

Step-2)
-------
> k create deploy mydeploy --image=nginx --dry-run=client -oyaml > test-deploy.yaml            <-- create a deployment 

Step-3)
> docker run --rm -v $(pwd):/project openpolicyagent/conftest test test-deploy.yaml 
OR
> conftest test test-deploy.yaml 


=============================================================================================
Testing policy for Dockerfile:
-----------------------------

Conftest -> Examples -> Dockerfile 

Tesing policy:


Step-1)
-------
/policy/testing-dockerfile.rego                        <-- always create a policy under the "policy" folder, name can be different 
-------------------------------
package main

denylist := ["openjdk"]

deny[msg] {
	some i
	input[i].Cmd == "from"
	val := input[i].Value
	contains(val[i], denylist[_])

	msg = sprintf("unallowed image found %s", [val])
}


Step-2)
--------

vi Dockerfile                    <-- it should not be in the "policy" folder 
---------------
FROM openjdk:8-jdk-alpine
VOLUME /tmp
ARG DEPENDENCY=target/dependency
COPY ${DEPENDENCY}/BOOT-INF/lib /app/lib
COPY ${DEPENDENCY}/META-INF /app/META-INF
COPY ${DEPENDENCY}/BOOT-INF/classes /app
RUN apk add --no-cache python3 python3-dev build-base && pip3 install awscli==1.18.1
ENTRYPOINT ["java","-cp","app:app/lib/*","hello.Application"]


Step-3)
-------
> conftest test Dockerfile                   <-- like this we can test, if we have installed "conftest" on ubuntu 

OR 

> docker run --rm -v $(pwd):/project openpolicyagent/conftest test Dockerfile     <-- directly the file, with default namespace  

OR
> docker run --rm -v $(pwd):/project openpolicyagent/conftest test Dockerfile --all-namespaces  <-- here it goes through all the namespaces.


##############################################################################################################################
Trivy    <-- for scaning the images 
-----



Installing in two ways :
---------------

> brew install trivy                    <-- we can directly install it on the ubuntu 
OR
> docker run aquasec/trivy             <-- or we can run test on a docker container 



Step-1) 
-------
> trivy image nginx                 <-- like this we can scan directly image with name.
OR 
> docker run aquasec/trivy image nginx           <-- like this we can scan image with in docker, without installing trivy on ubuntu.



#################################################################################################################################
Image Digest:
------------

Note: To make sure that a container always uses the same version of an image, you can specify its digest.
      The digest identifies a specific version of the image, so it is never updated atuomatically by kubernetes.


Whitelist Registries with OPA:
------------------------------



apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8strustedimages
spec:
  crd:
    spec:
      names:
        kind: K8sTrustedImages
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8strustedimages
        violation[{"msg": msg}] {
          image := input.review.object.spec.containers[_].image
          not startswith(image, "docker.io/")
          not startswith(image, "k8s.gcr.io/")
          msg := "ONLY DOCKER AND GOOGLE REGISTRY ARE ALLOWED"
        }
-------


apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sTrustedImages
metadata:
  name: pod-trusted-images
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]



#################################################################################################
################################################################################################
CKS Update:
----------

These links are allowwed extra for CKS exam:

https://kubernetes.io/docs/home/ 
https://github.com/aquasecurity/trivy
https://docs.sysdig.com/
https://falco.org/docs/
https://gitlab.com/apparmor/apparmor/-/wikis/Documentation



-----------------------------------------------------------








Understanding the kubernetes:
----------------------------


















Test-a)
Q) What is the user used to execute the sleep process within the ubuntu-sleeper pod?

> kubectl exec ubuntu-sleeper -- whoami


Q) Edit the pod ubuntu-sleeper to run the sleep process with user ID 1010.

Note: Only make the necessary changes. Do not modify the name or image of the pod.


##############################################################################################
kube-bench:
-----------


Note:   "kubebench install"      <-- when we put this in google, then it comes a "GitHub" page, just go inside and 
                                    install it, in the middle of the page is written the test command as shown below. 



> ./kube-bench --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml          <-- kube-bench test can be run with this command 


################################################################################################
System Hardening:
=================

Commands:
---------
> id            <-- it givses the info about, uid, gid, groups 
> id john       <-- like this we can see what is the id of John 
> who           <-- it give the list of user, who are currently logged in the system 
> last          <-- it give "last time" when the users were logged in the system. 

Information about user is stored at :
---------------------------------

> cat /etc/sudoers                  <- under this file all the "Sudo" related information is stored 
  %admin ALL=(ALL) ALL              <-- any name starts with "%" it means it is a Group 
  jim ALL=(ALL) ALL                 <-- any name like this it means it is a user. 


> /etc/passwd                       <-- here is the name of the users that exists in system 
> grep -i ^john /etc/passwd         <-- we can see the info about the user 

> /etc/shadow                       <-- under it we can see all the passwords    
> grep -i ^john /etc/shadow         <-- like this we can see the exact password of John 

> /etc/group                        <-- like this we can see all the defined groups  
> grep -i  ^john /etc/group         <-- like this we can see all the groups defined for "john" user


> usermod -s /bin/nologin john          <-- like this we can deactivate the any active user.  
> grep -i john /etc/                <-- here we can see "nologin" is added to the user after above command 


-------------------------------------------
> userdel john                      <-- we can delete the user 

> deluser john admin                <-- "admin" is a group, like this we can delete "john" from "admin" group 

> passwd john                       <-- like this we can change the password for "john" user 


=====================================================================
Package command:
-----------------

> systemctl list-units --type service              <-- it will give list of all services that are running in the server 

> systemctl list-units --all --state inactive      <-- it return all the "inactive" services.

> systemctl stop apache2               <-- like this we can "Stop" a service with name "apache2"

> systemctl disable apache2            <-- like this we can disable a service.

> apt remove apache2                   <-- After disbling the servie we can remove the complete "apache2" package 

> apt list --installed                 <-- it list all the installed packages on system 

> apt list --installed | grep python2-7       <-- it check if the "python2.7" is installed on system or not 

> 



=======================================================================
Modules:
-------

> lsmod                                      <-- to list all the modules loaded in the "Linux kernal"

> modprobe pcspkr                            <-- like this we can manually load a "module" in Linux kernal 

-----------------------------------
Steps to blacklist the modules:
--------------------------------

Step-1)
> cat /etc/modprobe.d/blacklist.conf         <-- In this file we can put a list of "moudle" that are not alloweed to be loded in "Linux Kernal" 
      blacklist sctp                         <-- in "blacklist.conf" file here we have blacklisted the "sctp" moudle 

Step-2)
> shutdown -r now                            <-- this command rebote the node 

Step-3)
> lsmod | grep stcp                          <-- After restaring in this "lsmod" command we will not see any result, as the "sctp" is blacklisted and removed 

=============================================================================
Disable Open ports 
------------------

> systemctl status ssh                    <-- like this we can check i

> netstat -an | grep -w LISTEN            <-- like this we can get a list of all the port that are connected (In-use) and listining


> cat /etc/services | grep -w <any-port-nr>  
> cat /etc/services | grep -w 53                   <-- in the file "/etc/services" we have a list of ports and where they are listening.






Accounts in Linux:
------------------
Their are 4 types of account in Linux

1) User Account:  we can create a user for "Developer" or for "Administrator"
2) Superuser Account:  "UID=0"   with user-id=0  we can create a super-user that has "root" access 
3) System Accounts: it is created by OS installation, it is used by softeware or services at the time of installation 
                    theses services should not have "root" permissions, so they use this Account.
4) Service Account: It is created by the software, e.g for a Nginx service will use a "Service-account" name "nginx"



Information about user is stored at :
---------------------------------

> /etc/passwd                       <-- here is the name of the users 
> grep -i ^john /etc/passwd         <-- we can see the info about the user 

> /etc/shadow                       <-- under it we can see all the passwords    
> grep -i ^john /etc/shadow         <-- like this we can see the exact password of John 

> /etc/group                        <-- like this we can see all the defined groups  
> grep -i  ^john /etc/group         <-- like this we can see all the groups defined for "john" user 


 
Q) We have a user account named mail on controlplane host. What kind of user account is it?

"mail" is a predefined "System Account" user, it is used for "sending mails" 

-----------------------------------------------------------
Q) We have created a user named john  on controlplane host. What is the uid of this user?
> id john       <-- like this we can see what is the id of John 
OR 
> cat /etc/passwd | grep john   <-- like this we can see the id 

--------------------------------
Q) How to Set new password for a existing user "John"
> passwd john                         <-- after this it ask for password and put it new 

------------------------------------
Q) Delete a user nane John and also from group called devs 

> groupdel john devs        <-- here we are deleting "John" from the group "devs"
> userdel  john             <-- here we are deleting the user "John" 

------------------------------------------------------------------------
Q) There is a user named himanshi on the controlplane host. Suspend this user account so 
   that this user cannot login to the system but make sure not to delete it.

> usermod -s /usr/sbin/nologin himanshi               <-- it will deactivate the user 

-----------------------------------------------------------------------
Q) Create a user named sam on the controlplane host. 
   The user's home directory must be /opt/sam. 
   Login shell must be /bin/bash and uid must be 2328. 
   Make sam a member of the admin group.

> useradd -d /opt/sam -s /bin/bash -G admin -u 2328 sam



=====================================================================================
SSH Hardening:
------------
Note:  SSH/port 22   <-- when we want to connect through SSH it uses  port 22 

Commands:
----------
> ssh <ip>                           < -- like this we can give the ip address of the node that we want to connect 
> ssh <hostname>                     <-- like this we can give the hostname of the node that we want to connect 


> ssh  user@<hostnme or IP>          <-- like this we can also pass a user with the hostname with it we want to login 
OR
> ssh -l <user> <hostname or IP> 


-------------------------------------------------------------------
How ssh login work?

> ssh node01         <-- as i am logged in on my linux-machine with user "sunny" so after this command it ask for 
                        password that i set for "sunny" 
OR 
> ssh sunny@<hostname or ip>       <-- it means we are logging with sunny user on the node or server 

------------------------------------------------------------------------
Q) How to generate a ssh key on you linux system and transfer to the node or server ?

Step-1)
> ssh-keygen -t rsa        <-- this command will cretate 
                             /home/sunny/.ssh/id_rsa.pub        <-- a public-key 
                             /home/sunny/.ssh/id_rsa            <-- private key 

Step-2)
> ssh-copy-id -i ~/.ssh/id_rsa.pub sunny@node01     <-- "ssh-copy-id"  when we use this commad first time to login to a node, it will not ask for password 
                                                    it will directly copy the public key that is generated on the linux maching and copy it to the "node01"
                                                     After that we no need to endter the password, as our public key is already placed their.

Note: The user public key is stored in the node or server  under the folder  "~/.ssh/authorized_keys"  or "/home/sunny/.ssh/authorized_keys" 
      At the time of login, system automatically check my public-key with the keys that placed in the above folder of the node. 

-----------------------------------------------------------------------------
Q) How to restrict a SSH login, so that no one can login as "root" user 

> vi /etc/ssh/sshd_config                     <-- this file is on "Node" and we can add the below values to restrict it 
  PermitRootLogin no                          <-- like this we can say that no user can login as "root"
  PasswordAuthentication no                   <-- like this no user can login directly with "password" he must use its private-key and store its public 
                                                  key on the node, 
                                              Main.point: as we have already copyied in the above example the key to the node, so now we can do these changes

Step-2)
> systemctl restart sshd                     <-- we need to restart the "sshd" server, this servise is responsible for SSH login
OR
> service sshd restart


------------------------------------------------------------------------------
Q) Create a user named john on node01 host and configure password-less ssh access from 
   controlplane host (from user root) to node01 host (to user john).

Step-1)
@controlplane> ssh-keygen -t rsa        <-- this command will cretate 
                                         /home/sunny/.ssh/id_rsa.pub        <-- a public-key 
                                         /home/sunny/.ssh/id_rsa            <-- private key 

@controlplane> ssh node01                                          <-- login to node01 from controlplane node 
@node01>       adduser john                                        <-- create a new user name "john" and create a directory structure with name "john@node01"
@controlplane> ssh-copy-id -i ~/.ssh/id_rsa.pub john@node01        <-- "ssh-copy-id"  when we use this commad first time to login to a node, it will not ask for password 
                                                                       This command will create a ".ssh/authorized_keys" folder on john@node01 and copy from local maching the
                                                                       public-key to the john@node01 when we will try to login 
                                                                       the local private-key will be used to authenticate and the key that is place under john@authorized_keys 
                                                                       will be checked and authenticate the user.
                                                            
                                                                       
@controlplane> ssh john@node01                                     <--it will work

------------------------------------------------------------------------------
@) Change the password of user john to 8DHdjjdk on host node01 and make him a sudo user.

> passwd john               <-- like this we can change the password of John user 

> vi /etc/sudoers 
  john ALL=(ALL:ALL) ALL       <-- like this we can make user john to use "sudo" command. 


-----------------------------------------------------------------------------
@) We want to update user jim on node01 host so that jim can run sudo commands without entering the sudo password. Please make appropriate changes

> ssh node01 

> vi /etc/sudoers 
  jim ALL(ALL:ALL) ALL      replace to   ->   jim ALL(ALL) NOPASSWD:ALL 

------------------------------------------------------------------------------
Q) Add a new user john on node01 host and set password to jid345kjf. 
   We already have a sudo group i.e admin on this host. 
   Make user john member of the admin group so that he can become a sudo user. 
   Do not to add the user directly in the sudoers file.

> ssh node01 
> adduser john

> usermod john -G admin              <-- like this we can add user john to "admin" group 

---------------------------------------------------------------------------------
Q) Which of the following commands is used to list all installed packages on an ubuntu system?

> apt list --installed


----------------------------------------------------------------------------------
Q) Check if python2.7 package is installed on the controlplane host


> apt list --installed | grep python2.7


-----------------------------------------------------------------------------
Q) Which of the following commands can be used to list only active and deactive services on a system?

> systemctl list-units --all                <-- it return all the active services 

> systemctl list-units --all --state inactive   <-- it return all the deactive services 


--------------------------------------------------------------------------------
Q) Which command can be used to list the kernel modules currently loaded on a system?

> lsmob


-----------------------------------------------------------------------------------
Q) On the controlplane host, we have nginx service running which isn't needed on that system. 
    Stop the nginx service and remove its service unit file. 
    Make sure not to remove nginx package from the system.

> systemctl list-units --all | grep nginx
> systemctl stop nginx
> systemctl status nginx
> rm /lib/systemd/system/nginx.service

-----------------------------------------------------------------------------------
Q) We want to blacklist the evbug kernel module on controlplane host. 
  Important Note: Do not reboot the node!

> vim /etc/modprobe.d/blacklist.conf            <-- in this file add the below entry to blacklist 
      blacklist evbug                              


-----------------------------------------------------------------------------------
Q) Remove the nginx package from controlplane host.


> apt remove nginx -y

---------------------------------------------------------------------------------
Q) We have a service running on controlplane host which is listening on port 9090. 
   Identify the service and stop the same to free the 9090 port.

> netstat -natp  | grep 9090            <-- Identify the service listening on port 9090
 
> systemctl stop apache2                <-- Kill/Stop the service to free the port


-----------------------------------------------------------------------------
Q) We have the wget package version v1.18 installed on the host controlplane. 
   There were issues reported with the current version of wget. 
   Please check for updates available for this package and update to the latest version available in the apt repos.

> apt install wget -y             <-- like this it will install the latest version.


=====================================================================================================
Uncomplicated Firewal  ( UFW )
---------------------

Steps to install UNF 
---------------------

> apt-get update 
> apt-get install ufw 
> systemctl enable ufw 
> systemctl start ufw 
> ufw status                     <-- till this step "status = inactive" it shows inactive status, as till this step we have 
                                     not created firewall rules, After creating firewall rules it will be in active state.

> ufw default allow outgoing     <-- here we are adding "allow" for outgoing, means every port is open for outgoing.
> ufw default deny incoming      <-- here we are setting "deny" for all incoming ports 

> ufw allow from 172.16.238.5 to any port 22 proto tcp        <-- like this we can open the port 22 for "172.16.238.5" 
                                                               rest as above "deny" all other ports are blocked.

> ufw allow from 172.16.238.5 to any port 80 proto tcp 
> ufw allow from 172.16.100.0/28 to any port 80 proto tcp        <-- here we are opening port for these IPs 

> ufw deny 8080                                   <-- here we are blocking the port 8080 so nobody can come through it.


> ufw enable                      <-- After adding the rules, at the end enable the firewal.


> ufw delete deny 8080             <-- as we have above created the "ufw deny 8080" rule, like this we can delete that rule. 


Delete a "ufw" rule by line number:
----------------------------------

> ufw status                     <-- it will print all the defined "ALLOW" and "DENY" rules 

> ufw delete 5                   <-- in this example we are deleting the rule defined at the "line 5"



---------------------------------------------------------------------------------
Q) Which of the following commands can be used to display the rules along with rule numbers next to each rule?

> ufw status numbered                 

---------------------------------------------------------------------------------
Q) What is the command to allow a tcp port range between 1000 and 2000 in ufw?

> ufw allow 1000:2000/tcp 

-------------------------------------------------------------------------------
Q) How can you reset ufw rules to their default settings?


> ufw reset                       <-- with this we can reset the allow and deny rules to default settings 

----------------------------------------------------------------------------
Q) On the node01 host, add a rule to allow incoming SSH connections.
Do not enable the firewall yet.


> ufw allow 22 

-------------------------------------------------------------------------------
Q) We have some services on node01 host, which are running on tcp port 9090 and 9091. 
   Add ufw rules to allow incoming connection on these ports from IP range 135.22.65.0/24 to any interface on node01.
   Once this is done, enable the firewall.



> ufw allow  from 135.22.65.0/24 to any port 9090 proto tcp
> ufw allow  from 135.22.65.0/24 to any port 9091 proto tcp
> ufw enable


----------------------------------------------------------------------------------
Q) There is a Lighttpd service running on the node01. Identify which port it is bound to.

> systemctl status lighttpd                      <-- first check if "lighttpd"  service is running 
> netstat -natulp | grep lighttpd                <-- now we can check on which port "lighttpd" servic is running

--------------------------------------------------------------------------------------
Q) This service was identified to have several vulnerabilities in it. Disable the port 80 on node01 for ALL incoming requests.

> ufw deny 80

-------------------------------------------------------------------------------------
Q) We want to temporarily disable ufw firewall on node01 host but later we will enable it 
   back so make sure to disable the firewall but preserve all rules so that same can be 
   effective when we enable it back.

> ufw disable

=======================================================================================
Syscalls:
--------

execve()
read()
write()
close()
send()
revc()
accept()                                   <-- these are the System-calls that we can make from container -> linux-kernale -> Hardware(CPU, Memory, Devices)




> which strace                  <-- like this we can check if "strace" is installed on the server or not 

> strace touch /tmp/abc.log      <-- if we put "strace" in front of this command, then we can see all the 
                                    SYS-Calls that this command has made for creating this file inside the "/tmp" folder,


====================================================================================
Seccomp:
--------

Note: By-default Docker has implemented "Seccomp" so it blocks automatically 60 different types of SYS-Calls by-default.

> docker run r.j3ss.co/amicontained amicontained        <-- like this we can get a list of all the by-default blocked SYS-Calls 
------------------------------------






> grep -i seccomp /boot/config-$(uname -r)               
  CONFIG_SECCOMP=y                                       <-- if the result of the above command come like this it means that 
                                                            kernel is supported by Seccomp.

Seccomp Modes:
-------------
Mode 0                    <-- it means "Seccomp" is disabled  
Mode 1                    <-- it means "Read()" "Write()" "Exit()" these methods are alloweed
Mode 2                    <-- it means "Seccomp" is filtring calls, but which calls it is on the configurations.


whitelist.json            <-- we can define all the syscalls types that we want to allows, 
                            Note: only those are alloweed that are defined, rest all will be blocked.

blacklist.json            <-- Only the defined syscall will blocked, rest all will be alloweed.



======================================================================================
AppArmor:
--------

> systemctl status apparmor              <-- like this we can check if the "Apparmor" is installed or not.

> cat /sys/module/apparmor/parameters/enabled   <-- if it print "Y" it means "Apparmor" is installed on the node. 

> cat /sys/kernel/security/apparmor/profiles    <-- if their are any profile is created for "Apparmor" then that will be defined 
                                                    in this file. This file defines that the application can 


How to create a "apparmor" profile on ubuntu.
------------------------------------

/root/my_data.sh 
-----------------
#!/bin/bash                             
data_directory=/opt/app/data
mkdir -p ${data_directory}
echo "=> File created at `date`" | tee {data_directory}/create.log             <-- this whole script create a file at 
                                                                                  "/opt/app/data/create.log" and wirte in it "File created at date of the day"

Step-1)
------
> apt-get install -y apparmor-utils               <-- this will install the package 

> aa-genprof /root/my_data.sh                     <-- "my_data.sh" we have a shell script file, we can execute this file with the  
                                                      "aa-gebprof" command, with this command the shell script will run from a different window,
                                                                   and it can scan all the events.
Step-2)
> ./my_data.sh                               <-- Now from the seperate window run this bash script, and the                                              

> s                                          <-- at the end press "s" to save 
> f                                          <-- at the end press "f" to finish   

Step-3) 
> aa-status                                <-- with this we can get the status of "AppArmor" profile, that is generated.

Step-4) 
> cat /etc/apparmor.d/root.my_data.sh        <-- at this place our result is stored automatically by the "AppArmor"

----------------------------------------------------
> apparmor_parser /etc/apparmor.d/root.my_data.sh           <-- with this command we can check if the profile is correctly loaded or not 
                                                                if after executing nothing is printed, it means profile is correctly loaded.


---------------------------------------------------------------
How to disable a existing AppArmor profile ?
--------------------------------------------
> apparmor_parser -R /etc/apparmor.d/root.my_data.sh       <-- "apparmor_parser -R" with this command we can disable a existing loaded profile 

> ln -s /etc/apparmor.d/root.my_data.sh /etc/apparmor.d/disable/    <-- after disabling with above command, it creates a file under this directory.

========================================================================================
AppArmor in Kubernetes:
----------------------
Note: with AppArmor we can restirct a container that what i can do and what it can not do.


Requirements for run AppArmor in kubernetes:

- AppArmor kernel Module Enabled 
- AppArmor Profile Loaded in the Kernel 
- Container Runtime should be Supported.











##########################################################################################
Minimize Microservice Vulnerabilites:
-------------------------------------

Q) What is the user used to execute the sleep process within the ubuntu-sleeper pod?

> kubectl exec ubuntu-sleeper -- whoami





##############################################################################################

Supply Chain Security:
---------------------










##########################################################################################
Monitoring, Logging and Runtime:
--------------------------------






#################################################################################################
Mock Exam-1:
-----------
Q-1)

A pod has been created in the omni namespace. However, there are a couple of issues with it.

    The pod has been created with more permissions than it needs.
    It allows read access in the directory /usr/share/nginx/html/internal causing an Internal Site to be accessed publicly.

    To check this, click on the button called Site (above the terminal) and add /internal/ to the end of the URL.
    Use the below recommendations to fix this.

    Use the AppArmor profile created at /etc/apparmor.d/frontend to restrict the internal site.
    There are several service accounts created in the omni namespace. Apply the principle of least privilege 
    and use the service account with the minimum privileges (excluding the default service account).
    Once the pod is recreated with the correct service account, delete the other unused service accounts 
    in omni namespace (excluding the default service account).

    You can recreate the pod but do not create a new service accounts and do not use the default service account.

Answer:
On the controlplane node, load the AppArmor profile:
apparmor_parser -q /etc/apparmor.d/frontend
The profile name used by this file is restricted-frontend (open the /etc/apparmor.d/frontend file to check).


To verify that the profile was successfully loaded, use the aa-status command:

root@controlplane:~# aa-status | grep restricted-frontend
   restricted-frontend
root@controlplane:~#

The pod should only use the service account called frontend-default as it has the least privileges 
of all the service accounts in the omni namespace (excluding default)
The other service accounts, fe and frontend have additional permissions (check the roles and rolebindings associated with these accounts)

Use the below YAML File to re-create the frontend-site pod:

apiVersion: v1
kind: Pod
metadata:
  annotations:
    container.apparmor.security.beta.kubernetes.io/nginx: localhost/restricted-frontend #Apply profile 'restricted-fronend' on 'nginx' container 
  labels:
    run: nginx
  name: frontend-site
  namespace: omni
spec:
  serviceAccountName: frontend-default #Use the service account with least privileges
  containers:
  - image: nginx:alpine
    name: nginx
    volumeMounts:
    - mountPath: /usr/share/nginx/html
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
       path: /data/pages
       type: Directory

Next, Delete the unused service accounts in the 'omni' namespace.

controlplane$ kubectl -n omni delete sa frontend
controlplane$ kubectl -n omni delete sa fe

------------------------------------------------------------------------
Q-2)
A pod has been created in the orion namespace. It uses secrets as environment variables. 
Extract the decoded secret for the CONNECTOR_PASSWORD and place it under /root/CKS/secrets/CONNECTOR_PASSWORD.

You are not done, instead of using secrets as an environment variable, mount the secret as a read-only 
volume at path /mnt/connector/password that can be then used by the application inside.

Answer:
------
To extract the secret, run: kubectl -n orion get secrets a-safe-secret -o jsonpath='{.data.CONNECTOR_PASSWORD}' | base64 --decode >/root/CKS/secrets/CONNECTOR_PASSWORD
One way that is more secure to distribute secrets is to mount it as a read-only volume.

Use the following YAML file to recreate the POD with secret mounted as a volume:

apiVersion: v1
kind: Pod
metadata:
    labels:
        name: app-xyz
    name: app-xyz
    namespace: orion
spec:
    containers:
        -
            image: nginx
            name: app-xyz
            ports:
            - containerPort: 3306
            volumeMounts: 
            - name: secret-volume
              mountPath: /mnt/connector/password
              readOnly: true
    volumes:
    - name: secret-volume
      secret:
        secretName: a-safe-secret

-----------------------------------------------------------------------------
Q-3)
A number of pods have been created in the delta namespace. Using the trivy tool, which has 
been installed on the controlplane, identify and delete pods except the one with least 
number of CRITICAL level vulnerabilities.

Note: Do not modify the objects in anyway other than deleting the ones that have critical vulnerabilities.

Answer:
------
First, get all the images of pods running in the delta namespace:

$ kubectl -n delta get pods -o json | jq -r '.items[].spec.containers[].image'

Next, scan each image using trivy image command. For example:

$ trivy image --severity CRITICAL kodekloud/webapp-delayed-start

Don't delete the associated pod which image has a least number of CRITICAL vulnerabilities as compared to others.

For example, if kodekloud/webapp-delayed-start, httpd and nginx:1.16 have these vulnerabilities:

$ kubectl -n delta delete pod simple-webapp-1
$ kubectl -n delta delete pod simple-webapp-3
$ kubectl -n delta delete pod simple-webapp-4

Ignore pods which use images of lower severity such as HIGH, MEDIUM, LOW e.t.c

---------------------------------------------------------------------------------
Q-4)
Create a new pod called audit-nginx in the default namespace using the nginx image. 
Secure the syscalls that this pod can use by using the audit.json seccomp profile in the pod's security context.

The audit.json is provided at /root/CKS directory. Make sure to move it under the profiles 
directory inside the default seccomp directory before creating the pod

Answer:
-------
Copy the audit.json seccomp profile to /var/lib/kubelet/seccomp/profiles on the controlplane node:

controlplane$ mv /root/CKS/audit.json /var/lib/kubelet/seccomp/profiles

Next, recreate the pod using the below YAML File

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: audit-nginx
spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/audit.json
  containers:
  - image: nginx
    name: nginx

---------------------------------------------------------------------------------------
Q-5)
The CIS Benchmark report for the Controller Manager and Scheduler is available at the tab called CIS Report 1.
Inspect this report and fix the issues reported as FAIL.

Answer:
------
The fixes are mentioned in the same report. Update the Controller Manager and Scheduler static pod definition file as per the recommendations.
1. Make sure that the --profiling=false parameter is set. 

-------------------------------------------------------------------------------------
Q-6) 
There is something suspicious happening with one of the pods running an httpd image in this cluster.
The Falco service shows frequent alerts that start with: File below a known binary directory opened for writing.

Identify the rule causing this alert and update it as per the below requirements:

    Output should be displayed as: CRITICAL File below a known binary directory opened for writing (user_id=user_id file_updated=file_name command=command_that_was_run)
    Alerts are logged to /opt/security_incidents/alerts.log

Do not update the default rules file directly. Rather use the falco_rules.local.yaml file to override.
Note: Once the alert has been updated, you may have to wait for up to a minute for the alerts to be written to the new log location.

Answer:
-------
Enable file_output in /etc/falco/falco.yaml on the controlplane node:

file_output:
  enabled: true
  keep_alive: false
  filename: /opt/security_incidents/alerts.log

Next, add the updated rule under the /etc/falco/falco_rules.local.yaml and hot reload the Falco service:

- rule: Write below binary dir
  desc: an attempt to write to any file below a set of binary directories
  condition: >
    bin_dir and evt.dir = < and open_write
    and not package_mgmt_procs
    and not exe_running_docker_save
    and not python_running_get_pip
    and not python_running_ms_oms
    and not user_known_write_below_binary_dir_activities
  output: >
    File below a known binary directory opened for writing (user_id=%user.uid file_updated=%fd.name command=%proc.cmdline)
  priority: CRITICAL
  tags: [filesystem, mitre_persistence]

To perform hot-reload falco use 'kill -1 /SIGHUP':

$ kill -1 $(cat /var/run/falco.pid)

Alternatively, you can also restart the falco service by running:

$ systemctl restart falco

----------------------------------------------------------------------------------------
Q-7)
A pod called busy-rx100 has been created in the production namespace. Secure the pod by recreating 
it using the runtimeClass called gvisor. You may delete and recreate the pod.

Answer:
------
Use the below YAML file to create the pod with the gvisor runtime class:

apiVersion: v1
kind: Pod
metadata:
    labels:
        run: busy-rx100
    name: busy-rx100
    namespace: production
spec:
    runtimeClassName: gvisor
    containers:
        -
            image: nginx
            name: busy-rx100

----------------------------------------------------------------------------------------
Q-8) 
We need to make sure that when pods are created in this cluster, they cannot use the 
latest image tag, irrespective of the repository being used.

To achieve this, a simple Admission Webhook Server has been developed and deployed. 
A service called image-bouncer-webhook is exposed in the cluster internally. 
This Webhook server ensures that the developers of the team cannot use the latest image tag. 
Make use of the following specs to integrate it with the cluster using an ImagePolicyWebhook:

    Create a new admission configuration file at /etc/admission-controllers/admission-configuration.yaml
    The kubeconfig file with the credentials to connect to the webhook server is located at /root/CKS/ImagePolicy/admission-kubeconfig.yaml. Note: The directory /root/CKS/ImagePolicy/ has already been mounted on the kube-apiserver at path /etc/admission-controllers so use this path to reference the admission configuration.
    Make sure that if the latest tag is used, the request must be rejected at all times.
    Enable the Admission Controller.

Finally, delete the existing pod in the magnum namespace that is in violation of the 
policy and recreate it, ensuring the same image but using the tag 1.27.

    NOTE: If the kube-apiserver becomes unresponsive, this can affect the validation of this exam. 
    In such a case, please restore the kube-apiserver using the backup file 
    created at: /root/backup/kube-apiserver.yaml, wait for the API to be available again and proceed.


Answer:
-------
Create the below admission-configuration inside /root/CKS/ImagePolicy directory in the controlplane node:

apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
- name: ImagePolicyWebhook
  configuration:
    imagePolicy:
      kubeConfigFile: /etc/admission-controllers/admission-kubeconfig.yaml
      allowTTL: 50
      denyTTL: 50
      retryBackoff: 500
      defaultAllow: false

The /root/CKS/ImagePolicy is mounted at the path /etc/admission-controllers directory in the kube-apiserver. So, you can directly place the files under /root/CKS/ImagePolicy.

Here is a snippet of the volume and volumeMounts (already added to apiserver config):

  containers:
  .
  .
  .
  volumeMounts:
  - mountPath: /etc/admission-controllers
      name: admission-controllers
      readOnly: true

  volumes:
  - hostPath:
      path: /root/CKS/ImagePolicy/
      type: DirectoryOrCreate
    name: admission-controllers

Next, update the kube-apiserver command flags and add ImagePolicyWebhook to the enable-admission-plugins flag. 
Use the configuration file that was created in the previous step as the value of admission-control-config-file.

Note: Remember, this command will be run inside the kube-apiserver container, so the path 
must be /etc/admission-controllers/admission-configuration.yaml (mounted from /root/CKS/ImagePolicy in controlplane).

    - --admission-control-config-file=/etc/admission-controllers/admission-configuration.yaml
    - --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook

In case we mess up while solving the question, API server could become unresponsive.

For example:

The connection to the server controlplane:6443 was refused - did you specify the right host or port?

In such case scenario restore kube-apiserver to it's default state using the backup provided at /root/backup/kube-apiserver.yaml

Run the below command to restore the kube-apiserver initial state:

cp -v /root/backup/kube-apiserver.yaml /etc/kubernetes/manifests


##########################################################################################################
Mock Exam-2)
--------------
Q-1)
A pod called redis-backend has been created in the prod-x12cs namespace. It has been exposed as a service of type ClusterIP.
 Using a network policy called allow-redis-access, lock down access to this pod only to the following:
1. Any pod in the same namespace with the label backend=prod-x12cs.
2. All pods in the prod-yx13cs namespace.
All other incoming connections should be blocked.

Use the existing labels when creating the network policy.

Answer:
-------
Create a network policy using the YAML below:

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-redis-access
  namespace: prod-x12cs
spec:
  podSelector:
    matchLabels:
      run: redis-backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          access: redis
    - podSelector:
        matchLabels:
          backend: prod-x12cs
    ports:
    - protocol: TCP
      port: 6379


----------------------------------------------------------------------------------------
Q-2)
A few pods have been deployed in the apps-xyz namespace. There is a pod called redis-backend which serves as 
the backend for the apps app1 and app2. The pod called app3 on the other hand, does not need access to this redis-backend pod. 
Create a network policy called allow-app1-app2 that will only allow incoming traffic from app1 and app2 to the redis-pod.

Make sure that all the available labels are used correctly to target the correct pods. Do not make any other changes to these objects.

Answer:
-------
Create a new network policy using the sample YAML file below:

kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-app1-app2
  namespace: apps-xyz
spec:
  podSelector:
    matchLabels:
      tier: backend
      role: db
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: app1
          tier: frontend
    - podSelector:
        matchLabels:
          name: app2
          tier: frontend
-----------------------------------------------------------------------------------------
Q-3)
A pod has been created in the gamma namespace using a service account called cluster-view. 
This service account has been granted additional permissions as compared to the default service 
account and can view resources cluster-wide on this Kubernetes cluster. While these permissions 
are important for the application in this pod to work, the secret token is still mounted on this pod.

Secure the pod in such a way that the secret token is no longer mounted on this pod. You may delete and recreate the pod.

Answer:
-------
Update the Pod to use the field automountServiceAccountToken: false

Using this option makes sure that the service account token secret is not mounted in the pod at the location /var/run/secrets/kubernetes.io/serviceaccount

Sample YAML shown below:

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: apps-cluster-dash
  name: apps-cluster-dash
  namespace: gamma
spec:
  containers:
  - image: nginx
    name: apps-cluster-dash
  serviceAccountName: cluster-view
  automountServiceAccountToken: false

---------------------------------------------------------------------------------------
Q-4)
A pod in the sahara namespace has generated alerts that a shell was opened inside the container.

To recognize such alerts, set the priority to ALERT and change the format of the output so that it looks like the below:

ALERT timestamp of the event without nanoseconds,User ID,the container id,the container image repository
Make sure to update the rule in such a way that the changes will persists across Falco updates.

You can refer the falco documentation Here

Answer:
------
Add the below rule to /etc/falco/falco_rules.local.yaml and restart the falco service to override the current rule.

- rule: Terminal shell in container
  desc: A shell was used as the entrypoint/exec point into a container with an attached terminal.
  condition: >
    spawned_process and container
    and shell_procs and proc.tty != 0
    and container_entrypoint
    and not user_expected_terminal_shell_in_container_conditions
  output: >
    %evt.time.s,%user.uid,%container.id,%container.image.repository
  priority: ALERT
  tags: [container, shell, mitre_execution]


Use the falco documentation to use the correct sysdig filters in the output.

For example, the evt.time.s filter prints the timestamp for the event without nano seconds. 
This is clearly described in the falco documentation here - https://falco.org/docs/rules/supported-fields/#evt-field-class

--------------------------------------------------------------------------------------
Q-5)
martin is a developer who needs access to work on the dev-a, dev-b and dev-z namespace. He should have 
the ability to carry out any operation on any pod in dev-a and dev-b namespaces. However, on the dev-z namespace, 
he should only have the permission to get and list the pods.

The current set-up is too permissive and violates the above condition. Use the above requirement and secure
 martin's access in the cluster. You may re-create objects, however, make sure to use the same name as the ones in effect currently.

Answer:
-------
The role called dev-user-access has been created for all three namespaces: dev-a, dev-b and dev-z. However, 
the role in the dev-z namespace grants martin access to all operation on all pods. To fix this, delete 
and re-create the role using the following YAML:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
    name: dev-user-access
    namespace: dev-z
rules:
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - get
    - list

-----------------------------------------------------------------------------------------
Q-6)
On the controlplane node, an unknown process is bound to the port 8088. Identify the process and prevent 
it from running again by stopping and disabling any associated services. Finally, remove the package that was responsible for starting this process.


Answer:
------
Check the process which is bound to port 8088 on this node using netstat:

controlplane $ netstat -natulp | grep 8088

This shows that the the process openlitespeed is the one which is using this port.
Check if any service is running with the same name:

controlplane $ systemctl list-units  -t service --state active | grep -i openlitespeed
lshttpd.service                    loaded active running OpenLiteSpeed HTTP Server

This shows that a service called openlitespeed is managed by lshttpd.service which is currently active.

Next, stop the service and disable it:

controlplane $ systemctl stop lshttpd
controlplane $ systemctl disable lshttpd

Finally, check for the package by the same name:

controlplane $ apt list --installed | grep openlitespeed

Uninstall the package:

controlplane $ apt remove openlitespeed -y

-------------------------------------------------------------------------------------------
Q-7)
A pod has been created in the omega namespace using the pod definition file located 
at /root/CKS/omega-app.yaml. However, there is something wrong with it and the pod is not in a running state.

We have used a custom seccomp profile located at /var/lib/kubelet/seccomp/custom-profile.json to 
ensure that this pod can only make use of limited syscalls to the Linux Kernel of the host operating
 system. However, it appears the profile does not allow the read and write syscalls. 
 Fix this by adding it to the profile and use it to start the pod.

Answer:
-------
The path to the seccomp profile is incorrectly specified for the omega-app pod.
As per the question, the profile is created at /var/lib/kubelet/seccomp/custom-profiles.json

controlplane $ kubectl -n omega describe po omega-app
.
.
.
Events:
  Type     Reason  Age              From             Message
  ----     ------  ----             ----             -------
  Normal   Pulled  5s (x3 over 7s)  kubelet, node01  Container image "hashicorp/http-echo:0.2.3" already present on machine
  Warning  Failed  5s (x3 over 7s)  kubelet, node01  Error: failed to generate security options for container "test-container": failed to generate seccomp security options for container: cannot load seccomp profile "/var/lib/kubelet/seccomp/profiles/custom-profile.json": open /var/lib/kubelet/seccomp/profiles/custom-profile.json: no such file or directory


Fix the seccomp profile path in the POD Definition file:

securityContext:
      seccompProfile:
        localhostProfile: custom-profile.json
        type: Localhost


Next, update the custom-profile.json to allow 'read' and 'write' syscalls.
Once done, you should see an output similar to below:

controlplane $ cat /var/lib/kubelet/seccomp/custom-profile.json | jq -r '.syscalls[].names[]' | grep -w write
write

controlplane $ cat /var/lib/kubelet/seccomp/custom-profile.json | jq -r '.syscalls[].names[]' | grep -w read 
read


Finally, re-create the pod

controlplane $ kubectl replace -f /root/CKS/omega-app.yaml --force


The POD should now run successfully.

NOTE: It may still run even if the above two syscalls are not added. However, adding the syscalls is required to successfully complete this question.

--------------------------------------------------------------------------------------
Q-8)
A pod definition file has been created at /root/CKS/simple-pod.yaml . Using the kubesec tool, 
generate a report for this pod definition file and fix the major issues so that the subsequent scan report no longer fails.

Once done, generate the report again and save it to the file /root/CKS/kubesec-report.txt


Answer:
-------
Remove the SYS_ADMIN capability from the container for the simple-webapp-1 pod in the POD definition file and re-run the scan.

controlplane $ kubesec scan /root/CKS/simple-pod.yaml > /root/CKS/kubesec-report.txt

The fixed report should PASS with a message like this:

[
  {
    "object": "Pod/simple-webapp-1.default",
    "valid": true,
    "fileName": "simple-pod.yaml",
    "message": "Passed with a score of 0 points",
    "score": 0,
  },
.
.
.

--------------------------------------------------------------------------------------------
Q-9)
Create a new pod called secure-nginx-pod in the seth namespace. Use one of the images from the below which has a least number of CRITICAL vulnerabilities.

    rancher/alpine-git:1.0.4
    nginx:1.19
    nginx:1.17
    nginx:1.20
    gcr.io/google-containers/nginx
    bitnami/jenkins:latest

Answer:
-------
Run trivy image scan on all of the images and check which one has the least number of CRITICAL vulnerabilities.

root@controlplane:~# trivy image --severity CRITICAL gcr.io/google-containers/nginx 
2022-01-25T20:36:08.878Z        WARN    You should avoid using the :latest tag as it is cached. You need to specify '--clear-cache' option when :latest image is changed
2022-01-25T20:36:08.881Z        INFO    Need to update DB
2022-01-25T20:36:08.882Z        INFO    Downloading DB...
25.67 MiB / 25.67 MiB [----------------------------------------------------------------] 100.00% 13.68 MiB p/s 2s
2022-01-25T20:36:11.357Z        INFO    Detecting Ubuntu vulnerabilities...
2022-01-25T20:36:11.372Z        INFO    Trivy skips scanning programming language libraries because no supported file was detected

gcr.io/google-containers/nginx (ubuntu 14.04)
=============================================
Total: 1 (CRITICAL: 1)

Next, use this image to create the pod

controlplane $ kubectl -n seth run secure-nginx-pod --image gcr.io/google-containers/nginx 




































































































