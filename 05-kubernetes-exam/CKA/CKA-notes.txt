Imp Points:
---------------------------------------------------------------
-)  Secrets are only "encoded" they are not "encrypted"
    Encoded:        <-- it means base64 encod-decode 
    Encrypted:      <-- it means we need public key to encrypt and private key to decrypt
    
--------------------------



i am on video: 169

Lectures need to see again
Node Selectors 62
Node Affinity  63

.-------------------------
80- Configuring scheduler profiles.


----------------------------------------------------
In Master-Node
---------------
Kube-api-server            6443   Port    (But somewhere i saw 8443 also)
coredns
etcd                       2379   Ports 
kube-proxy
kube-scheduler             10259  Port 
Kube-Controller-Manager    10257  Port 
kube-flannel-ds
Kubelet                    10250  Port          ( Kubelet can also be deployed on Master-node)

In-Nodes
-------
Kubelet                    10250  Port 
Kube-proxy                                      (watch in lecture 223  "Service Networking")
Services                   30000 till 32767 Ports  ( "> ps aux | grep kube-api-server" in this process or directly in the "kube-api-server.yaml  file we can see 
                                                                                       a attribute "--service-cluster-ip-range=x.x.x.x/x" from here we can find that 
                                                                                       under which range our cluster is alloweed to provide automatically a ip address 
                                                                                       to the "Service" when it is created 
----------------------------------------------------------



ETCD
=====

ETCD listen on default port  "2379"


CRI:       <-- "Container-Runtime-Interface"   e.g, Docker, RKT, CRI-O 
CNI:       <-- "Container-Network-Interface" 
CSI:       <-- "Container-Storage-Interface"   e.g AWS-EBS, AWS-EFS, Azure-Disks


Commands:
---------
> k config use-context "user.name"            <-- like this we can change the config, "user.name" come form the  
                                                "kubectl config view"  -> config.yaml -> users: -> name 

> alias k=kubectl                 <-- it will set k as alias for "Kubectl" command

> export ETCDCTL_API=3
> etcdctl snapshot save 
> etcdctl endpoint health
> etcdctl get
> etcdctl put

> cat /etc/kubernetes/manifests/kube-apiserver.yaml       <-- to view "api-server" options where cluster is created with "kubeadm" tool     
> ps -aus | grep kube-apiserver 

> kubectl -n <pod-name> exec -it <sidecar-container-name> -- cat /log/app.log 

> kubectl get nodes              <-- it also return the "Kubernetes version" which is installed.

> kubectl replace --force -f /tmp/kubectl-edit-2345685343.yaml

Imp Points:
-----------
-) Kubernetes Pod Network Ip-assign   10.32.0.0/12    it means   10.32.0.1  till 10.47.255.254  



Declarative commands   <-- when we use .yaml files, it means we are declearing resources in a .yaml file
Imperative commands  <-- wehn we try to create a resource directly through command line


Difference between "kubectl apply" and "kubectl create" and "kubectl replace"
----------------------------------------------------------------------------
> "Kubectl apply" creates kubernetes objects through "Declarative commands"  <- with this the last .yaml file will be stored under "metadata.annotations"
> "kubectl create" creates kubernetes objects through "Imperative commands"  <- with this the last .yaml file will not be stored under "metadata.annotations"
> "kubectl replace" creates kubernetes objects through "Imperative commands"  <- with this the last .yaml file will not be stored under "metadata.annotations"


Difference between  "kubectl replace" and "kubectl replace --force"  and "kubectl edit" commands
------------------------------------------------------------------
> "kubectl replace -f aby.yaml" replace kubernetes objects through "Declaratice commands", we first make change manually on .yaml file
   after that with "kubectl replace -f abc.yaml" it will just change the changes that we make, it will not delete and recreate the object

> "kubectl replace --force -f abc.yaml":   Wtih "--force" we are saying please delete the old object and then create the new object with the
                                          new configuration that we made

> "Kubectl edit" edits kubernetes objects through "Imperative commands", we can directly make the changes in live manifest, 
   But changes will not be recorded.


Note: 
Secrets are only encoded
Secrets are not encrypted in ETCD db of Kubernetes
Certificates are Encrypted.

Pod-Eviction-Timeout: 
---------------------
Kubernetes will wait for 5min by-default for a Pod if it goes offline, As the whole node goes down, if after 5min the pod does not 
comes up then kubernetes declare it dead and create new pod on the other available node only if it is a part of a "ReplicaSet" if it is not
a part of any "ReplicaSet" then the pod is completely deleted.
By-default  in the "kube-controller-manager" it is set to   "--pod-eviction-timeout=5m0s" 

If the Pod with the Node comes back inbetween 5min then it will be taken as live again and no process will be done further.

=============================================================================================================
Basic commands:
---------------

--dry-run=client  <-- this will not create the resource, instead, it will tell us weather the command is correct or not

--dry-run=client -oyaml  <-- it will check if the command is correct and generate a .yaml file also

> kubectl replace --force -f abc.yaml         <-- this command will delete the existing pod and create a new with this file
                                              normally we need to write first "delete" then "apply" but this command do both
                                              of the things in one time.



> kubectl explain pods   or   replicase  or deployment
> kubectl describe pods
> kubectl create pods --help
> kubectl create secret generic --help
> kubectl explain secret 
> kubectl run --help
> k taint --help 

> kubectl replace --force -f abc.pod          <-- like this it will forcefully delete and recreate the pod

> kubectl replace --force -f /tmp/kubectl-edit-3434343.yaml     <-- like this we can forcefully delete and recreate the pod

----------------------------------------
Pods :
------
> kubectl  run  mypod --image nginx -n my-namespace         <-- it will directly create a pod in "my-namespace"
> kubectl run mypod --image redis:alpine --labels="tier=db"
> kubectl run mypod --image redis:alpine --labels env=dec,ab=bc
> kubectl run my-pod --image nginx --port=8080
> kubectl run my-pod --image httpd:alpine --port 80 --expose true    <-- it create a "pod" and also a "Service" same time
> kubectl run my-pod --image busy-box --command sleep 3200 --dry-run=client -oyaml      
> k run mypod --image=ubuntu:18.4 --command -- sleep 1000            
> k run mypod --image=ubuntu:18.4 --dry-run=client --command -- sleep 1000              
> kubectl get pods 
> kubectl get pods -owide              <-- it also shows that on which node this pod is placed
> kubectl get pod -n my-ns --show-labels         <-- it will show the pods with their labels also
> kubectl describe pods 
> kubectl get pods --all-namespaces
> kubectl get pods -A
> kubectl get pods --selector app-labele=myvlaue          <-- like this we can search for pods that has a lable "app-labele=myvlaue"
> kubectl get pods --selector env=dev | wc -l             <-- the return the number of lines in the result, their is a HEADER in the result also
                                                              this is the reason we have to do -1 from the resutle, if result is 8, then 8-1= 7 is answer
> kubectl get pods --selector env=dev --no-headers | wc -l     <-- this will return a result with a number, in the result no HEADER is included 
                                                                  so as the last obove command we can directly get 7 as result.

> kubectl get all --selector env=prod --no-headers | wc -l   <-- return all the objects that has lable "env=prod"

> kubectl get all --selector  env=prod,tier=db,app=p-app 

> kubectl get pods --watch

> kubect set image   my-deployment my-container=nginx:1.9.1     <--like this we can set a new "image" in the Deployment 

> k api-resources                           <-- get all the resources 

> k api-resources --namespaced=true         <-- it will return a list of all the resources that we can create inside a "Namespace"

> k api-resources --namespaced=false        <-- it will return a list of all the resources that we can not create inside a "Namespace"
                                              those must be cluster scope 

######################################################################################################################################
Command from practise course:
----------------------------
> kubectl get pod -A --sort-by=.metadata.creationTimestamp           <-- sort all the pods as per Descending order as per creation-time
                                                                        by-default it output in Descending order

> kubectl get pod -A --sort-by=.metadata.creationTimestamp | tac      <-- with "tac" it will reverse the flow and make it to Ascending order


#####################################################################################################################################
ReplicaSet: 
-------------
> kubectl explain replicaset
> kubectl edit rs my-set
> kubectl scale rs my-set --replicase=5  

Deployment: 
-----------

> kubectl create deployment my-dep --image nginx --replicas=3

> kubectl create deployment --image nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml 

> kubectl create deploy

> kubectl expose deployment my-dep --port 80

> kubectl scale deloyment my-dep --replicas=5

> kubectl set image deployment my-deploy nginx=nginx:1.18

> kubectl get service 
> kubectl get svc
> kubectl describe svc my-service


Create a new Service
-------------------
> kubectl expose pod my-redis --port 6379 --name my-serice --dry-run=client -oyaml   <-- it will create a Serive and automatically exposes the port
                                                                                   or generate lable connection with the Pod name "my-redis"
                                                                                   it automatically uses the Pod lable and add in Service
                                                                                   By-default it creates a "clusterip" service
                                                                                   with this command we can not use --tcp=80:80 --node-port=30080 

> kubectl create service clusterip my-redis --tcp=6379:6379 --dry-run=client -oyaml     <-- the main point is that we can not add any selector with this 
                                                                                      command, it automatically only generate "app=my-redis" as tag
                                                                                      we can not explicitly define any selector with "kubectl create" command
                                                                                 
> kubectl expose pod my-pod --type NodePort --port=80 --name my-service --dry-run=client -oyaml    <-- it will create a new service and as we have used
                                                                                                    "kubectl expose" it will automatecally take the 
                                                                                                    Pod tags and put in Service-Selector
                                                                                                    with this command we can not use --tcp=80:80 --node-port=30080 

> kubectl create service nodeport my-service --tcp=80:80 --node-port 30080 --dry-run=client -oyaml    <-- the main point is that we can not add any selector with this 
                                                                                                       command, it automatically only generate "app=my-redis" as tag
                                                                                                      we can not explicitly define any selector with "kubectl create" command                                                                                                                 

---------------------------------------------------------------------------------
Service:
-------
Assume we have two namespaces  Namespace "A"   and Namespace  "B"

mysql.connect("db-service")          <-- if a web-app in Namespace "A" call a service in same namespace "A" then it directly use the service-name

mysql-connect("db-service.dev.svc.cluster.local")    <-- if a web-app in Namespace "A" call a service in different Namespace "B" this it should use 
                                                        this way, When we create a service a DNS name is created with this whole name "db-service.dev.svc.cluster.local"
                                                        in the Kubernetes cluster, As Service is a cluster specific object so when we call this DNS it will route 
                                                        request to this service

db-service         <-- name of the Service
dec                <-- name of the Namespace
svc                <-- It is a subdomain added in DNS, as we are calling a Service
cluster.local      <-- It is a subdomain for the cluster,As both of the Namespaces are in same cluster
--------------------------------------------------------------------------------

Labels and Selectors:
---------------------
> k get pods --selector app=App1

> k get pods --selector env=App | wc -l 
> k get pods --selector env=App --no-headers | wc -l 

> k label pod mypod env=dev,app=dev
> k label node node01 env=dev



> kubectl config set-context $(kubectl config current-context) --namespace=dev          <-- like this we can set the config to "dev" namesapce
                                                                                      it means when we do "k get pods" it will fetch pods details from "dev" namespace


ResourceQuota for Namespace
----------------------------
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec: 
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi  

-------------------------------------------------------------------------------------------------------
Taints:
-------
> kubectl describe node kubemaster | grep Taint         <-- this will show us the which "Taint" is applied on the Master-node

> kubectl taint node <any-node-name> key=value:Taint-Type

> kubectl taint node mynode  mykey22=myValue22:NoSchedule


To remove the Taint from a node
-----------------------------
> kubectl taint node mynode  node-role.kubernetes.io/master:NoSchedule-     <-- first go check the whole value of the Taint from the "describe node" command
                                                                            then put that value as above and at the end put "-" minus sign

Note: 
key       <-- it can be a string upto 253 characters
value     <-- it can be a string upto 63 characters


q) How to Taint a Node and define same Taint for the Pod ? 

Taint    
------

their are 3 tyes of "Taint" values we can use in "effect" key
1) NoSchedule                         <-- new pods that do not match the taint are not scheduled onto this node, But existing pods on the node remain running.
2) PreferNoSchedule                   <-- new pods that do not match the taint might be scheduled onto the node,
                                          but the existing pods on the node remain running.
3) NoExecute                          <-- in this case only same "Tainted" pods will be placed on this Node, and also if any old another "Tainted" pods
                                          are their then they will be removed from this Node

Note: we can use two types of"opertaor" 
1) Equal            <-- it means all   key,value,effect  all must be matched. (Imp:  this is a default parameter)
2) Exist            <-- it means  key,effect parameters must match, you must leave a blank value parameter, which matches any


> kubectl taint nodes node1 anyKeyName=blue:NoSchedule             <-- here we are "Tainting" a node, with  "key=app"   and "value= blue : NoSchedule" 
                                                            Note: this command we use to "Taint" a node

--------------------------------------------------
Defining a pod with the same "Taint" as we define above for the "Node"

apiVersion: v1                                                      
kind: Pod
metadata: 
  name: myPod-color
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  tolerations:
  - key: "mykey22"                    <-- this "anyKeyName" is the key defined above in the node command, we can put any name
    operator: "Equal"                 <-- as in the above Node command we use  "="   so here we have to put "Equal"
    value: "myvalue22"                <-- as in the above Node command we use Value = "blue" so here we are using blue
    effect: "NoSchedule"              <-- their are 3 types of values we can use as above defined, so here we are using one of them           
             
 Note: all the 4 values must be put inside the double quotes            

> kubectl describe node my-node 

> kubectl label node my-node my-lable=my-value

apiVersion: v1                                                      
kind: Pod
metadata: 
  name: myPod-color
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  nodeSelector:
    my-lable: my-value  

#######################################################################################################################
-----------------------------------------------------------------------------------------------------------
Node Affinity  
------------

q) Assume we have two Nodes, one with larger capacity means 5-cpu and 20GB ram and one Node has 2-cpu and 10-gb ram,
   how we can lable these Nodes and how we can Tell a Pod to go to a Larger Node

> kubectl label nodes node-1 size22=Large22                    <-- we can  put a Lable for the Node  "size=Large"  and if we put same lable in the 
                                                             Pod defination .yaml file, it means we are saying that that Pod should go to this
                                                             Node only   

options
1) requiredDuringSchedulingIgnoredDuringExecution      ( Must take this )
2) preferredDuringSchedulingIgnoredDuringExecution     ( it is just a advice )


Note: 
1) "requiredDuringSchedulingIgnoredDuringExecution" in this case  as per the condition if no "node" is found with matched label defined
   in the pod.yaml file then kubernetes will not create any pod and it will go in "Pending" stage, this "Pending" status shows us when we 
   do "kubectl get pods"
2) "preferredDuringSchedulingIgnoredDuringExecution" in this case as per the condition if no "node" is found with the matched label
   kubernetes will not stop the pod , it will just go to another node and create the pod 



apiVersion: v1
kind: Pod
metadata: 
  name: myapp-pod
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size22
            operator: In                        <-- using  "In"  operator
            values:
            - Large22                            <-- like this way we can define that this Pod can be placed on a Node which has a Lable
            - Medium22                               of   "size22=Large22"   or "size22=Medium22" 




"NotIn" example
---------------
apiVersion: v1
kind: Pod
metadata: 
  name: myapp-pod
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution
        nodeSelectorTerms:
        - matchExpressions:
          - key: size22
            operator: NotIn                          <-- using "NotIn"   <-- it means this Pod can go to any Node which do not have a Lable "size22=Small22"            
            values:
            - Small22

OR 
---
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution
        nodeSelectorTerms:
        - matchExpressions:
          - key: size22
            operator: Exists      <-- it checks only if the "key" with the name "size22" exists in the Node, then it directly place the pod in that Node


---------------------------------------------------------------
apiVersion: v1        
kind: Pod             
metadata:                 
  name: myapp 
  labels:              
    app: mypod22
    env: Prod-2                    
spec:             
  containers:                     
    - name: myapp-frontend             
      image: mydockerimage/1.0.0
      resources:
        requests:                         <--  "request" it means this pod must have minimum memory  
          memory: "64Mi"
          cpu: "0.5"          
        limits:                           <-- "limits"  it means this pods can take only memory upto  "4GB"
          memory: "4Gi"                      no more memory will be allocated to it.
          cpu: "1"    

Note: As laptops have 8-cores so it means 8 CPUs              

e.g
----
limits:
  cpu: 1             <- it means "1 CPU"  or "1 core" 

----
limits:
  cpu: 500m   or  100m  or 0.1          <- these all  means half CPU


e.g
---
limits:
  memory: 1Gi             <- it means the memory of "1 GB"  or "1024 MegaByte"   or "1024 Mi"


---
limits:
  memory: 512Mi             <- it means the memory of "512 MegaByte"    so we can say half of 1 GB

---------------------------------------------------------------------
ResourceQuota
-------------
apiVersion: v1
kind: ResourceQuota
metadata: 
  name: compute-quota                     
  namespace: dev                   
spec: 
  hard: 
    pods: "10" 
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi  


Note: we can set "ResourceQuota" at Namespace-level, every Namespace can have its own "Resource-Quota"

########################################################################################################################
-------------------------------------------------------------------------------
DaemonSet
--------

apiVersion: apps/v1   
kind: DaemonSet
metadata:
  name: monitoring-daemon 
spec:
  selector:
    matchLabels: 
      app: monitoring-agent-a            
  template:    
    metadata:                     
      labels:              
        app: monitoring-agent-a         
    spec:             
      containers:                  
        - name: monitoring-agent
          image: mydockerimage/2.0.0   

> kubectl get daemonsets

> kubectl describe daemonsets my-daemonset



How to create the path for the "Static-pod" in a Node ?

Kubelet.service file
---------------------

Way-1) 
------
ExecStart=/usr/local/bin/kubelet  \\
  --pod-manifest-path=/etc/Kubernetes/manifests \\           <-- here we are defining path for the Static-pod that is created in the Node


Way-2) (when we create cluster with "cubeadmin tool" then it usese the "way-2")
------
ExecStart=/usr/local/bin/kubelet  \\
  --config=kubeconfig.yaml \\                   <-- we can define the above path in a "kubeconfig.yaml" file and put the file name here 

Note: in the "kubeconfig.yaml" file we can find this path "staticPodPath: /etc/kubernetes/manifests"

######################################################################################################################
Static Pod
-------------
> cat var/lib/kubelet/                  <-- All 4 files i can find under it.

> cat var/lib/kubelet/config.yaml             <-- in this file "staticPodPath:  ...."  <-- see this key hier the path is defined
                                                  It is the kubelet on the Node who is creating the pod 
> cat var/lib/kubelet/kube-apiserver.yaml                                                  

> cat var/lib/kubelet/kube-controller-manager.yaml

> cat var/lib/kubelet/kube-scheduler.yaml 

> cat /etc/kubernetes/manifests/           <-- under this folder we can place all the "Static-pod"  yaml files.



#########################################################################################################################
Multi Scheduler:
================

> kubectl get events -o wide          <-- under it we can see if the new Pod is running with a custom-scheduler or not

> kubectl logs my-scheduler -n kube-system         <-- we can view logs of out scheduler

> k get events -o wide                    <-- like this we can see all the events, e.g  when a pod is created then 
                                          which "scheduler" has placed this pod in the node, we can se scheduler name in event

> k logs my-scheduler -n kube-system            <-- like this we can also see the logs of the "Scheduler" 


kube-scheduler.service    (Default Scheduler)
----------------------
ExecStart=/usr/local/bin/kube-scheduler  \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml


my-scheduler.service
--------------------
ExecStart=/usr/local/bin/kube-scheduler  \\
  --config=/etc/kubernetes/config/my-scheduler.yaml


----------------------------------------------------
How to define a Custom Scheduler ?

Step-1)
--------
my-scheduler.yaml
--------
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: true
  resourceNamespace: kube-system
  resourceName: lock-object-my-scheduler  


Step-2) 
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
  namespace: kube-system
spec:
  containers: 
    - command:
      - kube-scheduler
      - --address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --config=/etc/kubernetes/my-scheduler.yaml
      image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
      name: kube-scheduler    

Step-3)
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
  namespace: kube-system
spec:
  containers:
    - image: nginx
      name: nginx
  schedulerName: my-scheduler    


--------------------------------------------------------------------------------------------
PriorityClass:
-------------

How to create a PriorityClass ?

Step-1)  
Create a priorityClass file

abc.file
--------
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: my-high-priority
value: 100000
globalDefault: false


Step-2)

pod.yaml
-------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  priorityClassName: my-high-priority
  containers:
    - name: nginx
      image: nginx
      resources: 
        request: 
          memory: "1Gi"
          cpu: 10

########################################################################################################################
Monitoring
----------

> kubectl top node            <-- to get the metrics of all the Nodes
> kubectl top pod             <-- to get the metrics of all the pods

> kubectl logs -f my-pod           <-- it will return the logs of my-pod


> kubectl logs -f my-pod container-1       <-- if we have two container running in a pod, e.g one service and other sidecar contianer
                                               then we have to give the name in the command
> kubectl logs -f my-pod sidecar-container                               



######################################################################################################################
Rollout:
-------

Note: Rolling-update is the default deployment strategy. if we do not define any strategy, then it select by-default "Rolling-update" 
      as a strategy.

Rolling-update Strategy:
---------------
In Rolling-update if we have 5 pods are already their, and we want to update them with new version, then while doing
deployment, it delete one pod and bring one new, in this way it goes.

Recreate Strategy:
In this case it first delete all the pods and after that it create all new pods at once, this strategy can have downtime 
because at the time of first delete all the pods are deleted.


> kubectl get deployments 

> kubectl create -f abc.yaml          <-- with this command we can create a deployment first time

> kubectl apply -f abc.yaml           <-- As a deployment is already create, with this command it will update the Deployment
                                         If their is no deployment then with this command we can also deploy it first time

> kubect set image   my-deployment my-container=nginx:1.9.1    <-- like this we can change the image of current running deployment   
                                                                 my-deploment is the name of the Deployment
                                                                 my-container is the name of the Container                                      


> k rollout status deployment/my-deploy      <-- it will log the status of the deployment

> k rollout history deployment/my-deploy     <-- it will print the history of the deployemnt

> kubectl rollout status my-deployment      <-- with this we will get the status of the deplyoment

> kubectl rollout history my-deployment   

> kubectl rollout undo my-deployment          <-- with this we can rollback the changes to last deployed deployment.
 

###############################################################################
Docker Commands:
---------------

Note:  ENTRYPOINT ["sleep"]   or  CMD ["sleep"]     <-- in the both commands the first argument must be a executable
                                                        it means we must put ["sleep"]   if we put ["10"]  then we will get error
                                                        but if we use ENTRYPOINT and CMD then in the second place CMD can have 
                                                        CMD ["10"]  as argument.

Case-1) 
-------

dockerfile-a.yaml
-----------------
FROM Ubuntu
CMD sleep 5

> docker run dockerfile-a                     <-- in this case the Ubuntu pod execute for 5sec after that it destroys



Case-2) 
-------

dockerfile-a.yaml
-----------------
FROM Ubuntu
CMD sleep 5

> docker run dockerfile-a    sleep 10           <-- As we have used "CMD", then in this case which argument we pass while doing
                                                 "docker run dockerfile-a"  the whole argument will be replaced
                                                 "CMD sleep 5"   will be replaced to "CMD 10"   at the run time
                                                 if we just pass  "docker run dockerfile-a 10"  then we will get error 
                                                 as with the CMD the whole argumes will be replaced


Case-3) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         

> docker run dockerfile-a 10           <-- as we use "ENTRYPOINT" then any argument what we put in the command will be added at 
                                         the execution time. at the end as we pass "10" it will be "ENTRYPOINT sleep 10"


Case-4) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         

> docker run dockerfile-a         <-- in this case we forget to write the argument in the command so in this case 
                                 it throws error as at the end it rund "ENTRYPOINT sleep" only



Case-5) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         
CMD["10"]

> docker run dockerfile-a            <--in this case as we forget to put an argument then by-default it take 10 from CMD






Case-6) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         
CMD["10"]

> docker run dockerfile-a   20              <-- AS which argument we put at the run time it totally override the "CMD"
                                               so  "20" will override with CMD["20"] 

Case-7) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         
CMD["10"]

> docker run --entrypoint sleepAA dockerfile-a 20           <-- like this we can change the "ENTRYPOINT" argument from
                                                            "ENTRYPOINT ["sleep"]"   to "ENTRPOINT ["sleepA"]"
                                                            At the runtime it become  "sleepA 20"
                                                            Note: Their is not "sleepA" command, it is just for example i am showing


Case-8)

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command: ["sleep"]             <-- as in the above example "command" is same as "ENTRYPOINT"
      args: ["10"]                   <-- as in the above example "args" is same as "CMD"
 



Case-9)

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command: ["sleep" ,"10"] 


or


pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command:
      - "sleep"
      - "10"

or 

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command: ["sleep"]                    <-- command is same as "Entrypoint"
      args: ["10"]


> kubectl run my-pod --image-nginx --command -- sleepA     <-- like this we can override the "command: ["sleepA"]" argumen, 
                                                              as we do not define anything else so it take "10" from "args"

> kubectl run my-pod --image-nginx --command -- sleepA  2000   <-- like this we can override the "command: ["sleepA","2000]"  

> kubectl run my-pod --image=nginx -- 15                   <-- like this we can override the "args: ["15"]" argument
or
> kubectl apply -f pod.yaml -- 15                       <-- this is just my try example, but not sure if it works

> kubectl run my-pod --image=nginx -- sleep 15         <-- like this we can override the "args: ["sleep","15"]" argument


###################################################################################################################
Environment Variables  with ConfigMap and secret
-------------------------------------

Case-1) 
Environment varibale directly in the Pod

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
      env:                                      <-- like this we can define an "Environment variable", it will be then placed in the container 
        - name: my-key-a                            so the service can call directly from code the Environment variable
          value: my-val-a   
        - name: my-key-b
          value: my-val-b   

--------------------------------------------------------------------------------
Case-2) 
ConfigMap  & Secret
-------------------

Environemnt variable adding in pod through configMap

> kubectl get configmap
> kubectl describe configmap

> kubectl create configmap my-configmap --from-literal=my_key_AA=my_value_AA --from-literal=my_key_BB=my_value_BB 

> kubectl create configmap my-configmap --from-file=abc.properties                   <-- write the key-value in a file and put the file name here

> kubectl get secret 
> kubectl create secret my-secret --from-literal=my_key_AA=my_value_AA --from-literal=my_key_BB=my_value_BB 

> kubectl create secret my-secret --from-file=abc.properties                    <-- write the key-value in a file and put the file name here


> echo -n "my-pass | base64
> echo -n "cm9cdA==" | base64 --decode


Step-1)
configMap.yaml
--------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap-A
data:
  APP_COLOR: blue
  APP_MODE: prod

Step-2)
secret.yaml
--------
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret
data:
  my-username: dxNlcm5hbWU=
  my-password: cGFzc3dvcmQ=

Step-3)

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080     
      envFrom:
        - configMapRef:
            name: my-configmap-A                   <-- here we are putting the above define configmap
      envFrom:
        - secretRef:                               <--here we are refering the whole Secret to the pod 
            name: my-secret                           it means all the key-value will be inserted in as evn-variable

--------------------------------------------------------------------------
Case-3) 
Environment variable in ConfigMap or Secret in a Volume for Pod 
---------------------------------------------


Step-1)
configMap.yaml
--------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap-A
data:
  db-host: my-db-pass


Step-2)
secret.yaml
--------
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret
data:
  my-username: dxNlcm5hbWU=
  my-password: cGFzc3dvcmQ=


Step-3)
pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080     
      env:
        - name: MY_DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: MY_USERNAME                  <-- here we are inserting only this key from the secret, as env variable 
        - name: MY_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: my-password
        - name: MY_CONFIGMAP
          valueFrom:
            configMapKeyRef:
              name: my-configmap-A
              key: db-host

---------------------------------------------------------------------------------------------------
ConfigMap and Secret as a Volume
-----------------------------

Step-1) 
configMap.yaml
--------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap-B
data:
  mydata.conf: |               <-- this is a type of a file
    data_ aaaa
    data_ bbbb
    data_ cccc


Step-2)
secret.yaml
--------
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret-B
type: Opaque
data:
  secret.file |                                          <-- like this it is used as a file
    dfdjdkfjekERDF435454DFEjkgjkgRDLOIGME=    


Step-3)
pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
      volumeMounts:
        - name: my-confg-map-volume-DD 
          mountPath: /tmp/configmap/new-folder-in-container         <--with this we are creating a new container in the pod and the file in the configMap 
                                                                       "mydata.conf" will be placed in this newely created folder
        - name: my-secret-volume-DD
          mountPath: /tmp/secret/new-folder-in-container
  volumes:
    - name: my-confg-map-volume-DD                  <--- here we can give any name, it will just show in the container
      configMap:
        name: my-configmap-B                     <-- here we need to correct name for the mapping the above create config-map
    - name: my-secret-volume-DD
        secret:
          secretName: my-secret-B

##########################################################################################################
Multicontainer pods,  Sidecar
-----------------------------
> kubectl logs <pod-name> -c <container-name>       <--like this we can give the continer-name and see the logs


pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
    - name: my-sidecar-pod
      image: nginx    


----------------------------------------------------------------------
Attach only a "Volume" to containers in a Pod
---------------------------------------------

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
      volumeMounts:
        - mountPath: /var/log/event-simulator/
          name: my-ubunut-volume
    - name: my-sidecar-pod
      image: nginx   
      volumeMounts:
        - mountPath: /log
          name: my-sidecar-volume

####################################################################################################
InitContainers:
-------------

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
  initContainers:
    - name: my-init-container
      image: busybox
    - name: second-init-container
      image: nginx  

> k logs <pod-name> -c <init-contaier-name>               <-- like this we can see the logs coming from "InitContainer"      


######################################################################################################
Node:
----

-> ssh <node-name>    <-- like this we can switch to or we can say go inside the "node" that the name is defined 
-> exit               <-- to come out from the node back. 

Note: 
-) when a node goes offline the master-node wait for 5min before considering it dead. it is defined in the
   "kube-controller-manager"  --pod-eviction-timeout=5m0s", After this when a new Node come in place of old one 
  then it comes blank with no pods scheduled on it. 




> kubectl drain my-node-1          <-- if we want to delete all the pods on the node, after this command no pod will be available on the node
                                       and the "kube-scheduler" will not place new node on this node, Internally it do "kubectl cordon" that 
                                       so that no new node will be placed on the Node

> kubectl drain my-node-1 --ignore-daemonsets

> kubectl drain my-node-01 --force                                        

> kubectl uncordon my-node-1      <-- Only after doing "uncordon" the Node come online again and the "kube-scheduler" will start placing 
                                      Pods on this Node again.

> kubectl cordon my-node-1        <-- It marks the Node as unscheudable, After this command no new Pods will be place on this node,
                                      But the old Pods will still remain running on this Node, Only after "kubectl uncordon" the 
                                      Kube-scheduler is alloweed to place the pods again on this Node.



#############################################################################################################
Cluster Upgrade
----------------
if i am haveing kubernetes version v1.13.4

kube-apiserver      v1.13.4
Controller-manager  v1.13.4
Kube-scheduler      v1.13.4
Kubelet             v1.13.4
kube-proxy          v1.13.4 
kubectl             v1.13.4

ETCD Cluster        v3.2.18 
CoreDNS             v1.1.3         <-- these both of them have their own release versions, they are not same as kubernetes-version


Imp-points:
----------
-) As "Kube-apiserver" is the main component and all the other components are taking through "kube-apiserver" so in this case 
  all other component must be lower or equal to the version of "kube-apiserver".

-) If "kube-apiserver" has "v1.13.4" then  "Controller-manager" and "kube-scheduler" can be either has same version or can be one version lower then "kube-apiserver" version
   It means they can be on "v1.12"  version but not below the -1 version 

-) If "kube-apiserver" has "v1.13.4" then "kubelet" and "kube-proxy" can be either has same version or can be two version lower then "kube-apiserver" version   
   It means then can be on "v1.12" or "v1.11"

-) If "kube-apiserver" has "v1.13.4" then "kubectl" (Kubectl that we have installed on our computers), "Kubectl" can be one version higher or one version lower then
   the "kube-apiserver" version. it means "kubectl" can be "v1.12" or "v1.14"

Note:
-) Upgrading Cluster is always first we Upgrade "Master-node" after we upgrade "Worker-nodes"

-) If we upgrade the cluster with "kubeadm" tool then we should upgrade "kubelet" manually on every Node extra.
-) We must upgrade the kubeadm-tool itself before upgrading the cluster













> kubeadm upgrade plan

> kubeadm upgrade apply

Upgrading process with kubeadm tool

Step-1)  
> apt-get upgrade -y kubeadm=1.12.0-00            <-- First upgrade the "kubeadm" itself

Step.2) 
> kubeadm upgrade apply v1.12.0                   <-- After that upgrade the cluster

Step-3) 
Upgrade "Kubelet" on the Master-Node, Note: we do not need to upgrade the "Kubeadm" on the Master-node

> apt-get upgrade -y kubelet=1.12.0-00            <-- After upgrading the Control-plane on Master node, we have to upgrade the "Kubelet" first on the "Master" only
> systemctl restart kubelet                       <-- After upgrading the "Kubelet on the node, we have to restart the "Kubelet" then only the upgrade effects

Step-4) 
Upgrade "Kubeadm" and "Kubelet" on Worker Node

> kubectl drain my-node-1
> apt-get upgrade -y kubeadm=1.12.0-00
> apt-get upgrade -y kubelet=1.12.0-00
> kubeadm upgrade node config --kubelet-version v1.12.0
> systemctl restart kubelet 

> kubectl uncorden my-node-1

########################################################################################################################################
ETDC Backup
-----------

> ps -ef | grep -i etcd   <-- we can also get all the endpoint info from a "process", like this we are getting info from a "etcd" process 
                                      

> k get pods -n kube-system   
> k describe pod etcd-controlplane   <- under etcd.Image  <-- here in the image-name we can find the deployed version of "ETCD"



> export ETCDCTL_API=3    <-- like this we can set the ETCDCTL version to "3", normally we need this "ETCDCTL_API=3 command before 
                              every "etcdctl" command, so just "export" it then we do not need to put it in front of every command.
> etcdctl version          

-) ETDC cluster is placed on the master-node

-) "etcd.service"   <-- under this file we can find "--data-dir=/var/lib/etcd" <-- here by-default kubernetes store all the data for "ETCD"

Steps for Backup using snapshot:
---------------------
> etcdctl snapshot save snapshot.db 
> etcdctl snapshot status snapshot.db 

Steps for Restore ETCD db from Backup 
------------------------------------
> service kube-apiserver stop                                                     <-- first stop the "kube-apiserver"
> etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup       <-- here we put the path where the backup is stored

> change the new data directry path in teh "etcd.service" path. directory name specified in the above command "etcd-from-backup"

> systemctl daemon-reload
> service etcd restart 
> service kube-apiserver start 

Imp: 
-) With all the "etcdctl" we shoud add these 4 commands also 
--endpoints=https://127.0.0.1:2379
--cacert=/etc/etcd/ca.crt 
--cert=/etc/etcd/etcd-server-crt 
--key=/etc/etcd/etcd-server.key 







##########################################################################################################################################
Certificates
-------------

Public key extensions:
------------------------
*.pem 
*.crt             <--  Those end with these are "Public Key" or "Public Certificates"


Private key extensions:
----------------------
*.key
*-key.pem            <-- Those have in the name ".key" or "-key" are "Private Key"


> cat aaa.csr | base64             <-- it will encode the Certificate but print the certificate in multiple-lines 
> cat aaa.csr | bswe64 -w 0        <-- it will endcode the certificate and print the certificate as a Single-line




Note: Their are three types of Certificates:

Root certificates:     <-- These are configured on the "CA" (Certificate Authority)
Server Certificates:   <-- These are configured on the Servers 
Client Certificates:   <-- These are configured on the Clints  


---------------------------------------------------------------------
"Servers Components" in Kubernetes Cluster
-------------------------------------------
These below are three server and should have Server-Certificates

1) Kube-API Server 
2) ETCD     Server 
3) Kubelet  Server 


"Client Components" in Kubernetes Cluster
-----------------------------------------
These below are clients and should have Client-Certificates 

1) Kube-Scheduler           <-- It is a Client for Kube-api server, so we generate a Client-Certificate for it 
2) Kube-Controller-Manager  <-- It is a Client for "kube-api" server, so we generate a Client-Certificate for it 
3) kube-proxy               <-- It is a client for "kube-api" server, so we generate a Client-certificate for it. 






CERTIFICATE AUTHORITY ( CA )
----------------------------

> openssl genrsa --out ca.key 2048                                              <-- ca.key     it Generatey-key
> openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA"  -out ca.csr           <-- ca.csr     It generate "Certificate-Signing-Request"
> openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt                      <-- ca.crt     It generate "Sign-Certificates"


ADMIN USER:
-------------
> openssl genrsa -out admin.key 2048                                                      <-- admin.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin/O=system:masters" -out admin.csr   <-- admin.csr  It generate "Certificate-Signing-Request"
                                                                                           "O=system:masters"  <-- with this we are saying that this is not 
                                                                                                                  a normal user, it is a admin-user
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out ca.key                          <-- ca.crt     It generate "Sign-Certificates"

KUBE SCHEDULER
-------------
> openssl genrsa -out scheduler.key 2048                                            <-- scheduler.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin" -out scheduler.csr          <-- scheduler.csr  It generate "Certificate-Signing-Request"
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out scheduler.key             <-- scheduler.crt  It generate "Sign-Certificates"


KUBE CONTROLLER MANAGER
-----------------------
> openssl genrsa -out controller-manager.key 2048                                      <-- controller-manager.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin" -out controller-manager.csr    <-- controller-manager.csr  It generate "Certificate-Signing-Request"
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out controller-manager.key       <-- controller-manager.crt  It generate "Sign-Certificates"

KUBE PROXY
----------
> openssl genrsa -out kube-proxy.key 2048                                      <-- kube-proxy.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin" -out kube-proxy.csr    <-- kube-proxy.csr  It generate "Certificate-Signing-Request"
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out kube-proxy.key       <-- kube-proxy.crt  It generate "Sign-Certificates"


---------------------------------------------------------------------------------------------------------
> cat  /etc/kubernetes/manifests/kube-apiserver.yaml 

> openssl x509 -in <path to xxx.crt>  -text -noout                        
> openssl x509 -in /etc/kubernetes/oki/apiserver.crt -text -noout                <-- like this we can get all the info about the cretificate.
                                                                                   With "-noout" it will not create a new file with the Certificate 
                                                                                   it will just print the Certificate info on the console  
  
> k get csr                                         <--- to see all the "Certificate Signing Requests"
> k certificate approve <name-of-Certificate>       <-- with this we can approve the Certificate
> k get csr <name-of-certificate> -oyaml            <-- like this we can see the details inside the certificate 

> k certificate deny <name-of-certificate>          <-- like this we can deny a "Certificate Signing Request" 

> k delete csr <name-of-certificate>                <- in last step first we "deny" the request and after that we delete the CSR request. 










How we can make a REST call to "KUBE-API" server with these admin Certificates and keys ? 

> curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt    <-- This is the example of REST-Api call that we make 
                                                                                                       to the "KUBE-API" server.
  






#######################################################################################################
kubeconfig
----------

./kube/config            <-- be default "kubectl" look for the "config-file" under this folder 


> kubectl config -h                                   <-- like this we can list all the commands  for "config"

> kubectl config view                                 <-- with this we can view the config 

>> kubectl config use-context abc@production          <-- like this we are setting the default config to "abc@production" 

> kubectl config --kubeconfig=my-base-config use-contexts dev-frontend   <-- if we have two "./kube/config" files then we can add 
                                                                            the file name like this "--kubeconfig=my-base-config"

> k config use-context research --kubeconfig <path-to-file>       <-- "--kubeconfig" this attribute is used when we put the .config file 
                                                                     in anyother location rather then ./kube/config

> mv /root/my-kube-config   /root/.kube/config           <-- like this we can move from other place to default folder  the .config file

Steps to create Context file
----------------------------

Step-1) Add Cluster details
> kubectl config --kubeconfig=base-config set-cluster development --server=https://1.2.3.4           <--"my-base-config" is the file name for config

Step-2) Add user details
------------------------
> kubectl config --kubeconfig=base-config set-credentials my-user-a --username=dev --password=my-pass       <-- "my-user-a" is the name of the user for this config file.

Step-3) Setting Context 
> kubectl config --kubeconfig=base-config set-context dev-frontend --cluster=development --namespace=frontend --user=my-user-a




###################################################################################################################
RBAC    Role, Rolebinding, ClusterRole, ClusterRolebinding
----

Core API Group:  
---------------
Pods, Nodes, Namespaces, Services, Configmaps, Secrets, Events, Endpoints, PV, PVC 

Named API Groups:
-----------------
apps:           Deployments, ReplicaSets, StatefulSets, DaemonSets, DeploymentsRollback
extensions:     Ingress, NetworkPolicies, PodSecurityPolicies
batch:          Jobs, CronJobs
storage.k8s.io: StorageClass, VolumeAttachment
policy:         PodDisruptionBudgets



Role:
-----

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]                       <--"" indicates the core API group, Not the "Named Api Group" (imp)
  resources: ["pods"]
  verbs: ["get", "watch", "list"]



apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane                                  <-- "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role                                    <-- this must be Role or ClusterRole
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

> cat /etc/kubernetes/manifests/kube-apiserver.yaml       <-- to view "api-server" options where cluster is created with "kubeadm" tool     
> ps -aux | grep authorization 







> kubectl auth can-i  create deployments             <-- like this we can check if the current user is alloweed to "Create Deployments"

> kubectl auth can-i create pods                     <-- like this we can check if the current user is alloweed to "Create Pod"       

> kubectl auth can-i create pods --as dev-user       <-- like this we can check the permission for "dev-user" if he is alloweed or not 

> kubectl auth can-i create pods --as dev-user --namespace  my-ns-a  <-- like this we can check it "dev-user" has permission to create pod in the
                                                                             "my-ns-a" Namespace

> kubectl create role my-role --verb=list,create,delete --resource=pods 

> kubectl create role my-role --verb=list,create,delete --resource=pods --resourceNames=pod-a,pod-b   <-- this role is applied only for "pod-a" and "pod-b"

> kubectl create rolebinding my-rolebinding --role=my-role --user=dev-user




--------------------------------------------------------------------------------------
ServiceAccount 
--------------

serviceAccountName:      <-- this is the name of attribute that we should put under Deplyoment -> template -> spec -> serviceAccountName: xxx

imagePullSecrets:       <-- this we have to put under Deployment -> template -> spec -> imagePullSecrets: -> -name: xxxx    

Note: 
-) if we create new attribute in a Deployment for serviceAccount then it automatically recreate the pods with new SA 
-) But if we directly change the SA inside a pod then, we have to recreate the pod, then only it effects, so better to put 
inside the pod.yaml file 


> kubectl create serviceaccount my-sashboard-sa

> kubectl create token <service-account-name>            <-- like this we can create a Token for a existing "service-account"

> k create secret docker-registry -help        <-- "docker-registry" is  a type of special secret that we should create to pull images from DockerHub  
                                                  like this we can get help for creating a secret for pulling Image from DockerHub



------------------------------------------------------------------------------------------
Security contexts:
------------------

Pod level Security context
---------------------------
Pod -> spec -> securityContext -> runAsUser: 1010     <-- Pod level "securityContext"
                                                         like this we can forcefully say the pod to run all the process not as root-user 
                                                        it will run all the process as normal "user" with Id "1010"


Container level Security context
--------------------------------
Pod  -> spec -> containers -> securityContext -> runAsUser: 1010     <-- As we are defining the attribute "securityContext" under "containers"
                                                                        it is then a "container level" security context

SYS_TIME capability:
-------------------
pod -> spec -> containers -> securityContext -> capabilities -> add -> ["SYS_TIME"]        <-- note that the pod must run as "root user" then only it works
 

NET_ADMIN capability:
-------------------
pod -> spec -> containers -> securityContext -> capabilities -> add -> ["NET_ADMIN"]        <-- note that the pod must run as "root user" then only it works

-------------------------------------------------------------------------------------
Network Policies:
----------------
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db                                     <-- this is the pod to which the other Pods trying to access, 
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock: 
            cidr: 172.17.0.0/16                    <-- IP of the pod that from which we trying to access the main-pod 
            except:
              - 172.17.1.0/24
        - namespaceSelector:                      <-- Namespace in which all the pods that can access the main-pod
            matchLabels:
              project: myproject
        - podSelector:                             <-- the pod form which we want to  access the main-pod. This Pod should be in the same namespace that 
            matchLabels:                               the main-pod is having
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:                           <-- this is the IP that the main-pod can send request to 
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978
    - to:          
        - namespaceSelector:                      <-- Namespace to which the main-pod can send request to 
            matchLabels:
              project: myproject
      ports:
        - protocol: TCP
          port: 5978              
    - to:                        
        - podSelector:                             <-- Pod that the main-pod can send the request
            matchLabels:                          
              role: fronten
      ports:
        - protocol: TCP
          port: 5978

> k get networkpolicies 
> k get netpol my-network-policy 


######################################################################################################################
Storage:
-------



====================================================================================================
Volume Examples:
----------------

1) Storing Data directly from Pod in the 

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - image: alpine
      name: alpine
      command: ["/bin/sh", "-c"]
      args: ["shuf -i 0-100 -n l >> /my-storage-path/endnumber.out;"]
      volumeMounts:
        - mountPath: /my-storage-path        <-- (1) we are creating a "Volume" with path "/my-storage-path" in the container. 
          name: my-data-volume                       The service running on the pod will send logs to /my-storage-path folder.
  volumes:                                           Note: we can have two different path names but the "Volume" name must be same
    - name: my-data-volume
      hotstPath:
        path: /data                <--(2) Any data that comes from Pod in the "/my-storage-path" that will be stored at the end in the 
        type: Directory                    "/data" folder on the Node, as we defined "type: Director" so it create a folder on the Node



2) Create a Storage with AWS-EBS

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - image: alpine
      name: alpine
      command: ["/bin/sh", "-c"]
      args: ["shuf -i 0-100 -n l >> /my-storage-path/endnumber.out;"]
      volumeMounts:
        - mountPath: /my-storage-path        <-- (1) we are creating a "Volume" with path "/my-storage-path" in the container. 
          name: my-data-volume                       The service running on the pod will send logs to /my-storage-path folder.
  volumes:                                           Note: we can have two different path names but the "Volume" name must be same
    - name: my-data-volume
      awsElasticBlockStore:
        volumeID: <ID-Nummber>                <--(2) Any data that comes from Pod in the "/my-storage-path" that will be stored at the end in the 
        fsType: ext4                          "/data" folder on the Node, as we defined "type: Director" so it create a folder on the Node


=====================================================================================================================
Persistent Volume:
-----------------

Commands:

> kubectl get pv
> kubectl get pvc
> kubectl delete pvc my-claim-name
> 


-) Their are three types of "PersistentVolume" modes
   - RWO--ReadWriteOnce        <-- the volume can be mounted as read-write from pods placed in a single node. ReadWriteOnce access mode still can    
                                    allow multiple pods to access the volume when the pods are running on the same node.
   - ROX--ReadOnlyMany         <-- the volume can be mounted as read-only from pods placed in many nodes.
   - RWX--ReadWriteMany        <-- the volume can be mounted as read-write from pods placed in many nodes.
     RWOP--ReadWriteOncePod    <-- 

-) The "accessModes" of both PV and PVC should be match, then only PVC can claim a PV 

>) persistentVolumeReclaimPolicy: Retain   <-- After we delete a PVC, the attached PV will be remain their till we manually delete it, 
                                               It can not be re-used by any other Claims.
>) persistentVolumeReclaimPolicy: Delete   <-- when we delete the PVC the attached PV will also be delete at the same time

>) persistentVolumeReclaimPolicy: Recycle   <-- In this case when a PVC is deleted, the data stored in the PV will also be deleted,
                                              But the PV remain their, and after this recycle process the PV can be claimed by any other PVC 
                                               

Step-1) Create "Persistent Volume"

pv.yaml
--------
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: my-per-volume
  labels:
    name: my-pv
spec:
  accessModes:
    - ReadWriteOnce        
  capacity:
    storage: 1Gi
  persistentVolumeReclaimPolicy: Retain  
  hostPath:
    path: /data     
    type: Directory                    <-- "type: Directory" it is optional, if we use "hostPath" then it is automatically taken



pv.yaml
--------
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: my-per-volume
  labels:
    name: my-pv                    <-- here we can define a lable in the PV
spec:
  accessModes:
    - ReadWriteOnce        
  capacity:
    storage: 1Gi
  persistentVolumeReclaimPolicy: Delete  
  awsElasticBlockStore:
    volumeID: <ID-Nummber>  
    fsType: ext4 


Step-2) Create "Presistent Volume Claim"

pvc.yaml
--------
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: my-claim-aaaa
spec:
  selector:
    matchLabels:
    name: my-pv                       <-- As we have define the "Selector" then it search for "PV" with this lable and attach to it
  accessModes:
    - ReadWriteOnce        
  resources:
    request:
      storage: 500Mi



Step-3) Attach the above created "PVC" in the Pod.yaml below

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim-aaaa

----------------------------------------------------------------------------------------------------------
StorageClass:
------------

> k get storageclass 
> k get sc 


StorageClass can use three type of Disks that we define under "Parameters"

Silver StorageClass:               Gold StorageClass:          Platinum StorageClass:
-------------------                -----------------           ----------------------
type: pd-standard                  type: pd-standard           type: pd-ssd
replication-type: none             replication-type: none      replication-type: none 


Note:
volumeBindingMode: WaitForFirstConsumer  or  Immediate

Step-1) Create a "StorageClass"

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: my-storage-class-bb
provisioner: ebs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer 
parameters:
  type: pd-standard 
  replication-type: none  


Step-2) Create PVC 

pvc.yaml
--------
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: my-claim-aaaa
spec:
  storageClassName: my-storage-class-bb
  accessModes:
    - ReadWriteOnce        
  resources:
    request:
      storage: 500Mi


Step-3) Attach the above created "PVC" in the Pod.yaml below

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim-bb


#################################################################################################################################
Networking:
----------

----------------------------------
> route  
> ip route           <-- both of these command we use to see the "routing-table" entries 
-----------------------------------

> ip link    <-- to list and modify interface on the host.

> ip addr    <-- to see the ip-addresses link to above interfaces.

> ip addr add 192.168.1.10/24 dev eth0         <-- like this we can set ip-address on a Interface. This command we have to execute on linux machine 
                                                  Note: all the changes made above are valid only till a new re-start. All will be deleted.
                                                  If we want to presist the changes, then we have to put then in the Network-interface file.

> i route add 192.168.2.0/24  via  192.168.1.1

> ip route add default via  192.168.2.1

> pin <any ip>                         <-- if the ip address is reachable then it return result,

Node-A             Node-B           Node-C      
Network-A  -->   Network-B  -->   Network-C          <-- these are the networks created on every Node.

> echo 1 > /proc/sys/net/ipv4/ip_forward        <-- Execute this command on the Node-B, after doing this we can send a packet from 
                                                    Node-A to Node-c  through the Node-B, As we have done the "ip_forward" 
                                                    this command will make in the file "/etc/sysctl.conf = 1" file 

> cat /proc/sys/net/ipv4/ip_forward             <-- if it return  "0" it means no ip_forward is set for this Node or linux machine.



-------------------------------------------------------------------------------------
DNS Server:
----------

> cat >> /etc/hosts    <-- in this file add   ->   192.168.1.11  db      <-- this is file is placed on every linux machine.


Assume we have a DNS service with ip - 192.168.1.100

> /etc/resolv.conf     <-- on every Linux machine have a file "resolv.conf" in it just add the "DNS" server ip, then when we do a ping then 
                           it just check the ip in DNS server and return the result.

> cat /etc/nsswitch.conf             <-- here we can define, which DNS server, the host first look, it should search in local-file with DNS entry 
                                       or the call first go to any extern DNS server 
--------------------------------------------------------------------------------------------
Domain Names 
-------------

below is example of "www.google.com" 

.                             <-- Root 
.com, .net, .edu, .de         <-- these are Top Level Domain Name
google                        <-- This is domain name assign to google
www                           <-- Subdomain



-----------------------------------------------------------------------------------------------
Record Types  in Route 53:
-------------------------

A-Records           <-- mapping ipv4  
AAAA                <-- mapping ipv6
CNAME               <-- mapping  food.web  to  my-web.com, It is name-to-name mapping


--------------------------------------------------------------------------------
> nslookup                        <-- if we put a entry in local "/etc/resolv.conf" file, "nslookup" does not check this file
                                     It directly check and send request to DNS server 

> dig                            <-- if we put a entry in local "/etc/resolv.conf" file, "nslookup" does not check this file
                                     It directly check and send request to DNS server 

> ps aux                                        <-- when we we run this command on the pod, then it will show the process running on the pod
> ps aus                                        <-- when we run this command on the node, then it show all the process running on the whole node.


> ip netns add my-interface-a                   <-- with this we can create a new "Network-Namespace" on a Linux-host or we can say linux-machine 
> ip netns                                      <-- like this we can list all the Network-namespace available on the linux-host 

> ip link                                       <-- to like all the interfaces on the linux-host.

> ip netns exec <network-namespace-name> ip link         <-- like this we can see which Network interface that is defined inside the "Network-namespace"
OR
> ip -n <namesapce-name> link                        <--both of these commands are same 

> arp                                           <-- we can see entried in the linux-host, this show the list of other hosts that we have made connection 

> ip netns exec <namespace-name> arp             <-- this command runs now inside the given "namespace-name"

> route                                        <-- like this we can get the entries from the routing-table on the linux-host 

> ip netns exec <namespace-name> route          <-- like this we will get the routing-table entrie from the given namespace 

> ip address                                     <-- we can get the "Network-interface" and "MAC-address" that is attached to pod or Node

> ip address show <interface-name>             <-- e.g  eth0  <-- it can be a interface name, then it will give us the info about it.

> ip address show type bridge                  <-- it can show us all the "Bridge-network-interface" on the Machine, Node or Pod.

> ip route                                      <-- it will give info about all the routes that are defined.

> ip route add 192.168.1.0/24  via 192.168.2.2

> netstat -plnt 

> route 

> netstat  --help 

> ip link add <any-cable-endpoint-name> type veth peer name <any-cable-endpoint-name>        <-- like this we create two endpoints with a attachment 

> ip link add <cable-endpoint-name> netns <namespace-name>                       <-- like this we can attach one side of the cable-endpoint to the namespace 

> ip link add <cable-endpoint-name> netns <namespace-name>                       <-- like this we can attach other side of the cable-endpoint to the other namespace 

> ip -n <namespace-name> addr add 192.168.15.1 dev <cable-endpoint-name>         <-- like this we can define a ip-address to a namespace 

> ip -n <namespace-name> link set <cable-endpoint-name> up                       <-- like this we are activating the links connection 

> ip link add <any-bridge-name> type bridge                                      <-- like this we can create a network-bridge on a linux-machine 

> ip netns                                                                   <-- to get all the namesapces from a "Docker-host"

 
> ip address show type bridge                                <-- it will show all the "bridge" interface on the machine 



#####################################################################################################################
CNI    (Container network interface)
----
-) The main purpose of the CNI is when a new "Pod" is created then it should assign a "IP" address and also create connections to "Network-bridge"
   that a pod can call any other po or service.

-) kubelet.service          <-- in this file we can see the 
                                "--network-plugin=cni"            <-- which type of newtork-plugin we have defined 
                                "--cni-bin-dir=/opt/cni/bin"      <-- in this directory we see all the supported cni-plugins as executable files e.g  "bridge" "dhcp" "flannel"
                                "--cni-conf-dir=/etc/cni/net.d"   <-- in this location we can find a "configuration" file, this files is used by "kubelet" to know which plugin to be used. 

> ps -aux | grep kubelet       <-- like this we can also see all the configuration about the "CNI" defined in the "kubelet"


> cd /etc/cni/net.d/<name-of-file>               <-- here is the file that is executed by the CNI every time when a new container is created. 


> kubectl logs <wave-net-ere> -n kube-system       <-- As in this example we have "wave" as CNI installed, in the logs we can see 
                                                      a attribute "ipalloc-range:10.x.x.x/x"  <-- from here we can findout the defined 
                                                      range for the  ip-address that will be allocated to every pod. 


--------------------------------------------------------------------------------------------------------------------
Kube-proxy: 
-----------
-) When we create a new "Service" then automatically "kube-proxy" will be executed and it prove a ip address to the newely created "Service" 

> ps aux | grep kube-api-server       <--    in this process or directly in the "kube-apiserver.yaml  file we can see 
                                             a attribute "--service-cluster-ip-range=x.x.x.x/x" from here we can find that 
                                             under which range our cluster is alloweed to provide automatically a ip address 
                                             to the "Service" when it is created 

> iptables -L -t nat | grep <service-name>        <-- like this we can find the "Ip address" that is assigned to the service 

Logs:
----
> /var/log/kube-proxy.log                    <-- from here we can see all the logs that are produced by the "kube-proxy" 





##########################################################################################################
Services & kube-api-server & Kube-proxy   (Vid  223: Service-Networking)
----------------------------------------

kube-api-server --service-cluste-ip-range ipNet (Default: 10.0.0.0/24)

> ps aux | grep kube-api-server               <-- with this we get a output that from-till which range "kube-proxy" alloweed
                                                 to set ip ranges for Services.
                                        Output: kube-apiserver --authorization-mode=Node,RBAC --service-cluster-ip-range=10.96.0.0/12   

> iptables -L -t nat | grep my-db-service        <-- this commmand outputs the ip-address defined for the service and it also shows
                                                    the ip address that is for the pod that is mapped to this service 

> cat /var/log/kube-proxy.log                 <-- it will give us all the logs, how the kube-proxy assign all the ip to service and to pod 
                                                  and mapping them.


########################################################################################################
CoreDNS
-------

-) When a Service is created, then a ip-address is assigned to it, After Service is created then a entry in the CoreDNS table
   is made with the Name of the "service-map" to ip of the service, so when any pod send a request to the Name of the service then
   the request is forwarded to the ip of the service.
   

##############################################################################################################
Ingress:
---------
> k create ingress --help

###########################################################################################################
100 Questions


Q-1) Update the image of the deployment  ?

1)
> k get deploy -owide                                 <-- like this we will see all the deployment with the image-name defined on them 

2)
> k set image deployment <deploy-name> <image-name>   <-- like this we can update the image 

----------------------------------------------------------------------------------------
Q-2) Change the static pod path ?

1) 
> ps -aus | grep kubelet 

2)
--config=/var/lib/kubelet/config.yaml            <-- then find this path 

3)
vi /var/lib/kubelet/config.yaml                 <-- open this file and see the option "staticPodPath: ......" and update the path 
                                                    after that just save the file and no need to do anything else 

----------------------------------------------------------------------------------------
Q-3)  Upgrade the cluster ?
                    -kubeadm: 1.19.0
                    -kubelet: 1.19.0
                    -kubectl: 1.19.0

1) k drain <node-name>
2) k cordon <node-name>
3) apt update 
4) apt install kubeadm=1.19.0-00
5) kubeadm upgrade apply v1.19.0
6) apt install kubelet=1.19.0-00
7) apt install kubectl=1.19.0-00
8) systemctl restart kubelet 
9) kubectl uncordon <node-name> 


------------------------------------------------------------------------------------------
Q-4) Create a new deployment and scale with 2 replicas 
            - name: httpd-deploy 
            - image: http:alpine 

1) k create deployment <deploy-name> --image=http:alpine 

2) k scale deployment <deploy-name> --replicas=2

------------------------------------------------------------------------------------------
Q-5) Create a pod with labels 
     -name: mypod
     -image: redis:alpine
     -labels: tier=redis 

1) k run my-pod --image=redis:alpine -l tier=redis 

-------------------------------------------------------------------------------------
Q-6) Creat a pod in a namespace 
     -namespace: tech
     -pod name: my-pod
     -image: redis:alpin

1) k create  namespace tech 
2) k run my-pod --image=redis:alpine -n tech 

----------------------------------------------------------------------------------------
Q-7) Create a pod and expose it
      -name: mypod
      -image: redis:alphine
      -service name: my-service
      -port: 8080
      -targe-port: 8080

1) k run mypod --image=redis:alpine 
2) k expose pod mypod --name=my-service --port=8080 --target-port:8080

-----------------------------------------------------------------------------------------
Q-8)  create a deployment with 3 replca and upgrade by using "rolling-update"
      -name: my-deploy 
      -image: nginx:1.16
      -upgrage: nginx:1.17

Note: As we need to show the rollingupdate so we will create a .yaml file first then create it 


1) k create deploy my-deploy --image=nginx:1.16 --replicas=3 --dry-run=client -oyaml > deploy.yaml 

2) k set image deployment/my-deploy nginx=nginx:1.17 --record        <-- the main point it we need to use the 
                                                                      name of the "image"  "nginx=nginx:1.17"
                                                                      and also "--record" that we can rollback 
                                             Note: try how to rollback to old version and again this version 

3) k rollout history deployment my-deploy 


-------------------------------------------------------------------------------------------
Q-9) create a static pod with a command  "sleep 1000"
       -name: my-pod 
       -image: redis:alpine 

1) 
> ps -aus | grep kubelet 

2)
--config=/var/lib/kubelet/config.yaml            <-- then find this path 


3) k get nodes                            

4) my-node  ssh                           <-- we have to first go inside the node 

5) sudo -i                               <-- we have to change to the "root" user 

6) vi /var/lib/kubelet/config.yaml                 <-- open this file and see the option "staticPodPath: ......"
                                                      in this attribute the path is defined, Just copy that path 

7) logout                                <-- just logout from the node 

8) k run my-pod --image=busybox --command sleep 1000 --dry-run=client -oyaml > pod.yaml 

9) k get nodes -o wide                  <-- here we are taking the ip of the node 

10) sudo scp pod.yaml <node-ip>:/root/        <-- here we are copying the file from worker-node to controleplane node 

11) minikube ssh                              <-- we ssh to main node again 

12) sudo -i                                    <-- get permissions as "root" user 

13) cp /root/pod.yaml    /etc/kubernetes/manifests/       <-- "/etc/kubernetes/manifests/"   <-- this is the static pod path on controleplane node 

14) logout                        <-- as "kubelet" is not installed on "master" node so we have to logout every time out from master-node 


--------------------------------------------------------------------------------------------------
Q-10)  Tain a node to be unschedulabel and test it by creating a pod on it ?
        -node-name: test-node 
        -Taint  
         -key:env_type
         -value: production
         operator: NoSchedule 

  Note: just for testing i will create two pods, one with same labels and one with no labels that it can not be placed on the node 

1) k taint node <node-name> env_type=production:NoSchedule  

2) k describe node <node-name> | grep -l taint               <-- like this we can check that the above "taint" command works or not 

3) k run test-pod-a --image=redis:alpine                    <-- after creating check if this pod is running on the tainted node or not 

4) k run test-pod-b --image=redis:alpine --dry-run=client -oyaml > pod.yaml 

5) 
spec:                                               <-- like this in  pod defination file, under "spec" we can create a tolerations
  tolerations:
  - key: env_type 
    effect: NoSchedule 
    operator: Equal 
    value: porduction  

6) k apply -f pod.yaml 


----------------------------------------------------------------------------------------------------
Q-11) Create a new Service-account, Cluster-role, Cluster-rolebinding ? 
      and make it possible to list the persistent volumes, And create a pod with the service-account 
      -service account name: my-sc-account 
      -clusterrole name: pv-role
      -clusterrolebinding: pv-binding 
      -pod name: pv-pod 
      -image: redis 


1) k create serviceaccount my-sc-account 

2) k create clusterrole pv-role --resource=persistentvolumes --verb=list 

3) k create clusterrolebinding pv-binding --clusterrole=pv-role --serviceaccount=default:my-sc-account        <-- "default" we need to put this name 
                                                                                                              as we want to make it as default 

4) k run pv-pod --image=redis --dry-run=client -oyaml > pod.yaml 

5) 
spec:
  serviceAccountName: my-sc-account                    <-- like this we can add the "ServiceAccount" to the pod 

6) k create -f pod.yaml 

----------------------------------------------------------------------------------------------------
Q-12) Create a NetworkPolicy that allows  all pods in the "tech-deploy" namespace to have communication only on a single port 
      -NetworkPolicy name: tech-policy 
      -Port: 80/TCP 

1) k label namespace tech-deploy app=tech-deploy            <-- first we add a "label" to the namespace 


2) vi network.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              app: tech-deploy 
      ports:
        - protocol: TCP
          port: 80

> k apply -f network.yaml 

-------------------------------------------------------------------------------------------------------
Q-13) List all the internla IPs of all the nodes in the cluster and save it to file /doc/ip_nodes.txt 


> search in kubernetes documentation for "kubernetes cheat sheet"

> kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'   <-- this is the command we get from the  
                                                                                                    "kubernetes cheat sheet" page 

> kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /doc/ip_nodes.txt     <-- here i have changed 
                                                                                                                          "ExternalIP"  to "InternalIP"

Note: just for double check, check the internal ip of the node and then check what is written on the file .


----------------------------------------------------------------------------------------------------------
Q-14) Create a multipod with two containers. And add the command "sleep 3600" to container 2
      -name container 1: micro 
      -image: nginx 

      -name container 2: mega 
      -image: busybox 


1) k run multi-pod --image=busybox --command sleep 3600 --dry-run=client -oyaml > multi.yaml 

2) 
spec:
  containers:
  - command:
    - sleep 
    - "3600"
    image: buxybox
    name: mega
  - name: micro
    image: nginx   

3) k apply -f multi.yaml 

--------------------------------------------------------------------------------------------------
Q-15) A new colleague "Jan" has joined your team. Create a new user and grant him access to the cluster.
      He should have the permission to create, list, get, update and delete pods in the "tech" namespace. 


1) Search in kubernetes-documentation "Certificate signing request" 

2) openssl genrsa -out jan.key 2048

3) openssl req -new -key jan.key -out jan.csr -subj "/CN=jan"        <-- after executing this command it ask many yes-no   just press enter 

4) vi certificatesigningrequest.yaml 

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: jan 
spec:
  request: xxxxxxxxxxx paste the certificate here xxxxxxxxxxxx
  signerName: kubernetes.io/kube-apiserver-client
  expirationSeconds: 86400  # one day
  usages:
  - client auth

5) cat jan.csr | base64 | tr -d "\n"           <-- it will return a "Certificate", just copy that certificate and past in the above certificatesigningrequest.yaml  file 
 

6) k apply -f certificatesigningrequest.yaml   

7) k get csr                                   <-- the certificate-signing-request  should be in a pending start, we should approve it 

8) k certificate approve jan                   <-- like this we can approve the Certificate-signing-request 

9) vi role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: tech                                     <-- as it is a "role" not a cluster-role and above defined permissions for "tech" namespace 
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "create", "list", "update"]

10) vi rolebinding.yaml 

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: tech
subjects:
- kind: User
  name: jan                               # "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role                 #this must be Role or ClusterRole
  name: pod-reader                # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io

11) k apply -f role.yaml
12) k apply -f rolebinding.yaml 

13) k auth can-i delete pods -n tech --as jan             <-- here i am checking if "jan" user is alloweed to delete a pod in the "tech" namespace 


-----------------------------------------------------------------------------------------------------------
Q-16) Create a service from the "green" pod and run a DNS lookup to check the service and write to file /doc/lookup.txt 
      -service name: green-service
      -port: 80

Note: As per given, "green" pod is their, i have to create a new Service for it and after that i create a new pod and run inside this new 
       pod "nslookup <service-name>"

1) k expose pod green --name=green-service --port=80

2) k run my-pod --image=busybox:1.28 --command sleep 3600         <--here we are creating a new pod, just for running "nslookup" command as pre given in the question 

3) k exec -it my-pod -- nslookup green-service               <-- first check if the output is correct 

4) k exec -it my-pod -- nslookup green-service > /doc/lookup.txt         <-- now put the output in a .txt file  

5) cat /doc/lookup.txt                             <-- at the end always check the content of the file if everything is ok 


----------------------------------------------------------------------------------------------------------
Q-17) Create a secret and mount it to the pod "yellow"
      -secret name: yellow-secret 
      -secret content: password=kube1234


1) k create secret generic yellow-secret --from-literal=password=kube1234

2) k get pod yellow -oyaml > yellow.yaml 

3) 
apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
    - name: dotfile-test-container
      image: registry.k8s.io/busybox
      volumeMounts:
        - mountPath: "/secret"      
          name: yellow-secret-a 
  volumes:
    - name: yellow-secret-a 
      secret:
        secretName: yellow-secret

4) k delete pod yello                     <-- as we want to create a new pod with the .yaml file, so we first delete the old pod. 

5) k apply -f yellow.yaml 

6) k describe pod yellow                  <-- check it the secret is attached to the pod 

Note: just check in the real how the secret is mounted, it has created a folder for volume or how it has done 

---------------------------------------------------------------------------------------------------------------
Q-18) List all the persistent volumes sorted by capacity and write to file /doc/pervol.txt ?

Note: just for testing create 3 different pv with different capacity 

Note: first go to kubernetes documentation "cheat sheet" and search for  "sorted by"

1) k get pv --sort-by=.spec.capacity.storage            <-- check if the order is sorted or not 

2) k get pv --sort-by=.spec.capacity.storage > /doc/pervol.txt          

3) cat /doc/pervol.txt                                      <-- always check the output 

------------------------------------------------------------------------------------------------------------------
Q-19) Find the pod label  environment=process, find all the pods running high CPU workloads and write the name of which is 
      consuming the most CPU to the file /doc/cpu.txt 


1) k get pods -l environment=process           <-- first check how many pods are having a lable "environment=process" 

2) k top pod --sort-by cpu -l environment=process       <-- it return the pod 

3) k top pod --sort-by cpu -l environment=process | head -2        <-- check in the process what is the effect of  "head -2"

4) k top pod --sort-by cpu -l environment=process | head -2 > /doc/cpu.txt    

------------------------------------------------------------------------------------------------------------------
Q-20) Use JSON path to get all the node names and store them in the file /doc/nameofnodes.txt 

Note: go to kubernetes documentation "cheat sheet" search for "get nodes"

1) kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'         <-- copy this from the documentation page 

2) kubectl get nodes -o jsonpath='{.items[*].metadata.name}'      <-- modify the command and check the output 

3) kubectl get nodes -o jsonpath='{.items[*].metadata.name}' > /doc/nameofnodes.txt        


--------------------------------------------------------------------------------------------------------------------
Q-21) Show the logs from the container and save it to /doc/nginx.log 
       -pod name: direct-pod 
       -Container: nginx 
       -Namespace: dev-net 

1) k get pod direct-pod -n dev-net                   <-- first check the pod is running 

2) k logs direct-pod -c nginx -n dev-net              <-- check if the logs can be displayed 

3) k logs direct-pod -c nginx -n dev-net  /doc/nginx.log 

--------------------------------------------------------------------------------------------------------------------
Q-22) Create a new ingress resource and exposes service "hello" on path /hello by using service port 5678
       -name: ingress-connect 
       -namespace: host-dev 

1)  vi ingress.yaml                         <-- just copy from the documentation the example ingress, this is easy to solve this question 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-connect 
  namespace: host-dev
spec:
  rules:
  - http:
      paths:
      - path: /hello 
        pathType: Prefix
        backend:
          service:
            name: hello 
            port:
              number: 5678

2) k apply -f ingress.yaml 

------------------------------------------------------------------------------------------------------------------------
Q-23) Overwrite the label of the dev-nginx pod with the value "env=true"

Note: their is already a label "env=green" defined on the pod, we have to overwrite this label 


1)  k label pod/dev-nginx env=true --overwrite 


----------------------------------------------------------------------------------------------------------------------
Q-24) Upgrade the image in the deployment "green" to buxybox:1.28, check the history and roll back 

1) k set image deployment green busybox=busybox:1.28 --record

2) k get deployment green                                       <-- check the image version is updated or not 

3) k rollout undo deployment green                              <-- rollback the updation 

4) k get deployment green                                       <-- check if the old version is again their or not .


--------------------------------------------------------------------------------------------------------------------------
Q-25) Find out how many pods are available with the label env=green in the cluster and write them to the file /doc/podsavailable.txt 

1) k get pods -l env=green > /doc/podsavailable.txt 



----------------------------------------------------------------------------------------------------------------------------
Q-26) The pod "red" is failing, Find out why and fix the issue ?

1) k describe pod red                 <-- here we see "sleeeep" command is written wrong through the error logs"

2) k get pod red -oyaml  > red.yaml    <-- in the red.yaml file just update the "sleep" 

3) k apply -f red.yaml                

---------------------------------------------------------------------------------------------------------------------------
Q-27) Create a pod that will only be scheduled on a node with a specific label 
      -pod name: blue
      -image: nginx 

1) k label nodes <node-name> disk=green                               <-- to label the pod, first we have to label the node 

2) k describe nodes <node-name>                                       <-- check if the label is created 

3) k run blue --image=nginx --dry-run=client -oyaml > blue.yaml  

4) vi pod.yaml 

spec:
  nodeSelector:
    disk: green 

5) k apply/create -f pod.yaml                      


------------------------------------------------------------------------------------------------------------------------
Q-28) Create a pod which uses a persistent volume for storage
      -pod name: yellow
      -image:    busybox 
      -persistent volume name:       yellow-pv-volume 
      -persistent volume claim:      pvc-yellow 
      -persistent volume claim size: 100mi

Note their is already pv volume "yellow-pv-volume"


1) vi persistentvolumeclaim.yaml 

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-yellow 
spec:
  accessModes:
    - ReadWriteOnce 
  resources:
    requests:
      storage: 100mi

2) k apply -f persistentvolumeclaim.yaml 

3) k run yellow --image=busybox --dry-run=client -oyaml > yellow.yaml 

4) vi yellow.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: yellow 
spec:
  containers:
    - name: busybox
      image: busybox 
      volumeMounts:
      - mountPath: "/data"
        name: pv-storage 
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: pvc-yellow

5) k apply -f yellow.yaml 

--------------------------------------------------------------------------------------------------------------
Q-29) Remove the taint added to the node "my-node-a",
      Taint: key=red:NoSchedule 

> k describe node my-node-a | grep Taint                   <-- it will show the taint value 

> k taint nodes my-node-a key=red:NoSchedule-                <-- just put "-" at the end of the Taint then it will be removed

> k describe node my-node-a | grep Taint                 <-- now after removing it has no value. 

--------------------------------------------------------------------------------------------------------------
Q-30) Take a backup and restore ETCD ?


-------------------------------------------------------------------------------------------------------------
Q-31) Schedule a pod on the node "my-node-a" by using tolerations 
      -pod name: blue-pod 
      -image: nginx 

Note: Node has already a Taint   Taint: app=frontend:NoSchedule 

1) k run blue-pod --image=nginx --dry-run=client -oyaml > blue-pod.yaml 

2) vi blue-pod.yaml 

spec:
  tolerations:
  - effect: NoSchedule 
    key: app 
    operator: Equal 
    value: frontend 


3) k apply -f blue-pod.yaml 

4) k get pod blue-pod -owide            <-- with this we can see on which node this pod is running on 


----------------------------------------------------------------------------------------------------------------
Q-32) Apply autoscaling to the "green-deployment" with a minimum of 5 and maximum of 10 replicas and a target CPU of 75%

Note: their is a deployment name "green-deployment"

1) k autoscale deployment green-deployment --min=5 --max=10 --cpu-percent=75  

2) k get hpa      <-- with this we can see the values of "green-deployment"

3) k get pods                   <-- check if we are having 5 replicas of the pods 


-------------------------------------------------------------------------------------------------------------------
Q-33) Check how many nodes are in ready state and write it to the file /doc/readynodes.txt 

> k describe nodes | grep ready | wc -l             <-- it returns the number count of ready nodes 
                                                    Note: just check if this command also counts header also 


> k describe nodes | grep ready | wc -l   > /doc/readynodes.txt 

--------------------------------------------------------------------------------------------------------------------
Q-34) Create a pod and set the environment variable "dev=dev10"
       -pod name: grey 
       -image: nginx 

1) k run grey --image=nginx --restart=Never --env=dev=dev10

2) k exec -it grex -- sh -c 'echo $dev'                               <-- like this we can check that env variable is set or not in the pod 
                                                                        it should return us the value 

3) k describe pod grey              <-- like this we can also see the configured env variable 


-------------------------------------------------------------------------------------------------------------------
Q-35) Create a configmap and add it to the pod 
        -pod name: blue
        -configmap name: my-config 
        -Data: user=root,password=pass1234


1) k create configmap my-config --from-literal=user=root --from-literal=password=pass1234  

2) k describe configmap my-config                        <-- like this we can re-check the created configmap 

3) k get pod blue -oyaml > blue-pod.yaml 

4) vi blue-pod.yaml 

spec:
  containers:
  - envFrom:
    - configMapRef:
        name: my-config 

5) k delete pod blue                     <-- first delete the old pod 

6) k apply -f blue-pod.yaml               

7) k exec -it blue -- env                   <-- like this we can directly on the pod if the env variable is set or not 

-----------------------------------------------------------------------------------------------------------------
Q-36) List all the events sorted by the timestamp and write the result to file /doc/events.log 

Note: search in kubernetes-documentation "cheat sheet" after that in page search "get events"

1)  kubectl get events --sort-by=.metadata.creationTimestamp         <-- check the output 

2)  kubectl get events --sort-by=.metadata.creationTimestamp   /doc/events.log 



----------------------------------------------------------------------------------------------------------------
Q-37) Create a pod with a non-persistent volume 
      -pod name: non-per-pod 
      -image: redis 
      -mount path: /data/per-redis 


Note: As it ask for "non-persistent" so first we search in the kubernetes-documentation "emptyDir: {}" 

1) vi non-persistent-pod.yaml 

apiVersion: v1           
kind: Pod
metadata:
  name: non-pre-pod 
spec:
  containers:
  - image: redis 
    name: test-container
    volumeMounts:
    - mountPath: /data/per-redis 
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:                          <-- this example is from documentation but in video he shows  "emptyDir: {}"
      sizeLimit: 500Mi


------

apiVersion: v1           
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}

2) k apply -f non-persistent-pod.yaml 

----------------------------------------------------------------------------------------------
Q-38) The node "minikube" is in "NotReady" state, Investigate and bring the node back to ready state.


1) k get nodes                                 <-- first check the "STATUS" of the node .

2) k describe node minikube                    <-- in the "Events" below it shows also same "NodeNotReady"

3) <name-of-node> ssh                          <-- first go inside the node .

4) sudo -i                                     <-- like this we will get "root" priveleges 

5) systemctl status kubelet                    <-- check the status of the "kubelet"
                                                  If in the third or fourth line "Acticve: inactive (dead)"
                                                  like this we found thats mean the problem is with "kubelet"

6) systemctl start kubelet 

7) systemctl enable kubelet 

8) systemctl status kubelet                       <-- here it should show the "active (running)

9) logout 

10) k get nodes                                <-- now it should show the status "READY" 

-------------------------------------------------------------------------------------------------
Q-39) Make the node "my-node" unavailable and reschedule all the pods on it.


1) k get pods -owide                                    <-- check how many pods are their and on which node they are running 

2) k get nodes                                          <-- it should show two or more nodes, so that we can "cordon" one node 

3) k cordon <my-node>                                   <-- now we have make it unschduable 

4) k drain <my-node>  --ignore-daemonsets --force       <-- now we are releasing the pods from this node.

5) k get pods -owide                                    <- now we can see the pods are running on another node .

6) k get nodes                                           <-- on the node "STATUS = Ready,SchedulingDisabled" 
                                                            it means we can not schedule any pod on it 


-----------------------------------------------------------------------------------------------------
Q-40) Create a pod that echo's "goggggg" and then exists, The pod should be deleted automatically when it is completed 
     -pod name: tech-pod 
     -image: busybox

1) k run tech-pod --image=busybox -it --rm --restart=Never -- /bin/sh -c 'echo goggggg'   <-- After executing this command in the output it should 
                                                                                            print the message and also a seperate message, 
                                                                                            that pod is deleted            
                                                                                       

-----------------------------------------------------------------------------------------------------
Q-41) Annotate the existing pod "yellow" and use the value "name=yellow-pod" 

Note: we use "Annotations" when we do want the metadata to be identified, like with labels we can identify the pods 
      and rearrange it with a category.


1) k annotate pod yellow name=yellow-pod                        <-- just check in the .yaml formate if it is "Annotated" or not 


-----------------------------------------------------------------------------------------------------
Q-42) Get list of all pods in all namespaces and write it to the file /doc/podsnamespaces.txt 

1) k get pods --all-namespaces                <-- first check the output 

2) k get pods --all-namespaces   > /doc/podsnamespaces.txt            <-- after that check the file also .



----------------------------------------------------------------------------------------------------
Q-43) Update the password in the existing configmap "config-green" to "NewPass1234" 

1) k describe configmap config-green            <-- first check which password is currenty written in the configmap 

2) k get configmap config-green -oyaml > config-green.yaml    <-- here update the password in the .yaml file 

3) k replace --force -f config-green.yaml 

4) k get pods                                  <-- at the end we must check, if the pods is still running or not,
                                               if the configmap is not their then pod stops running 

5) k describe pod green                       <-- check under "Environment" that config-map is defined or not.

6) k exec -it green -- env                   <-- it should also print "password=NewPass1234"



----------------------------------------------------------------------------------------------------------
Q-44) You just created the pod "blue", but it is not scheduling on the node. Troubleshoot and fix the issue 

1) k describe pod blue               <-- check the "Events" thier it shows some issue with "nodeselector" 

2) k describe node minikube         <-- check the label defined on it  "app=blue"

3) k get pod blue -oyaml > blue-pod.yaml    

spec: 
  nodeSelector:
    app: blue                          <-- make the correction in the name 

4) k replace --force -f blue-pod.yaml 

5) k get pods                                    <-- at the end check if the pod "blue" is running 

6) k describe pod blue                            <-- also check in the ein, if nodeselector is correctly updated 

---------------------------------------------------------------------------------------------------------
Q-45) Create a network policy and allow traffic from the "green" pod to the "finance-service" and the "data-service"
      -policy name:  internal-policy 
      -policy type:  egress 
      -label pod:    role=post 
      -egress allow: finance 
      -finance port: 8080
      -egress allow: data 
      -data port:    5432



1) network-policy.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy 
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: post
  policyTypes:
    - Egress
  egress:
    - to:  
        - podSelector:
            matchLabels:
              name: frnance
    - to:  
        - podSelector:
            matchLabels:
              name: data 

------------------------------------------------------------------------------------------------------------
Q-46) Create a pod and set "SYS_TIME" + sleep 3600 seconds 
       -pod name: grey 
       -image: busybox 

1) k run grey --image=busybox --command sleep 3600 --dry-run=client -oyaml > sys-time-pod.yaml 

2) go to kuberentes documentation search for "sys_time"

3) vi sys-time-pod.yaml 

spec:
  containers:
  - name: grey 
    image: busybox 
    securityContext:
      capabilities:
        add: ["SYS_TIME"]

4) k apply -f sys-time-pod.yaml 

5) k describe pod grey                        <-- just check if commands and SYS_TIME is correctly configured or not 


------------------------------------------------------------------------------------------------------------
Q-47) Create a clusterrole and a clusterrolebinding which provides get,watch,list access to the pods 
       
       -clusterrole name:       cluster-administrator 
       -cluserrolebinding name: clusterbinding-administrator 
       -serviceaccount:         admin-sa 

1) k create clusterrole cluster-administrator --verb=get,watch,list --resource=pods 

2) k create clusterrolebinding clusterbinding-administrator --clusterrole=cluster-administrator --serviceaccount=admin-sa 

3) k auth can-i list pods --as system:serviceaccount:default:admin-sa         <-- like this we can recheck if it works 


-----------------------------------------------------------------------------------------------------------
Q-48) Get the IP address of the "blue" pod and write it to the file /doc/ip.txt 

1) k get pods -l run=blue -A -o jsonpath='{range.items[*]}{@.status.podIP}{""}{"\n"}{end}'

Note: check how he has done and how we get the output of a pod as a json 

or alternatively, just copy the ip address of the blue pod replicas and paste them to the .txt file 


---------------------------------------------------------------------------------------------------------
Q-49) Find out the version of the cluster and write it to the file  /doc/versioncluster.txt 


Note: he did not show the answer, but told that use the same above command to print the answer on the .txt file 

1) k get nodes  -l run=blue -A -o jsonpath='{range.items[*]}{@.status.podIP}{""}{"\n"}{end}'     <-- just check if this command works 



----------------------------------------------------------------------------------------------------------
Q-50) Change the mountpath of the nginx container in the "online" statefulset to  /usr/share/nginx/updated-html"


1) k get statefulset online                  <-- first check the state for the statefulset if it is in "READY" state 

2) k get statefulset online -oyaml > online-statefulset.yaml 

3) vi online-statefulset.yaml       

spec:
  containers:
  - name: nginx 
    volumeMounts:
      mountPath: /usr/share/nginx/updated-html        <-- change the "mountPath" value under "nginx" container 


4) k replace -force -f online-statefulset.yaml       

5) k get statefulset online            <-- check if it is in ready state

6) k describe statefulset online           <-- check if the mount path is updated 


-----------------------------------------------------------------------------------------------------------
Q-51)  Create a cronjob which prnts the date and "Running" every minute
       -pod name: show-date-job 
       -image:    busybox 

1) vi cronjob.yaml 

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Running               <-- "- date;"   it is a DATE command in linux and it print 
          restartPolicy: OnFailure                 "echo Running"  it prints "Running" every min 


2) k apply -f cronjob.yaml 


--------------------------------------------------------------------------------------------------------------
Q-52) User JSONPATH and get a list of all the pods with name and namespaces, and write to the file /doc/name-namespace.txt 

1) kubectl get pods -A -o jsonpath='{range.items[*]}{.metadata.name}{"\t"}{.metadata.namespace}{"\n"}{end}'    <-- this command he made by himself 

2) kubectl get pods -A -o jsonpath='{range.items[*]}{.metadata.name}{"\t"}{.metadata.namespace}{"\n"}{end}' > /doc/name-namespace.txt 



---------------------------------------------------------------------------------------------------------------
Q-53) Create a networkpolicy and allow traffic from all the pods in the "dev-tech" namespace  and from pods with 
      the label "type=review" to the pods matching the label "app=postgres" 

Note: to understand the question, In the question he define "allow traffic from" it means we have to put this policy on the 
      target port so every thing that comes to target port shoul be filtered.

1) k label namespace/dev-tech app=dev-tech              <-- first label the namespace 

       
1) vi network-policy.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: postgres 
  policyTypes:
    - Ingress 
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              app: dev-tech 
        - podSelector:
            matchLabels:
              type: review

Note: if in the question they do not define any port, then we have also not to define a port in the networkpolicy 


---------------------------------------------------------------------------------------------------------------
Q-54) Create a pod with container port  80. It should check the pod running at endpoint /healthz on port 80 + verify + delete the pod 
      -pod name: health-pod 
      -Image: nginx 

> k run health-pod --image=nginx --port=80 --restart=Always --dry-run=client -oyaml > liveness-pod.yaml 

1) vi liveness-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: health-pod 
spec:
  containers:
  - name: nginx 
    image: nginx
    livenessProbe:                           <-- From the kubernetes-documentation page i have taken this "livenessProbe" option 
      httpGet:
        path: /healthz
        port: 80
      initialDelaySeconds: 300              
  restartPolicy: Always        

3) k apply -f liveness-pod.yaml 

4) k descirbe pod health-pod               <-- check if the pod works fine 

5) k delete pod health-pod                  <-- As asked in the question at the end delete the pod .


-------------------------------------------------------------------------------------------------------------
Q-55) Monoitor the logs of the pod "yellow", extract all the log lines matching with "not found" and write to the 
      file /doc/failed.log 

1) k get logs yellow | grep "not found" 

2) k get logs yellow | grep "not found" > /doc/failed.log   



--------------------------------------------------------------------------------------------------------------7
Q-56) Rollback the deployment "blue-deplyoment" to revision 1 

1) k rollout history deployment blue-deploy                      <-- here it will show some revision 1,2 or maybe 3 

2) k rollout history deployment blue-deploy --revision=1         <-- it will rollback to the "revision 1"


------------------------------------------------------------------------------------------------------------------
Q-57)  List the orange pod with the custom columns POD_STATIS and POD_NAME and write to the file  /doc/status.txt 

1) k get pod orange -o=custom-columns="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"      <-- this command is made by him  


--------------------------------------------------------------------------------------------------------------
Q-58) For the orange pod , set the CPU memory requests and limits 
      Reqiests: CPU=20m, memory=40Mi
      Limits: CPU=160m, memory=200Mi

Note: first check it may be a case that the requests and limits are already their and we need just to update them,
        In this case we assume that these are their 

1) k get pod  orange -oyaml > orange-pod.yaml 

2) vi orange-pod.yaml                             <-- just change the resources as given in the question 

3) k replace --force -f orange-pod.yaml 


------------------------------------------------------------------------------------------------------------
Q-59) Create a pod with a non-persistent storage 
      -pod name: redis-pod 
      -Image: redis

Note: i have created this pod from kubernetes document search "emptyDir"

apiVersion: v1           
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}


------------------------------------------------------------------------------------------------------------
Q-60) Troubleshoot the failed pod "dev-blue" and make it running again 


Note: In this case the image name was wrong, So first he creates a .yaml file from pod then use "k replace -f " to create new 



--------------------------------------------------------------------------------------------------------------
Q-61) Expose the yellow-tech pod internally and create a test-pod for look-up 
      -port: 80
      -service name: yellow-tech-service 
      -test pod name: test-yellow-tech 
      -Type: ClusterIP 


1) k expose pod yellow-tech-service --name=yellow-tech-service --port=80 --target-port=80 --type=ClusterIP 

2) k run test-yellow-tech --image=busybox:1.28  --it --restart=Never -- nslookup yellow-tech-service


----------------------------------------------------------------------------------------------------------
Q-62) Create a DaemonSet 
       -name:  blue-daemon 
       -image: httpd:alpine  

> k get nodes                <-- check how many different nodes are running, at the end on ever Node one "DaemonSet" should run 

1) k create deployment blue-daemon --image=httpd:alpine --dry-run=client -oyaml > blue-daemon.yaml 

2) vi blue-daemon.yaml 

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: blue-daemon 
  labels:
    app: blue-daemon 
spec:
  selector:
    matchLabels:
      app: blue-daemon 
  template:
    metadata:
      labels:
        app: blue-daemon 
    spec:
      containers:
      - name: blue-daemon 
        image: httpd:alpine 

4) k apply -f blue-daemon.yaml 

5) k get pods  -A             <-- it will return all the DaemonSets, check if all the DaemonSets are running on different Nodes 


-------------------------------------------------------------------------------------------------------------
Q-63) There'S an issue with the node "my-node". The admin is not able to schedule any pods on the node.
      Fix the issue and deploy pod-1.yaml on the node. 


1) k get node my-node -owide             <-- As per in this question the STATUS= Ready,SchedulingDisabled
                                            As it is "SchedulingDisabled" it means we can not Schedule any pod on it 


2) k describe node my-node              <-- In the Events we can check, their should also be the same message showing 

3) k uncordone my-node                  

4) k get node my-node -owide             <-- now STATUS=Ready state 

5) k apply -f pod-1.yaml             <-- First chceck the pod-1.yaml file if the "nodeName" is correct, after that execute the "apply" command 



---------------------------------------------------------------------------------------------------------------
Q-64) Get all the objects in all the namespaces and write to file /doc/all.txt 

1) k get all --all-namespaces > /doc/all.txt 




--------------------------------------------------------------------------------------------------------------
Q-65) Create a pod and assign it to the node my-node 
      -pod name: dev-grey
      -image:    nginx 

1) first check if the node with name "my-node" exists 

2) k run dev-grey --image=nginx --dry-run=client -oyaml > dev-grey.yaml 

3) vi dev-grey.yaml 

spec:
  nodeName: my-node 

4) k replace --force -f dev-grey.yaml 


-------------------------------------------------------------------------------------------------------------
Q-66) Create all the pods with the label "env=tech-dev" and write to the file  /doc/label.txt 

1) k get pods -l env=tech-dev                           <-- first check the output of this command 

2) k get pods -l env=tech-dev  > /doc/label.txt            




-------------------------------------------------------------------------------------------------------------
Q-67) Create a taint on the node "my-node" with the key "spray" value of "red" and effect of "NoSchedule" 

Note: first check if the node "my-node" is in READY state and also their is no "SchedulingDisabled" option is written in STATUS 

1) k taint node my-node spray=red:NoSchedule 

2) k describe node my-node | grep Taints          <-- like this we can see if Taint was successfull or not 


----------------------------------------------------------------------------------------------------------
Q-68) Create a pod and set tolerations 
      -pod name: dev-green 
      -Image: nginx 
      -Tolerations: spray=red:NoSchedule 

1) k describe node  my-node | grep Taints            <-- like this we can check if the Taints on the Node matches the given Tolerations 

2) k run dev-green --image=nginx --dry-run=client -oyaml > dev-green.yaml 

3) vi dev-green.yaml 

spec:
  tolerations:
  - effect: NoSchedule 
    key: spray 
    opersator: Equal
    value: red 
  

4) k apply -f dev-green.yaml 

5) k describe pod dev-green | grep Node     <-- like this we can check on which node this pod is placed 


--------------------------------------------------------------------------------------------------------------
Q-69) Check the image version of the "grey" pod without using the describe command and write to file /doc/grey-image.txt 

1) k get pod grey -o jsonpath='{.spec.containers[*].image}{"/n"}'          <-- i am not sure ".containers[*]  <-- "*" star should be their or not 

2) k get pod grey -o jsonpath='{.spec.containers[*].image}{"/n"}'  > /doc/grey-image.txt 



----------------------------------------------------------------------------------------------------------
Q-70) Create a pod with a sidecar container for logging 
       -pod name: pod-logging
       -image: busy-box 

Note: first go to kubernetes-documentation search for  "sidecar logging", then it opens a page under "Logging Architecture"
      just copy the pod manifest file, and delete the last container from it change the name of the pod 


1) vi logging-pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: pod-logging 
spec:
  containers:
  - name: count
    image: busybox:1.28
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done      
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-1
    image: busybox:1.28
    args: [/bin/sh, -c, 'tail -n+1 -F /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}

2) k apply -f logging-pod.yaml 



--------------------------------------------------------------------------------------------------------
Q-71) Find out where the kubernetes master and KubeDNS are running at and write to file  /doc/info.txt 
      
Note: "Kubernetes master" means  master node 
      "KubeDNS"  or they can also say "CoreDNS" 

1) k cluster-info                       <-- see the output of this commands 

2) k cluster-info > /doc/info.txt 



------------------------------------------------------------------------------------------------------
Q-72) Print the pod names and start times to the file /doc/start.txt 

1) k get pods -o jsonpath='{range.items[*]}{.metadata.name}{"\t"}{.status.startTime}{"\n"}{end}'  





----------------------------------------------------------------------------------------------------
Q-73) Create a pod and run the command which shows "Welcome from tech" and sleep for 100 seconds 
      -pod name: tech-blue
      -image: busybox 


1) k run tech-blue --image=busybox  > tech-blue-pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo Welcome from tech-educator; sleep 100; done"]
    image: busybox 


--------------------------------------------------------------------------------------------------
Q-74) Create the pod "green" and specify a CPU request of "1" and CPU limit of "2"
      -image: nginx 


Note: First i go to kubernetes-documentation and search for  "pod cpu"

apiVersion: v1
kind: Pod
metadata:
  name: green 
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        cpu: "2"
      requests:
        cpu: "1"


-----------------------------------------------------------------------------------------------
Q-75) Scale the "blue" deployment to 5 replicas 

1) k scale deployment blue --replicas=5 



--------------------------------------------------------------------------------------------
Q-76) List al the secrets and configmaps in the cluster in all namespaces and write to file /doc/config-secret.txt 


1) k get configmaps,secrets --all-namespaces       <-- check th eoutput 

2) k get configmaps,secrets /doc/config-secret.txt 



--------------------------------------------------------------------------------------------
Q-77) Create a NetworkPolicy which denies all the ingress traffic 


Note: In the kubernetes documentation i searched for "default deny ingress" then in page i search for "default deny ingress"
      their we can also check "default allow all ingress", and both the same rules for "Egress" traffic 


1) vi network-policy.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress


-----------------------------------------------------------------------------------------------
Q-78) Create an init-container in a pod which creates the file "check.txt" in the "tech-dir" directory.
      Use the main container to check if the "check.txt" file exists and execute the "sleep 300" command when it exists 
      -pod name: blue-check 
      -image initcontainer: busybox:1.28 
      -image container: alpine 



apiVersion: v1
kind: Pod
metadata:
  name: green 
spec:
  volumes:
  - name: blue-volume
    emptyDir: {}
  initContainers:
  - name: init-busybox 
    image: busybox:1.28
    command: ['sh', '-c', 'mkdir /tech-dir; echo > /tech-dir/check.txt']
    volumeMounts:
    - name: blue-vol 
      mountPath: /tech-dir 
  containers:
  - name: alpine 
    image: alpine 
    command: ['sh','-c','if [ -f /tech-dir/check.txt ]; then sleep 300; fi']


------------------------------------------------------------------------------------------------------
Q-79) List the logs of the pod names "dev-green" and search for the pattern "start" and write it to the 
     file /doc/start.txt 



1) k get pods     <-- check the name of the pod, it may be it not directly "dev-green" it can be a part of deployment 

2) k logs dev-green-erere-eeer | grep start            <-- check if logs are showing 

3) k logs dev-green-erere-eeer | grep start > /doc/start.txt 



-------------------------------------------------------------------------------------------------------
Q-80) Expose the deployment  "blue-deploy" 
       -name service: blue-service 
       -port: 6379 
       -type: NodePort 

1) k get deployment blue-deploy       <-- first check if deployment is running 

2) k expose deployment blue-deploy --name blue-service --type=NodePort --port=6379 --target-port=6379

3) k describe service blue-service 


-----------------------------------------------------------------------------------------------------
Q-81) Create two pods with different labels 
      -pod 1 name: pod-1
      -image: nginx
      -label: env=green 

      -pod 2 name: pod-2
      -image: nginx
      -label: env=red 

1) k run pod-1 --image=nginx -l env=green 

2) k run pod-2 --image=nginx -l env=red 

3) k get pod -l env=green 

3) k get pod -l env=red 


-------------------------------------------------------------------------------------------------
Q-82) Create a new clusterrole named "green-clusterrole" which allows you to create deployments 
      After create a new servivceaccount named "green-sa" in the "tech" namespace.
      And finally, bind the clusterrole to the serviceaccount by creating a rolebinding named "green-rb"


1) k create clusterrole green-clusterrole --verb=create --resource=deployment 

2) k create serviceaccount green-sa --namespace=tech 

3) k create rolebinding green-rb --clusterrole=green-clusterrole --serviceaccount=default:green-sa --namesapce=tech 


-------------------------------------------------------------------------------------------------
Q-83) Find the static pod path and copy the location to /doc/staticpath.txt 

1) k get nodes                  <-- first check all the node 

2) minikube ssh                 <-- as "minikube" is node their so we need to first "ssh" in it 

3) sudo -i                      <-- change the login as "root"

4) ps -aux | grep kubelet         <-- with this we can find the location of the config file "--config=/var/lib/kubelet/config.yaml" 

5) cat /var/lib/kubelet/config.yaml | grep staticPodPath 

Output:
staticPodPath: /etc/kubernetes/manifests              <-- we copy this path with mouse right-click 

6) logout                                         <-- we logout from the "minikube" node 

7) echo /etc/kubernetes/manifests > /doc/staticpodpath.txt 



-------------------------------------------------------------------------------------------------
Q-84) Delete a pod "white-shark" without any delay 

1) k get pods                 <-- check if "white-shark" pod is running 

2) k delete pod white-shark --grace-period=0 --force 


----------------------------------------------------------------------------------------------
Q-85) Grep the current context and write it to the file /doc/current.txt 

1) k config current-context                    <-- first check the current context 

2) cat ~/.kube/config                       <-- it will print the config file 

3) cat ~/.kube/config | grep current               <-- check if it prints the current context same as the output in command 1

4) 3) cat ~/.kube/config | grep current > /doc/current.txt 



---------------------------------------------------------------------------------------------
Q-86) Get a list of all the pods which were recently deleted 
      Write the list to the file /doc/recentdelete.txt 





--------------------------------------------------------------------------------------------
Q-87) There is something wrong with the "dark-blue" pod. Troubleshoot and fix the issue 

1) k describe pod dark-blue 

-) in this question their is a type in the "Args"     "d0ne"  it should be "done"


--------------------------------------------------------------------------------------------
Q-88) Create a pod "yellow" with the image "redis:alpine" and a storage which lasts as long as the lifetime of the pod 
      
Note: in the indirect way they are asking to create a "non-persistent" storage with "emptyDir"      


1) vi non-persist.yaml
 
apiVersion: v1           
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - image: redis:alpine 
    name: redis 
    volumeMounts:
    - mountPath: /data/redis                           <-- normally we can use any path, but "/data/redis/" is used by him in video 
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}


-----------------------------------------------------------------------------------------
Q-89) Create a pod and add "runAsUser:2000" and "fsGroup:5000"
       -Pod name: sec-pod 
       -image:    nginx 


1) k run sec-pod --image=nginx --dry-run=client -oyaml > sec-pod.yaml 

2) vi sec-pod.yaml 


apiVersion: v1           
kind: Pod
metadata:
  name: sec-pod 
spec:
  securityContext:
    runAsUser: 2000
    fsGroup: 5000
  containers:
  - image: nginx 
    name: nginx 


--------------------------------------------------------------------------------------------
Q-90) There's an issue with the kubeconfig file located in the folder ~/.kube/config 
      Troubleshoot and fix the issue 

clusters:
  - cluster:
      server: https://192.168.49.2:8443                  <-- in the video here was "9000" written, change it to 8443
                                                       Note: in some case we use 6443 and in some 8443 so just check with both,      


---------------------------------------------------------------------------------------------
Q-91) Create a pod named "sec-blue" with the image "nginx" and set "NET_ADMIN"

Note: i just go to kubernetes documentation and search for  "net_admin"

apiVersion: v1
kind: Pod
metadata:
  name: sec-blue 
spec:
  containers:
  - name: nginx
    image: nginx 
    securityContext:
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]



-------------------------------------------------------------------------------------------
Q-92) Delete all the pods with the label "environment: orange" 

1) k get pod -l environment=orange 

2) k delete pod -l environment=orange 

2) k get pod -l environment=orange                     <-- check if the pods are still their 



---------------------------------------------------------------------------------------------
Q-93) Create a multipod with three containers:
      -Name container 1: container-1
      -image:            nginx 

      -Name container 2: container-2
      -image:            redis 

      -Name container 3: container-3
      -image:            alpine 



apiVersion: v1
kind: Pod
metadata:
  name: multi-pod 
spec:
  containers:
  - name: container-1
    image: nginx
  - name: container-2
    image: redis 
  - name: container-3
    image: alpine 

----------------------------------------------------------------------------------------------
Q-94) Replace the "grey" pod with the existing yaml file "pod-replace.yaml" and verify after 

Note: Their is a existing pod name "grey", and their is pod-replace.yaml  file and it has also a pod with same name "grey"

1) k get pod grey -oyaml                      <-- in this case existing pod has a image: nginx:1.17

1) k replace --force -f pod-replace.yaml 

2) k get pod grey -oyaml                      <-- after replacing it has a new image: nginx:1.18



---------------------------------------------------------------------------------------------
Q-95) Change the requested storage size of the PersistentVolumeClaim "storage-pvc" to 800Mi

1) k get pvc storage-pvc            <-- their is already a pvc with the current size of "2Gi"

2) k get pvc storage-pvc > pvc.yaml 

3) vi pvc.yaml     

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 800Mi                   <-- Here change the storage from "2Gi" to "800Mi"


4) k replace --force -f pvc.yaml           <-- like this we can replace the older with the current.

---------------------------------------------------------------------------------------------
Q-96) Edit and existing pod "green-nginx" and add the command "sleep 3600"

1) k get pod green-nginx 

2) k get pod green-nginx > green-nginx.yaml 

3) vi green-nginx.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: sec-blue 
spec:
  containers:
  - name: nginx
    image: nginx 
    command:
    - "sleep"
    - "3600" 

4) k replace --force -f green-nginx.yaml 


----------------------------------------------------------------------------------------------
Q-97) Add a readiness probe to the existing deployment "ready-deployment"
       -path: /ready
       -port: 80

Note: Readiness probe checks that, the this pod is ready to recieve traffic.

1) k get deploy ready-deployment -oyaml > ready-deployment.yaml 

2) vi ready-deployment.yaml 


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:                <-- "httpGet" <-- this example i did not saw in the kubernetes-documentation so i have to remember it 
            path: /ready
            port: 80
          successThreshold: 3  

3) k replace --force -f ready-deployment.yaml          <-- try and check this if it works 

--------------------------------------------------------------------------------------------
Q-98) Get all contexts and write it to the file /doc/contexts.txt 

Note: With "contexts" he means "contexts" in the  "~/.kube/config" file 

1) k config get-contexts           <-- see the output 

2) k config get-contexts > /doc/contexts.txt 


-----------------------------------------------------------------------------------------
Q-99) Create a replicaset "front-replicaset" with the image "nginx" which has 3 replicas 

Note: As with imperative commands we can not create directly replicaset, so in this case 
      First we will create a Deployment, After that we need to change the "kind"

1) k create deployment front-replicaset --image=nginx --dry-run=client -oyaml > replicaset.yaml 

2) vi replicaset.yaml 

apiVersion: apps/v1
kind: ReplicaSet                       <-- change "Deployment" to "ReplicaSet"
metadata:
  name: front-replicaset 
  labels:
    app: front-replicaset 
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front-replicaset
  strategy: {}                              <-- we have to remove this field "strategy"     
  template:
    metadata:
      labels:
        app: front-replicaset 
    spec:
      containers:
      - name: nginx
        image: nginx


3) k apply -f replicaset.yaml 

4) k delete pod <take-name-of-pod>           <-- just delete one pod and it should create new pod again as part of ReplicaSet 

--------------------------------------------------------------------------------------------
Q-100) List all the control plane components and write to the file /doc/controlcomp.txt 

1) k get pods -n kube-system                      <-- see the output 

2) k get pods -n kube-system > /doc/controlcomp.txt 























































































