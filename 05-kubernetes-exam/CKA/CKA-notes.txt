Questions
-----------------
pages open first 

Volumes 
env with secret 
env with configmap 
cronjob 
job 
ingress 
------------------------
Topics: Ingress, config, 
Labs: 66,70,80,102,114,11, 141  (164 till 170 ) (150- till - 163)

Labs: 141,152 

------------------------------------------------------------------
How we can enable "alpha" or " beta" versions for any Resources.
1) 
-) rabc.authorization.k8s.io    <-- enable "/v1alpha1" version 

> vi /etc/kubernetes/mainifests/kube-apiserver.yaml 
> under command:   - --runtime-config=rbac.authorization.k8s.io/v1alpha1              <-- like this we can update the config.


1-a) Api versions

> kube-apiserver.yaml      ->  command:   --runtime-config=batch/v2alpha1       <-- like this we can enable "batch/v2alpha1" version.

-------------------------------------------------------------
2) How to get the preferred version of a resource.

> k proxy 8001&        -> curl localhost:8001/apis/authorization.k8s.io       <-- "authoriazation.k8s.io  <-- like this we can find the 
                                                                                                              "preferredversion"


------------------------------
2) How to create a "tls" Secret 

> k create secret tls --key=<put-the-given-key>  --cret=<put-the-given-cret> -n my-namespace 

-----------------------------------------------------
In Docker ui - Find the base OS of the image "python:3.4" 

docker run python:3.6 cat /etc/*release*



--------------------------------------------
Pod as Root-User:

spec:
  securityContext:
    runAsUser: 0 






















Imp Points:
---------------------------------------------------------------
-)  Secrets are only "encoded" they are not "encrypted"
    Encoded:        <-- it means base64 encod-decode 
    Encrypted:      <-- it means we need public key to encrypt and private key to decrypt
    
--------------------------
Imp Points:

-) Kube-scheduler is responsible for scheduling the pod, it means it will decide on which Node the pod 
should be placed, if a "Pod" is in a pending state, then first check "kube-scheduler" pod is running or not in "kube-system" namespace 

-) Kube-Controller-Manager  is responsible for deployment, like how many replicaSet should be their, if after increasing the replica
  of a "Deployment" no new replica is created, then first check the "kube-controller-manager" pod in the "kube-system" namespace 


-) "kubectl api-resources"         <-- with this command we can get the calue name for  "--resource="  that we have to put in "Role" or "ClusterRole"

-) k get pods --kubeconfig /root/CKS/super.kubeconfig               <-- Normally by default kubernetes by default look for config file under  ".kube/config" file 
                                                                      but with the "--kubeconfig <file-path>" we can use a seperate configuration file also for testing.

> kubectl config get-contexts  > /config.txt       <-- to get all the contexts 

> kubectl config current-context                 <-- to get the current context 

> kubectl config use-context <context-name>       <-- Command to switch to a specific context 


-) k label pod my-pod env=prod --overwrite     <-- before their was a label "env=dev" and we want to change to "env=prod" then just use this "--overwrite" command

------------------------------------------------------------------------------------------
Tesing Pod:
----------

-) k run test-pod alpine/curl --rm -it  -- sh                 <-- like this we can create a test pod fastly just for executing curl commands, "-rm" it will remove the pod 
                                                                  after executin this command, or after exit.

-) k run pod --image=busybox -it --rm --restart=Never -- sh -c 'echo aaaaaa vvvv'              <-- Main point is when using "busybox" then must use "sh -c" or "/bin/sh -c"
                                                                                                "--restart-Never" we must use this command otherwise it will not work, and 
                                                                                                it struct in some never ending loop 

-) k run mypod --image=ubuntu:18.4 --command sleep 1000        <-- check the  difference between "--"  using it or not ? 

OR 

-) k run mypod --image=ubuntu:18.4 --command -- sleep 1000            

-) k run mypod --image=ubuntu:18.4 --dry-run=client --command -- sleep 1000 

-) k run tech-pod --image=busybox -it --rm --restart=Never -- /bin/sh -c 'echo goggggg'   <-- After executing this command in the output it should 
                                                                                            print the message and also a seperate message, 
                                                                                            that pod is deleted     

-) k run test-yellow-tech --image=busybox:1.28  --it --restart=Never -- nslookup yellow-tech-service


-) k create job --image=busybox:1.28 $do > aa.yaml -- sh -c "sleep 2 && echo done"        <-- like this we can create a direct job 




------------------
1) k run my-pod --image=busybox:1.28 --command sleep 3600         <--here we are creating a new pod, just for running "nslookup" command as pre given in the question 

2) k exec -it my-pod -- nslookup green-service               <-- first check if the output is correct 
-------------------

-) k run test-yellow-tech --image=busybox:1.28  --it --restart=Never -- nslookup yellow-tech-service


-) > k get events -o jsonpath='{range.items[?(@.reason=="Killing")]}{.involvedObject.name}{"\n"}{end}'         <-- get the last deleted objects

-) k get --raw "/version" | jq -f '.gitVersion'     <-- Get the version of the Cluster.

-) k cluster-info              <-- to get where is the "CoreDNS" and "control-plane" running at 


How To test a service in another namespace from a pod in another namespace:
--------------------------------------------------------------------------
Step-1) 
> k run test --image=busybox -it -- /bin/sh

Step-2)
# nc -v -z -w 2 <service-name>   <port-number>    <-- "service-name" means the service to whom we want to connect from this pod, we have to put a space in 
                                                  between pod and port-number. port-number is the port of the service. it should return 
                                                  Answer it returns:    <service-name> (ip-address) "open" or "blocked"

OR

Step-2)
# nslookup <service-name>.<namespace>.svc.cluster.local                





######################################################################################################
-) systemctl status kubelet   

-) systemctl start kubelet 

-) systemctl enable kubelet 

-) systemctl status kubelet                       <-- here it should show the "active (running)

OR

-) journalctl -u kubelet 

or

7) systemctl daemon-reload 

8) systemctl restart kubelet 

--------------------------------------------------------------------

-) systemctl restart etcd 




---------------------------------------
  containers:
  - name: busybox
    command:
    - "/bin/sh"
    - "-c"
    - "echo 'This is my best pod' && sleep 1000" 

--------------------------------------------------------
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:                <-- "httpGet" <-- this example i did not saw in the kubernetes-documentation so i have to remember it 
            path: /ready
            port: 80
          successThreshold: 3  

-----------------------------------------------------------
spec:
  containers:
  - name: nginx 
    image: nginx
    livenessProbe:                           <-- From the kubernetes-documentation page i have taken this "livenessProbe" option 
      httpGet:
        path: /healthz
        port: 80
      initialDelaySeconds: 300  


##################################################################################################

<service-name>                                            <-- like this we can call a service directy within the same namesapce 
> my-service                                     

<service-name>.<namespace-name>.svc.cluster.local         <-- like this we can call a Service from another namesapce 
> my-service.tech.svc.cluster.local 

<stateful-pod-name>.<headless-service-name>.<namespace-name>.svc.cluster.local       <-- like this we can call the Headless-pod through headless-service 
> my-sa-0.myheadless-service.tech.svc.cluster.local 

<pod-internal-ip>.<namespace>.pod 
10-50-192-2.default.pod                                    <-- like this we can call the pod directly 

<pod-internal-ip>.<namespace>.pod.cluster.local  
10-50-192-2.default.pod.cluster.local                      <-- in the ip add address use "-" insted of "."



#####################################################################################################



-----------------------------------------------------
Important-Question 1-100

Search: 13,33,36,48,49,52,55,57,69,71,72,73,76,79,83,85,86,98,99,100


pod-commands: 34,40,78

Error: 38,63
Network-policy: 12,45,77

liveness-probe: 54
Readiness-probe: 97

3,9(see-copy-method)

----------------------------------------------------
In Master-Node
---------------
Kube-api-server            6443   Port    (But somewhere i saw 8443 also)   "https://controlplane:6443/......
coredns
etcd                       2379   Ports 
kube-proxy
kube-scheduler             10259  Port 
Kube-Controller-Manager    10257  Port 
kube-flannel-ds
Kubelet                    10250  Port          ( Kubelet can also be deployed on Master-node)

In-Nodes
-------
Kubelet                    10250  Port 
Kube-proxy                                      (watch in lecture 223  "Service Networking")
Services                   30000 till 32767 Ports  ( "> ps aux | grep kube-api-server" in this process or directly in the "kube-api-server.yaml  file we can see 
                                                                                       a attribute "--service-cluster-ip-range=x.x.x.x/x" from here we can find that 
                                                                                       under which range our cluster is alloweed to provide automatically a ip address 
                                                                                       to the "Service" when it is created 
----------------------------------------------------------


> /etc/kubernetes/manifests/             <-- on the "controlplane" or "master" node we can get all the controlplane resources .yaml files 



ETCD
=====

ETCD listen on default port  "2379"


CRI:       <-- "Container-Runtime-Interface"   e.g, Docker, RKT, CRI-O 
CNI:       <-- "Container-Network-Interface" 
CSI:       <-- "Container-Storage-Interface"   e.g AWS-EBS, AWS-EFS, Azure-Disks


Commands:
---------
> k config use-context "user.name"            <-- like this we can change the config, "user.name" come form the  
                                                "kubectl config view"  -> config.yaml -> users: -> name 

> k config set-context --current --namespace=<namespace-name>   <-- like this we can set a default namesapce and every query goes to that namesapce.

> alias k=kubectl                 <-- it will set k as alias for "Kubectl" command

> export ETCDCTL_API=3
> etcdctl snapshot save 
> etcdctl endpoint health
> etcdctl get
> etcdctl put

> cat /etc/kubernetes/manifests/kube-apiserver.yaml       <-- to view "api-server" options where cluster is created with "kubeadm" tool     
> ps -aus | grep kube-apiserver 

> kubectl -n <pod-name> exec -it <sidecar-container-name> -- cat /log/app.log 

> kubectl get nodes              <-- it also return the "Kubernetes version" which is installed.

> kubectl replace --force -f /tmp/kubectl-edit-2345685343.yaml

Imp Points:
-----------
-) Kubernetes Pod Network Ip-assign   10.32.0.0/12    it means   10.32.0.1  till 10.47.255.254  



Declarative commands   <-- when we use .yaml files, it means we are declearing resources in a .yaml file
Imperative commands  <-- wehn we try to create a resource directly through command line


Difference between "kubectl apply" and "kubectl create" and "kubectl replace"
----------------------------------------------------------------------------
> "Kubectl apply" creates kubernetes objects through "Declarative commands"  <- with this the last .yaml file will be stored under "metadata.annotations"
> "kubectl create" creates kubernetes objects through "Imperative commands"  <- with this the last .yaml file will not be stored under "metadata.annotations"
> "kubectl replace" creates kubernetes objects through "Imperative commands"  <- with this the last .yaml file will not be stored under "metadata.annotations"


Difference between  "kubectl replace" and "kubectl replace --force"  and "kubectl edit" commands
------------------------------------------------------------------
> "kubectl replace -f aby.yaml" replace kubernetes objects through "Declaratice commands", we first make change manually on .yaml file
   after that with "kubectl replace -f abc.yaml" it will just change the changes that we make, it will not delete and recreate the object

> "kubectl replace --force -f abc.yaml":   Wtih "--force" we are saying please delete the old object and then create the new object with the
                                          new configuration that we made

> "Kubectl edit" edits kubernetes objects through "Imperative commands", we can directly make the changes in live manifest, 
   But changes will not be recorded.


Note: 
Secrets are only encoded
Secrets are not encrypted in ETCD db of Kubernetes
Certificates are Encrypted.

Pod-Eviction-Timeout: 
---------------------
Kubernetes will wait for 5min by-default for a Pod if it goes offline, As the whole node goes down, if after 5min the pod does not 
comes up then kubernetes declare it dead and create new pod on the other available node only if it is a part of a "ReplicaSet" if it is not
a part of any "ReplicaSet" then the pod is completely deleted.
By-default  in the "kube-controller-manager" it is set to   "--pod-eviction-timeout=5m0s" 

If the Pod with the Node comes back inbetween 5min then it will be taken as live again and no process will be done further.

=============================================================================================================
Basic commands:
---------------

--dry-run=client  <-- this will not create the resource, instead, it will tell us weather the command is correct or not

--dry-run=client -oyaml  <-- it will check if the command is correct and generate a .yaml file also

> kubectl replace --force -f abc.yaml         <-- this command will delete the existing pod and create a new with this file
                                              normally we need to write first "delete" then "apply" but this command do both
                                              of the things in one time.



> kubectl explain pods   or   replicase  or deployment
> kubectl describe pods
> kubectl create pods --help
> kubectl create secret generic --help
> kubectl explain secret 
> kubectl run --help
> k taint --help 

> kubectl replace --force -f abc.pod          <-- like this it will forcefully delete and recreate the pod

> kubectl replace --force -f /tmp/kubectl-edit-3434343.yaml     <-- like this we can forcefully delete and recreate the pod

----------------------------------------
Pods :
------
> kubectl  run  mypod --image nginx -n my-namespace         <-- it will directly create a pod in "my-namespace"
> kubectl run mypod --image redis:alpine --labels="tier=db"
> kubectl run mypod --image redis:alpine --labels env=dec,ab=bc
> kubectl run my-pod --image nginx --port=8080
> kubectl run my-pod --image httpd:alpine --port 80 --expose true    <-- it create a "pod" and also a "Service" same time
> kubectl run my-pod --image busy-box --command sleep 3200 --dry-run=client -oyaml      
> k run mypod --image=ubuntu:18.4 --command -- sleep 1000            
> k run mypod --image=ubuntu:18.4 --dry-run=client --command -- sleep 1000              
> kubectl get pods 
> kubectl get pods -owide              <-- it also shows that on which node this pod is placed
> kubectl get pod -n my-ns --show-labels         <-- it will show the pods with their labels also
> kubectl describe pods 
> kubectl get pods --all-namespaces
> kubectl get pods -A
> kubectl get pods --selector app-labele=myvlaue          <-- like this we can search for pods that has a lable "app-labele=myvlaue"
> kubectl get pods --selector env=dev | wc -l             <-- the return the number of lines in the result, their is a HEADER in the result also
                                                              this is the reason we have to do -1 from the resutle, if result is 8, then 8-1= 7 is answer
> kubectl get pods --selector env=dev --no-headers | wc -l     <-- this will return a result with a number, in the result no HEADER is included 
                                                                  so as the last obove command we can directly get 7 as result.

> kubectl get all --selector env=prod --no-headers | wc -l   <-- return all the objects that has lable "env=prod"

> kubectl get all --selector  env=prod,tier=db,app=p-app 

> kubectl get pods --watch

> kubect set image   my-deployment my-container=nginx:1.9.1     <--like this we can set a new "image" in the Deployment 

> k api-resources                           <-- get all the resources 

> k api-resources --namespaced=true         <-- it will return a list of all the resources that we can create inside a "Namespace"

> k api-resources --namespaced=false        <-- it will return a list of all the resources that we can not create inside a "Namespace"
                                              those must be cluster scope 

######################################################################################################################################
Command from practise course:
----------------------------
> kubectl get pod -A --sort-by=.metadata.creationTimestamp           <-- sort all the pods as per Descending order as per creation-time
                                                                        by-default it output in Descending order

> kubectl get pod -A --sort-by=.metadata.creationTimestamp | tac      <-- with "tac" it will reverse the flow and make it to Ascending order


########################################################################################################################
Cluster Upgrade:
----------------
Q-3)  Upgrade the cluster ?
                    -kubeadm: 1.19.0
                    -kubelet: 1.19.0
                    -kubectl: 1.19.0


-) kubeadm upgrade plan 

-) cat /etc/*release*            <-- this command shows us which operating systemm i am currently working, 

1) k drain <node-name>
2) k cordon <node-name>
3) apt update 
4) apt install kubeadm=1.19.0-00
5) kubeadm upgrade apply v1.19.0
6) apt install kubelet=1.19.0-00
7) apt install kubectl=1.19.0-00
8) systemctl restart kubelet 
9) kubectl uncordon <node-name> 



Q-15)(check) Upgrade the cluster (Master and worker Node) from  1.18.0   to   1.19.0 

1) k get nodes              <-- we have "controlplane" (master)  and node01 (worker) nodes 

2) k drain controlplane --ignore-daemonsets          <-- Drain the master node first, it will also "cordon" the node 
3) ssh controlplane
4) apt update 
5) apt install kubeadm=1.19.0-00
6) kubeadm upgrade apply v1.19.0 
7) apt install kubelet=19.19.0-00
8) systemctl restart kubelet 
9) k uncordon controlplane 
10) k get nodes                               <-- at this point we can see the "master" node upgrade is finished 
                                              In the output recheck the verion for master node 

Note: we run same commands for "worker node" again 

1) k drain node01 --ignore-daemonsets          <-- Drain the worker node now , it will also "cordon" the node 

2) ssh node01
3) apt update 
4) apt install kubeadm=1.19.0-00
5) kubeadm upgrade node 
6) apt install kubelet=19.19.0-00
7) systemctl restart kubelet 
8) k uncordon node01
9) k get nodes                               <-- at this point we can see the "worker" node upgrade is finished 
                                              In the output recheck the verion for worker  node 


#####################################################################################################################################
ReplicaSet: 
-------------
> kubectl explain replicaset
> kubectl edit rs my-set
> kubectl scale rs my-set --replicase=5  

Deployment: 
-----------

> kubectl create deployment my-dep --image nginx --replicas=3

> kubectl create deployment --image nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml 

> kubectl create deploy

> kubectl expose deployment my-dep --port 80

> kubectl scale deloyment my-dep --replicas=5

> kubectl set image deployment my-deploy nginx=nginx:1.18

> kubectl get service 
> kubectl get svc
> kubectl describe svc my-service
######################################################################################################################
Rollout:
-------

Note: Rolling-update is the default deployment strategy. if we do not define any strategy, then it select by-default "Rolling-update" 
      as a strategy.

Rolling-update Strategy:
---------------
In Rolling-update if we have 5 pods are already their, and we want to update them with new version, then while doing
deployment, it delete one pod and bring one new, in this way it goes.

Recreate Strategy:
------------------
In this case it first delete all the pods and after that it create all new pods at once, this strategy can have downtime 
because at the time of first delete all the pods are deleted.

> kubectl create -f abc.yaml  --record          <-- like this we can create a new Deployment and also recording it for the "k rollout" directly from first time.
                                          Note: As at the creation time we put the flag "--record" now on every update of this deployment it will be automatically 
                                                saved versions for "k rollout" we do not need everytime to user this "--record" flag on any updation.
                                          Note: if we do at the time of update a Deployment "--record" , if we do update after this one then we do not 
                                                need to put this "--record" flag again.     


> kubectl get deployments 

> kubectl create -f abc.yaml       <-- with this command we can create a deployment first time

> kubectl apply -f abc.yaml           <-- As a deployment is already create, with this command it will update the Deployment
                                         If their is no deployment then with this command we can also deploy it first time

> kubect set image   my-deployment my-container=nginx:1.9.1    <-- like this we can change the image of current running deployment   
                                                                 my-deploment is the name of the Deployment
                                                                 my-container is the name of the Container                                      


> k rollout status deployment/my-deploy      <-- it will log the status of the deployment

> k rollout history deployment/my-deploy     <-- it will print the history of the deployemnt

> kubectl rollout status my-deployment      <-- with this we will get the status of the deplyoment

> kubectl rollout history my-deployment   

> kubectl rollout undo my-deployment          <-- with this we can rollback the changes to last deployed deployment.
 
> k rollout undo deployment my-deployment --to-revision=3           <-- like this we can rollback to any version    

Create a new Service
-------------------
> kubectl expose pod my-redis --port 6379 --name my-serice --dry-run=client -oyaml   <-- it will create a Serive and automatically exposes the port
                                                                                   or generate lable connection with the Pod name "my-redis"
                                                                                   it automatically uses the Pod lable and add in Service
                                                                                   By-default it creates a "clusterip" service
                                                                                   with this command we can not use --tcp=80:80 --node-port=30080 

> kubectl create service clusterip my-redis --tcp=6379:6379 --dry-run=client -oyaml     <-- the main point is that we can not add any selector with this 
                                                                                      command, it automatically only generate "app=my-redis" as tag
                                                                                      we can not explicitly define any selector with "kubectl create" command
                                                                                 
> kubectl expose pod my-pod --type NodePort --port=80 --name my-service --dry-run=client -oyaml    <-- it will create a new service and as we have used
                                                                                                    "kubectl expose" it will automatecally take the 
                                                                                                    Pod tags and put in Service-Selector
                                                                                                    with this command we can not use --tcp=80:80 --node-port=30080 

> kubectl create service nodeport my-service --tcp=80:80 --node-port 30080 --dry-run=client -oyaml    <-- the main point is that we can not add any selector with this 
                                                                                                       command, it automatically only generate "app=my-redis" as tag
                                                                                                      we can not explicitly define any selector with "kubectl create" command                                                                                                                 

---------------------------------------------------------------------------------
Service:
-------
Assume we have two namespaces  Namespace "A"   and Namespace  "B"

mysql.connect("db-service")          <-- if a web-app in Namespace "A" call a service in same namespace "A" then it directly use the service-name

mysql-connect("db-service.dev.svc.cluster.local")    <-- if a web-app in Namespace "A" call a service in different Namespace "B" this it should use 
                                                        this way, When we create a service a DNS name is created with this whole name "db-service.dev.svc.cluster.local"
                                                        in the Kubernetes cluster, As Service is a cluster specific object so when we call this DNS it will route 
                                                        request to this service

db-service         <-- name of the Service
dec                <-- name of the Namespace
svc                <-- It is a subdomain added in DNS, as we are calling a Service
cluster.local      <-- It is a subdomain for the cluster,As both of the Namespaces are in same cluster
--------------------------------------------------------------------------------

Labels and Selectors:
---------------------
> k get pods --selector app=App1

> k get pods --selector env=App | wc -l 
> k get pods --selector env=App --no-headers | wc -l 

> k label pod mypod env=dev,app=dev
> k label node node01 env=dev



> kubectl config set-context $(kubectl config current-context) --namespace=dev          <-- like this we can set the config to "dev" namesapce
                                                                                      it means when we do "k get pods" it will fetch pods details from "dev" namespace


ResourceQuota for Namespace
----------------------------
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec: 
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi  


#####################################################################################################################
Readiness & Liveness 
--------------------


Readiness-Probe:     
----------------
At the time of starting a pod, kubelet check under this "readinessProbe" weather the pod is ready or not.
Once the pod is ready after that kubelet will not call this endpoint anymore.

Liveness-probe:
--------------





Their are three type of "readinessProbe"
1) httpGet:

2) tcpoSocket:

3) exec:
    command                 <-- with this we can define our own command 


-) httpget: 

apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: registry.k8s.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      httpGet:
        path: /api/any/ready            <--if we have any endpoint in the application, then we can  automatically a request to that endpoint,
        port: 8080                         that path here and at the time of starting the containter kubelet send it if return 200 then it is ready 
      initialDelaySeconds: 15           <-- assume i have a JAVA service and it takes 10-15sec to start then we can put that starting time here                                                             
      periodSeconds: 10                 <-- Default value is 10-sec:  As kubelet make a "readiness" check that this pod is ready or not, here we can define that time 
                                            after how many seconds kubelet should make a call to check the status of the pod. Deafult value is 10sec 
      failureThreashold: 4              <-- Default value is 3   if some how after 15 second as above defined the service is not ready then, kubernetes will try 3 
                                            times to check if the service is ready or not, if after that it is not ready then it will mark pod as faulty. 



-) tcpSocket:

apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: registry.k8s.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:                                           <-- if we have a TCP-Socket connection then use this 
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10


-) exec: 
     command: 

apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: registry.k8s.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:                                      
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10








#######################################################################################################################
Jobs:
-----


apiVersion: batch/v1
kind: Job
metadata:
  name: my-job
spec:
  completions: 3                             <-- it will create 3 replicas of this pod    
  template:
    spec:
      containers:
      - name: my-job
        image: ubuntu
        command: ['expr','3','+','2']
      restartPolicy: Never   


apiVersion: batch/v1
kind: Job
metadata:
  name: my-job
spec:
  completions: 3            
  parallelism: 3                       <-- it will create 3 pods that run parallaly, it means one job is done by three pods    
  template:
    spec:
      containers:
      - name: my-job
        image: ubuntu
        command: ['expr','3','+','2']
      restartPolicy: Never  

apiVersion: batch/v1
kind: Job
metadata:
  name: my-job
spec:
  completions: 3                             <-- it will create 3 replicas of this pod    
  template:
    spec:
      containers:
      - name: my-job
        image: kodekloud/throw-dice
      backoffLimit: 25                         <-- it will run 25 times this "Job" as a job starts and if it does not complete then it tries for "25" times
      completions: 3                          <-- their is some logic written in the image, that the result should be "6" then only the task is completed 
                                               "completions" means that task should be completed, if the result be 4 or 5 then task is not completed 
                                                like this we can apply a completions


##########################################################################################
CronJob:
--------


kubectl create cronjob my-cronjob --image=your-image:latest --schedule="*/1 * * * *" --restart=OnFailure --dry-run=client -o yaml > cronjob.yaml


apiVersion: batch/v1
kind: CronJob
metadata:
  name: my-job
spec:                                   <-- this "spec" section is for "Cronjob" itself 
  schedule: "* * * * *"
  successfulJobsHistoryLimit: 2             <-- This field means keep the history of "2" Successful Jobs 
  failedJobsHistoryLimit: 4                 <-- This field means keep the history of "4" failed jobs.
  jobTemplate:
    spec:                               <-- As "CronJob" run "Job" internally, so this "spec" if for "Job"
      completions: 3            
      parallelism: 3
      activeDeadlineSeconds: 10           <--This time is set for the "Job" no matters how many pods are running this job, once it reach "10" sec and 
      template:                              if the job does not finish then it will terminate all the pods.
        spec:                            <-- this "spec" section is for "container" itself
          containers:
          - name: my-job
            image: ubuntu
            command: ['expr','3','+','2']
          restartPolicy: Never
          terminationGracePeriodSeconds: 8        <-- it will terminate the pod after "8" seconds


###################################################################################################################
Admission Controllers:
-----------------------

> kube-apiserver -h | grep  enable-admission-plugins        <-- it will show the list of all the "enabled Admission Controllers"

> kubectl exec kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep  enable-admission-plugins       <-- if cluster is deployed with "kubeadm" 
                                                                                                                      then use this command. 

> ps -ef | grep kube-apiserver | grep admission-plugins          <-- to get the list of all the "enabled" and "disabled"  Admission-controllers 

kube-apiserver.yaml         
------------------
spec:
  containers:
  - command:
    - --enable-admission-plugins=<Any-Admission-controller-name>            <-- to enable any new "Admission controller"
    - ---disable-admission-plugins=<Any-Admission-controller-name>       <-- to disable any existing "Admission controller"


    


######################################################################################################################
Taints:
-------
> kubectl describe node kubemaster | grep Taint         <-- this will show us the which "Taint" is applied on the Master-node

> kubectl taint node <any-node-name> key=value:Taint-Type

> kubectl taint node mynode  mykey22=myValue22:NoSchedule


To remove the Taint from a node
-----------------------------
> kubectl taint node mynode  node-role.kubernetes.io/master:NoSchedule-     <-- first go check the whole value of the Taint from the "describe node" command
                                                                            then put that value as above and at the end put "-" minus sign

Note: 
key       <-- it can be a string upto 253 characters
value     <-- it can be a string upto 63 characters


q) How to Taint a Node and define same Taint for the Pod ? 

Taint    
------

their are 3 tyes of "Taint" values we can use in "effect" key
1) NoSchedule                         <-- new pods that do not match the taint are not scheduled onto this node, But existing pods on the node remain running.
2) PreferNoSchedule                   <-- new pods that do not match the taint might be scheduled onto the node,
                                          but the existing pods on the node remain running.
3) NoExecute                          <-- in this case only same "Tainted" pods will be placed on this Node, and also if any old another "Tainted" pods
                                          are their then they will be removed from this Node

Note: we can use two types of"opertaor" 
1) Equal            <-- it means all   key,value,effect  all must be matched. (Imp:  this is a default parameter)
2) Exist            <-- it means  key,effect parameters must match, you must leave a blank value parameter, which matches any


> kubectl taint nodes node1 anyKeyName=blue:NoSchedule             <-- here we are "Tainting" a node, with  "key=app"   and "value= blue : NoSchedule" 
                                                            Note: this command we use to "Taint" a node

--------------------------------------------------
Defining a pod with the same "Taint" as we define above for the "Node"

apiVersion: v1                                                      
kind: Pod
metadata: 
  name: myPod-color
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  tolerations:
  - key: "mykey22"                    <-- this "anyKeyName" is the key defined above in the node command, we can put any name
    operator: "Equal"                 <-- as in the above Node command we use  "="   so here we have to put "Equal"
    value: "myvalue22"                <-- as in the above Node command we use Value = "blue" so here we are using blue
    effect: "NoSchedule"              <-- their are 3 types of values we can use as above defined, so here we are using one of them           
             
 Note: all the 4 values must be put inside the double quotes            

> kubectl describe node my-node 

> kubectl label node my-node my-lable=my-value

apiVersion: v1                                                      
kind: Pod
metadata: 
  name: myPod-color
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  nodeSelector:
    my-lable: my-value  

#######################################################################################################################
-----------------------------------------------------------------------------------------------------------
Node Affinity  
------------

q) Assume we have two Nodes, one with larger capacity means 5-cpu and 20GB ram and one Node has 2-cpu and 10-gb ram,
   how we can lable these Nodes and how we can Tell a Pod to go to a Larger Node

> kubectl label nodes node-1 size22=Large22                    <-- we can  put a Lable for the Node  "size=Large"  and if we put same lable in the 
                                                             Pod defination .yaml file, it means we are saying that that Pod should go to this
                                                             Node only   

options
1) requiredDuringSchedulingIgnoredDuringExecution      ( Must take this )
2) preferredDuringSchedulingIgnoredDuringExecution     ( it is just a advice )


Note: 
1) "requiredDuringSchedulingIgnoredDuringExecution" in this case  as per the condition if no "node" is found with matched label defined
   in the pod.yaml file then kubernetes will not create any pod and it will go in "Pending" stage, this "Pending" status shows us when we 
   do "kubectl get pods"
2) "preferredDuringSchedulingIgnoredDuringExecution" in this case as per the condition if no "node" is found with the matched label
   kubernetes will not stop the pod , it will just go to another node and create the pod 



apiVersion: v1
kind: Pod
metadata: 
  name: myapp-pod
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size22
            operator: In                        <-- using  "In"  operator
            values:
            - Large22                            <-- like this way we can define that this Pod can be placed on a Node which has a Lable
            - Medium22                               of   "size22=Large22"   or "size22=Medium22" 




"NotIn" example
---------------
apiVersion: v1
kind: Pod
metadata: 
  name: myapp-pod
spec: 
  containers: 
  - name: nginx-container
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution
        nodeSelectorTerms:
        - matchExpressions:
          - key: size22
            operator: NotIn                          <-- using "NotIn"   <-- it means this Pod can go to any Node which do not have a Lable "size22=Small22"            
            values:
            - Small22

OR 
---
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution
        nodeSelectorTerms:
        - matchExpressions:
          - key: size22
            operator: Exists      <-- it checks only if the "key" with the name "size22" exists in the Node, then it directly place the pod in that Node


---------------------------------------------------------------
apiVersion: v1        
kind: Pod             
metadata:                 
  name: myapp 
  labels:              
    app: mypod22
    env: Prod-2                    
spec:             
  containers:                     
    - name: myapp-frontend             
      image: mydockerimage/1.0.0
      resources:
        requests:                         <--  "request" it means this pod must have minimum memory  
          memory: "64Mi"
          cpu: "0.5"          
        limits:                           <-- "limits"  it means this pods can take only memory upto  "4GB"
          memory: "4Gi"                      no more memory will be allocated to it.
          cpu: "1"    

Note: As laptops have 8-cores so it means 8 CPUs              

e.g
----
limits:
  cpu: 1             <- it means "1 CPU"  or "1 core" 

----
limits:
  cpu: 500m   or  100m  or 0.1          <- these all  means half CPU


e.g
---
limits:
  memory: 1Gi             <- it means the memory of "1 GB"  or "1024 MegaByte"   or "1024 Mi"


---
limits:
  memory: 512Mi             <- it means the memory of "512 MegaByte"    so we can say half of 1 GB

---------------------------------------------------------------------
ResourceQuota
-------------
apiVersion: v1
kind: ResourceQuota
metadata: 
  name: compute-quota                     
  namespace: dev                   
spec: 
  hard: 
    pods: "10" 
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi  


Note: we can set "ResourceQuota" at Namespace-level, every Namespace can have its own "Resource-Quota"

########################################################################################################################
-------------------------------------------------------------------------------
DaemonSet
--------

apiVersion: apps/v1   
kind: DaemonSet
metadata:
  name: monitoring-daemon 
spec:
  selector:
    matchLabels: 
      app: monitoring-agent-a            
  template:    
    metadata:                     
      labels:              
        app: monitoring-agent-a         
    spec:             
      containers:                  
        - name: monitoring-agent
          image: mydockerimage/2.0.0   

> kubectl get daemonsets

> kubectl describe daemonsets my-daemonset



How to create the path for the "Static-pod" in a Node ?

Kubelet.service file
---------------------

Way-1) 
------
ExecStart=/usr/local/bin/kubelet  \\
  --pod-manifest-path=/etc/Kubernetes/manifests \\           <-- here we are defining path for the Static-pod that is created in the Node


Way-2) (when we create cluster with "cubeadmin tool" then it usese the "way-2")
------
ExecStart=/usr/local/bin/kubelet  \\
  --config=kubeconfig.yaml \\                   <-- we can define the above path in a "kubeconfig.yaml" file and put the file name here 

Note: in the "kubeconfig.yaml" file we can find this path "staticPodPath: /etc/kubernetes/manifests"

######################################################################################################################
Static Pod
-------------
> cat var/lib/kubelet/                  <-- All 4 files i can find under it.

> cat var/lib/kubelet/config.yaml             <-- in this file "staticPodPath:  ...."  <-- see this key hier the path is defined
                                                  It is the kubelet on the Node who is creating the pod 
> cat var/lib/kubelet/kube-apiserver.yaml                                                  

> cat var/lib/kubelet/kube-controller-manager.yaml

> cat var/lib/kubelet/kube-scheduler.yaml 

> cat /etc/kubernetes/manifests/           <-- under this folder we can place all the "Static-pod"  yaml files.



#########################################################################################################################
Multi Scheduler:
================

> kubectl get events -o wide          <-- under it we can see if the new Pod is running with a custom-scheduler or not

> kubectl logs my-scheduler -n kube-system         <-- we can view logs of out scheduler

> k get events -o wide                    <-- like this we can see all the events, e.g  when a pod is created then 
                                          which "scheduler" has placed this pod in the node, we can se scheduler name in event

> k logs my-scheduler -n kube-system            <-- like this we can also see the logs of the "Scheduler" 


kube-scheduler.service    (Default Scheduler)
----------------------
ExecStart=/usr/local/bin/kube-scheduler  \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml


my-scheduler.service
--------------------
ExecStart=/usr/local/bin/kube-scheduler  \\
  --config=/etc/kubernetes/config/my-scheduler.yaml


----------------------------------------------------
How to define a Custom Scheduler ?

Step-1)
--------
my-scheduler.yaml
--------
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: true
  resourceNamespace: kube-system
  resourceName: lock-object-my-scheduler  


Step-2) 
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
  namespace: kube-system
spec:
  containers: 
    - command:
      - kube-scheduler
      - --address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --config=/etc/kubernetes/my-scheduler.yaml
      image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
      name: kube-scheduler    

Step-3)
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
  namespace: kube-system
spec:
  containers:
    - image: nginx
      name: nginx
  schedulerName: my-scheduler    


--------------------------------------------------------------------------------------------
PriorityClass:
-------------

How to create a PriorityClass ?

Step-1)  
Create a priorityClass file

abc.file
--------
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: my-high-priority
value: 100000
globalDefault: false


Step-2)

pod.yaml
-------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  priorityClassName: my-high-priority
  containers:
    - name: nginx
      image: nginx
      resources: 
        request: 
          memory: "1Gi"
          cpu: 10

########################################################################################################################
Monitoring
----------

> kubectl top node            <-- to get the metrics of all the Nodes
> kubectl top pod             <-- to get the metrics of all the pods

> kubectl logs -f my-pod           <-- it will return the logs of my-pod


> kubectl logs -f my-pod container-1       <-- if we have two container running in a pod, e.g one service and other sidecar contianer
                                               then we have to give the name in the command
> kubectl logs -f my-pod sidecar-container                               





###############################################################################
Docker Commands:
---------------

Note:  ENTRYPOINT ["sleep"]   or  CMD ["sleep"]     <-- in the both commands the first argument must be a executable
                                                        it means we must put ["sleep"]   if we put ["10"]  then we will get error
                                                        but if we use ENTRYPOINT and CMD then in the second place CMD can have 
                                                        CMD ["10"]  as argument.

Case-1) 
-------

dockerfile-a.yaml
-----------------
FROM Ubuntu
CMD sleep 5

> docker run dockerfile-a                     <-- in this case the Ubuntu pod execute for 5sec after that it destroys



Case-2) 
-------

dockerfile-a.yaml
-----------------
FROM Ubuntu
CMD sleep 5

> docker run dockerfile-a    sleep 10           <-- As we have used "CMD", then in this case which argument we pass while doing
                                                 "docker run dockerfile-a"  the whole argument will be replaced
                                                 "CMD sleep 5"   will be replaced to "CMD 10"   at the run time
                                                 if we just pass  "docker run dockerfile-a 10"  then we will get error 
                                                 as with the CMD the whole argumes will be replaced


Case-3) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         

> docker run dockerfile-a 10           <-- as we use "ENTRYPOINT" then any argument what we put in the command will be added at 
                                         the execution time. at the end as we pass "10" it will be "ENTRYPOINT sleep 10"


Case-4) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         

> docker run dockerfile-a         <-- in this case we forget to write the argument in the command so in this case 
                                 it throws error as at the end it rund "ENTRYPOINT sleep" only



Case-5) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         
CMD["10"]

> docker run dockerfile-a            <--in this case as we forget to put an argument then by-default it take 10 from CMD






Case-6) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         
CMD["10"]

> docker run dockerfile-a   20              <-- AS which argument we put at the run time it totally override the "CMD"
                                               so  "20" will override with CMD["20"] 

Case-7) 

dockerfile-a.yaml
-----------------
FROM Ubuntu
ENTRYPOINT ["sleep"]                         
CMD["10"]

> docker run --entrypoint sleepAA dockerfile-a 20           <-- like this we can change the "ENTRYPOINT" argument from
                                                            "ENTRYPOINT ["sleep"]"   to "ENTRPOINT ["sleepA"]"
                                                            At the runtime it become  "sleepA 20"
                                                            Note: Their is not "sleepA" command, it is just for example i am showing


Case-8)

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command: ["sleep"]             <-- as in the above example "command" is same as "ENTRYPOINT"
      args: ["10"]                   <-- as in the above example "args" is same as "CMD"
 



Case-9)

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command: ["sleep" ,"10"] 


or


pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command:
      - "sleep"
      - "10"

or 

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-scheduler
spec:
  containers:
    - name: my-ubuntu-sleeper
      image: ubuntu-sleep
      command: ["sleep"]                    <-- command is same as "Entrypoint"
      args: ["10"]


> kubectl run my-pod --image-nginx --command -- sleepA     <-- like this we can override the "command: ["sleepA"]" argumen, 
                                                              as we do not define anything else so it take "10" from "args"

> kubectl run my-pod --image-nginx --command -- sleepA  2000   <-- like this we can override the "command: ["sleepA","2000]"  

> kubectl run my-pod --image=nginx -- 15                   <-- like this we can override the "args: ["15"]" argument
or
> kubectl apply -f pod.yaml -- 15                       <-- this is just my try example, but not sure if it works

> kubectl run my-pod --image=nginx -- sleep 15         <-- like this we can override the "args: ["sleep","15"]" argument


###################################################################################################################
Environment Variables  with ConfigMap and secret
-------------------------------------

Case-1) 
Environment varibale directly in the Pod

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
      env:                                      <-- like this we can define an "Environment variable", it will be then placed in the container 
        - name: my-key-a                            so the service can call directly from code the Environment variable
          value: my-val-a   
        - name: my-key-b
          value: my-val-b   

--------------------------------------------------------------------------------
Case-2) 
ConfigMap  & Secret
-------------------

Environemnt variable adding in pod through configMap

> kubectl get configmap
> kubectl describe configmap

> kubectl create configmap my-configmap --from-literal=my_key_AA=my_value_AA --from-literal=my_key_BB=my_value_BB 

> kubectl create configmap my-configmap --from-env-file=abc.properties                   <-- write the key-value in a file and put the file name here

> kubectl get secret 
> kubectl create secret my-secret --from-literal=my_key_AA=my_value_AA --from-literal=my_key_BB=my_value_BB 

> kubectl create secret my-secret --from-env-file=abc.properties                    <-- write the key-value in a file and put the file name here


> echo -n "my-pass | base64
> echo -n "cm9cdA==" | base64 --decode


Step-1)
configMap.yaml
--------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap-A
data:
  APP_COLOR: blue
  APP_MODE: prod

Step-2)
secret.yaml
--------
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret
data:
  my-username: dxNlcm5hbWU=
  my-password: cGFzc3dvcmQ=

Step-3)

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080     
      envFrom:
        - configMapRef:
            name: my-configmap-A                   <-- here we are putting the above define configmap
      envFrom:
        - secretRef:                               <--here we are refering the whole Secret to the pod 
            name: my-secret                           it means all the key-value will be inserted in as evn-variable

--------------------------------------------------------------------------
Case-3) 
Environment variable in ConfigMap or Secret in a Volume for Pod 
---------------------------------------------


Step-1)
configMap.yaml
--------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap-A
data:
  db-host: my-db-pass


Step-2)
secret.yaml
--------
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret
data:
  my-username: dxNlcm5hbWU=
  my-password: cGFzc3dvcmQ=


Step-3)
pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080     
      env:
        - name: MY_DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: MY_USERNAME                  <-- here we are inserting only this key from the secret, as env variable 
        - name: MY_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: my-password
        - name: MY_CONFIGMAP
          valueFrom:
            configMapKeyRef:
              name: my-configmap-A
              key: db-host

---------------------------------------------------------------------------------------------------
ConfigMap and Secret as a Volume
-----------------------------

Step-1) 
configMap.yaml
--------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap-B
data:
  mydata.conf: |               <-- this is a type of a file
    data_ aaaa
    data_ bbbb
    data_ cccc


Step-2)
secret.yaml
--------
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret-B
type: Opaque
data:
  secret.file |                                          <-- like this it is used as a file
    dfdjdkfjekERDF435454DFEjkgjkgRDLOIGME=    


Step-3)
pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
      volumeMounts:
        - name: my-confg-map-volume-DD 
          mountPath: /tmp/configmap/new-folder-in-container         <--with this we are creating a new container in the pod and the file in the configMap 
                                                                       "mydata.conf" will be placed in this newely created folder
        - name: my-secret-volume-DD
          mountPath: /tmp/secret/new-folder-in-container
  volumes:
    - name: my-confg-map-volume-DD                  <--- here we can give any name, it will just show in the container
      configMap:
        name: my-configmap-B                     <-- here we need to correct name for the mapping the above create config-map
    - name: my-secret-volume-DD
        secret:
          secretName: my-secret-B



-------------------------------------
Create a configmap from file:
-------------------------

abc.txt 
------
user=my-user
pass=1234


> k create cm  test-cm --from-env-file=abc.txt          <-- we must use "--from-env-file"

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080     
      env:
        - name: MY_DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: test-cm
              key: user 



##########################################################################################################
Multicontainer pods,  Sidecar
-----------------------------
> kubectl logs <pod-name> -c <container-name>       <--like this we can give the continer-name and see the logs


pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
    - name: my-sidecar-pod
      image: nginx    


----------------------------------------------------------------------
Attach only a "Volume" to containers in a Pod
---------------------------------------------

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
      volumeMounts:
        - mountPath: /var/log/event-simulator/
          name: my-ubunut-volume
    - name: my-sidecar-pod
      image: nginx   
      volumeMounts:
        - mountPath: /log
          name: my-sidecar-volume

####################################################################################################
InitContainers:
-------------

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - name: my-ubuntu
      image: ubuntu
      ports:
        - containerPort: 8080
  initContainers:
    - name: my-init-container
      image: busybox
    - name: second-init-container
      image: nginx  

> k logs <pod-name> -c <init-contaier-name>               <-- like this we can see the logs coming from "InitContainer"      


######################################################################################################
Node:
----

-> ssh <node-name>    <-- like this we can switch to or we can say go inside the "node" that the name is defined 
-> exit               <-- to come out from the node back. 

Note: 
-) when a node goes offline the master-node wait for 5min before considering it dead. it is defined in the
   "kube-controller-manager"  --pod-eviction-timeout=5m0s", After this when a new Node come in place of old one 
  then it comes blank with no pods scheduled on it. 




> kubectl drain my-node-1          <-- if we want to delete all the pods on the node, after this command no pod will be available on the node
                                       and the "kube-scheduler" will not place new node on this node, Internally it do "kubectl cordon" that 
                                       so that no new node will be placed on the Node

> kubectl drain my-node-1 --ignore-daemonsets

> kubectl drain my-node-01 --force                                        

> kubectl uncordon my-node-1      <-- Only after doing "uncordon" the Node come online again and the "kube-scheduler" will start placing 
                                      Pods on this Node again.

> kubectl cordon my-node-1        <-- It marks the Node as unscheudable, After this command no new Pods will be place on this node,
                                      But the old Pods will still remain running on this Node, Only after "kubectl uncordon" the 
                                      Kube-scheduler is alloweed to place the pods again on this Node.



#############################################################################################################
Cluster Upgrade
----------------
if i am haveing kubernetes version v1.13.4

kube-apiserver      v1.13.4
Controller-manager  v1.13.4
Kube-scheduler      v1.13.4
Kubelet             v1.13.4
kube-proxy          v1.13.4 
kubectl             v1.13.4

ETCD Cluster        v3.2.18 
CoreDNS             v1.1.3         <-- these both of them have their own release versions, they are not same as kubernetes-version


Imp-points:
----------
-) As "Kube-apiserver" is the main component and all the other components are taking through "kube-apiserver" so in this case 
  all other component must be lower or equal to the version of "kube-apiserver".

-) If "kube-apiserver" has "v1.13.4" then  "Controller-manager" and "kube-scheduler" can be either has same version or can be one version lower then "kube-apiserver" version
   It means they can be on "v1.12"  version but not below the -1 version 

-) If "kube-apiserver" has "v1.13.4" then "kubelet" and "kube-proxy" can be either has same version or can be two version lower then "kube-apiserver" version   
   It means then can be on "v1.12" or "v1.11"

-) If "kube-apiserver" has "v1.13.4" then "kubectl" (Kubectl that we have installed on our computers), "Kubectl" can be one version higher or one version lower then
   the "kube-apiserver" version. it means "kubectl" can be "v1.12" or "v1.14"

Note:
-) Upgrading Cluster is always first we Upgrade "Master-node" after we upgrade "Worker-nodes"

-) If we upgrade the cluster with "kubeadm" tool then we should upgrade "kubelet" manually on every Node extra.
-) We must upgrade the kubeadm-tool itself before upgrading the cluster













> kubeadm upgrade plan

> kubeadm upgrade apply

Upgrading process with kubeadm tool

Step-1)  
> apt-get upgrade -y kubeadm=1.12.0-00            <-- First upgrade the "kubeadm" itself

Step.2) 
> kubeadm upgrade apply v1.12.0                   <-- After that upgrade the cluster

Step-3) 
Upgrade "Kubelet" on the Master-Node, Note: we do not need to upgrade the "Kubeadm" on the Master-node

> apt-get upgrade -y kubelet=1.12.0-00            <-- After upgrading the Control-plane on Master node, we have to upgrade the "Kubelet" first on the "Master" only
> systemctl restart kubelet                       <-- After upgrading the "Kubelet on the node, we have to restart the "Kubelet" then only the upgrade effects

Step-4) 
Upgrade "Kubeadm" and "Kubelet" on Worker Node

> kubectl drain my-node-1
> apt-get upgrade -y kubeadm=1.12.0-00
> apt-get upgrade -y kubelet=1.12.0-00
> kubeadm upgrade node config --kubelet-version v1.12.0
> systemctl restart kubelet 

> kubectl uncorden my-node-1

########################################################################################################################################
ETDC Backup
-----------

> ps -ef | grep -i etcd   <-- we can also get all the endpoint info from a "process", like this we are getting info from a "etcd" process 
                                      

> k get pods -n kube-system   
> k describe pod etcd-controlplane   <- under etcd.Image  <-- here in the image-name we can find the deployed version of "ETCD"



> export ETCDCTL_API=3    <-- like this we can set the ETCDCTL version to "3", normally we need this "ETCDCTL_API=3 command before 
                              every "etcdctl" command, so just "export" it then we do not need to put it in front of every command.
> etcdctl version          

-) ETDC cluster is placed on the master-node

-) "etcd.service"   <-- under this file we can find "--data-dir=/var/lib/etcd" <-- here by-default kubernetes store all the data for "ETCD"

Steps for Backup using snapshot:
---------------------
> etcdctl snapshot save snapshot.db 
> etcdctl snapshot status snapshot.db 

Steps for Restore ETCD db from Backup 
------------------------------------
> service kube-apiserver stop                                                     <-- first stop the "kube-apiserver"
> etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup       <-- here we put the path where the backup is stored

> change the new data directry path in teh "etcd.service" path. directory name specified in the above command "etcd-from-backup"

> systemctl daemon-reload
> service etcd restart 
> service kube-apiserver start 

Imp: 
-) With all the "etcdctl" we shoud add these 4 commands also 
--endpoints=https://127.0.0.1:2379
--cacert=/etc/etcd/ca.crt 
--cert=/etc/etcd/etcd-server-crt 
--key=/etc/etcd/etcd-server.key 







##########################################################################################################################################
Certificates
-------------

Public key extensions:
------------------------
*.pem 
*.crt             <--  Those end with these are "Public Key" or "Public Certificates"


Private key extensions:
----------------------
*.key
*-key.pem            <-- Those have in the name ".key" or "-key" are "Private Key"


> cat aaa.csr | base64             <-- it will encode the Certificate but print the certificate in multiple-lines 
> cat aaa.csr | bswe64 -w 0        <-- it will endcode the certificate and print the certificate as a Single-line




Note: Their are three types of Certificates:

Root certificates:     <-- These are configured on the "CA" (Certificate Authority)
Server Certificates:   <-- These are configured on the Servers 
Client Certificates:   <-- These are configured on the Clints  


---------------------------------------------------------------------
"Servers Components" in Kubernetes Cluster
-------------------------------------------
These below are three server and should have Server-Certificates

1) Kube-API Server 
2) ETCD     Server 
3) Kubelet  Server 


"Client Components" in Kubernetes Cluster
-----------------------------------------
These below are clients and should have Client-Certificates 

1) Kube-Scheduler           <-- It is a Client for Kube-api server, so we generate a Client-Certificate for it 
2) Kube-Controller-Manager  <-- It is a Client for "kube-api" server, so we generate a Client-Certificate for it 
3) kube-proxy               <-- It is a client for "kube-api" server, so we generate a Client-certificate for it. 






CERTIFICATE AUTHORITY ( CA )
----------------------------

> openssl genrsa --out ca.key 2048                                              <-- ca.key     it Generatey-key
> openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA"  -out ca.csr           <-- ca.csr     It generate "Certificate-Signing-Request"
> openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt                      <-- ca.crt     It generate "Sign-Certificates"


ADMIN USER:
-------------
> openssl genrsa -out admin.key 2048                                                      <-- admin.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin/O=system:masters" -out admin.csr   <-- admin.csr  It generate "Certificate-Signing-Request"
                                                                                           "O=system:masters"  <-- with this we are saying that this is not 
                                                                                                                  a normal user, it is a admin-user
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out ca.key                          <-- ca.crt     It generate "Sign-Certificates"

KUBE SCHEDULER
-------------
> openssl genrsa -out scheduler.key 2048                                            <-- scheduler.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin" -out scheduler.csr          <-- scheduler.csr  It generate "Certificate-Signing-Request"
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out scheduler.key             <-- scheduler.crt  It generate "Sign-Certificates"


KUBE CONTROLLER MANAGER
-----------------------
> openssl genrsa -out controller-manager.key 2048                                      <-- controller-manager.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin" -out controller-manager.csr    <-- controller-manager.csr  It generate "Certificate-Signing-Request"
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out controller-manager.key       <-- controller-manager.crt  It generate "Sign-Certificates"

KUBE PROXY
----------
> openssl genrsa -out kube-proxy.key 2048                                      <-- kube-proxy.key  It "Generate-Key"
> openssl req -new -key admin.key -subj "CN=kube-admin" -out kube-proxy.csr    <-- kube-proxy.csr  It generate "Certificate-Signing-Request"
> openssl x509 -req -in admin.csr -CA ca.crt -CAkey  -out kube-proxy.key       <-- kube-proxy.crt  It generate "Sign-Certificates"


---------------------------------------------------------------------------------------------------------
> cat  /etc/kubernetes/manifests/kube-apiserver.yaml 

> openssl x509 -in <path to xxx.crt>  -text -noout                        
> openssl x509 -in /etc/kubernetes/oki/apiserver.crt -text -noout                <-- like this we can get all the info about the cretificate.
                                                                                   With "-noout" it will not create a new file with the Certificate 
                                                                                   it will just print the Certificate info on the console  
  
> k get csr                                         <--- to see all the "Certificate Signing Requests"
> k certificate approve <name-of-Certificate>       <-- with this we can approve the Certificate
> k get csr <name-of-certificate> -oyaml            <-- like this we can see the details inside the certificate 

> k certificate deny <name-of-certificate>          <-- like this we can deny a "Certificate Signing Request" 

> k delete csr <name-of-certificate>                <- in last step first we "deny" the request and after that we delete the CSR request. 










How we can make a REST call to "KUBE-API" server with these admin Certificates and keys ? 

> curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt    <-- This is the example of REST-Api call that we make 
                                                                                                       to the "KUBE-API" server.
  






#######################################################################################################
kubeconfig
----------

./kube/config            <-- be default "kubectl" look for the "config-file" under this folder 


> kubectl config -h                                   <-- like this we can list all the commands  for "config"

> kubectl config view                                 <-- with this we can view the config 

>> kubectl config use-context abc@production          <-- like this we are setting the default config to "abc@production" 

> kubectl config --kubeconfig=my-base-config use-contexts dev-frontend   <-- if we have two "./kube/config" files then we can add 
                                                                            the file name like this "--kubeconfig=my-base-config"

> k config use-context research --kubeconfig <path-to-file>       <-- "--kubeconfig" this attribute is used when we put the .config file 
                                                                     in anyother location rather then ./kube/config

> mv /root/my-kube-config   /root/.kube/config           <-- like this we can move from other place to default folder  the .config file

Steps to create Context file
----------------------------

Step-1) Add Cluster details
> kubectl config --kubeconfig=base-config set-cluster development --server=https://1.2.3.4           <--"my-base-config" is the file name for config

Step-2) Add user details
------------------------
> kubectl config --kubeconfig=base-config set-credentials my-user-a --username=dev --password=my-pass       <-- "my-user-a" is the name of the user for this config file.

Step-3) Setting Context 
> kubectl config --kubeconfig=base-config set-context dev-frontend --cluster=development --namespace=frontend --user=my-user-a




###################################################################################################################
RBAC    Role, Rolebinding, ClusterRole, ClusterRolebinding
----

Core API Group:  
---------------
Pods, Nodes, Namespaces, Services, Configmaps, Secrets, Events, Endpoints, PV, PVC 

Named API Groups:
-----------------
apps:           Deployments, ReplicaSets, StatefulSets, DaemonSets, DeploymentsRollback
extensions:     Ingress, NetworkPolicies, PodSecurityPolicies
batch:          Jobs, CronJobs
storage.k8s.io: StorageClass, VolumeAttachment
policy:         PodDisruptionBudgets



Role:
-----

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]                       <--"" indicates the core API group, Not the "Named Api Group" (imp)
  resources: ["pods"]
  verbs: ["get", "watch", "list"]



apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane                                  <-- "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role                                    <-- this must be Role or ClusterRole
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

> cat /etc/kubernetes/manifests/kube-apiserver.yaml       <-- to view "api-server" options where cluster is created with "kubeadm" tool     
> ps -aux | grep authorization 







> kubectl auth can-i  create deployments             <-- like this we can check if the current user is alloweed to "Create Deployments"

> kubectl auth can-i create pods                     <-- like this we can check if the current user is alloweed to "Create Pod"       

> kubectl auth can-i create pods --as dev-user       <-- like this we can check the permission for "dev-user" if he is alloweed or not 

> kubectl auth can-i create pods --as dev-user --namespace  my-ns-a  <-- like this we can check it "dev-user" has permission to create pod in the
                                                                             "my-ns-a" Namespace

> kubectl create role my-role --verb=list,create,delete --resource=pods 

> kubectl create role my-role --verb=list,create,delete --resource=pods --resourceNames=pod-a,pod-b   <-- this role is applied only for "pod-a" and "pod-b"

> kubectl create rolebinding my-rolebinding --role=my-role --user=dev-user




--------------------------------------------------------------------------------------
ServiceAccount 
--------------

serviceAccountName:      <-- this is the name of attribute that we should put under Deplyoment -> template -> spec -> serviceAccountName: xxx


spec:
  automountServiceAccountToken: false     <-- normally kubernetes automatically add the "default" serviceaccount to the pod, but 
                                             with this field we can define that the kubernetes should not add any "service-account" 
                                            token by itself. 



imagePullSecrets:       <-- this we have to put under Deployment -> template -> spec -> imagePullSecrets: -> -name: xxxx    

Note: 
-) if we create new attribute in a Deployment for serviceAccount then it automatically recreate the pods with new SA 
-) But if we directly change the SA inside a pod then, we have to recreate the pod, then only it effects, so better to put 
inside the pod.yaml file 


> kubectl create serviceaccount my-sashboard-sa

> kubectl create token <service-account-name>            <-- like this we can create a Token for a existing "service-account"

> k create secret docker-registry -help        <-- "docker-registry" is  a type of special secret that we should create to pull images from DockerHub  
                                                  like this we can get help for creating a secret for pulling Image from DockerHub



------------------------------------------------------------------------------------------
Security contexts:
------------------

> whoami                                 <-- i return that i am a "root" user or any other "user" on ubuntu 
 
> k exec my-pod -- whoami                <-- here we are checking which user permission i have inside the pod "my-pod" 



Pod level Security context
---------------------------
Pod -> spec -> securityContext -> runAsUser: 1010     <-- Pod level "securityContext"
                                                         like this we can forcefully say the pod to run all the process not as root-user 
                                                        it will run all the process as normal "user" with Id "1010"


Container level Security context
--------------------------------
Pod  -> spec -> containers -> securityContext -> runAsUser: 1010     <-- As we are defining the attribute "securityContext" under "containers"
                                                                        it is then a "container level" security context

SYS_TIME capability:
-------------------
pod -> spec -> containers -> securityContext -> capabilities -> add -> ["SYS_TIME"]        <-- note that the pod must run as "root user" then only it works
 

NET_ADMIN capability:
-------------------
pod -> spec -> containers -> securityContext -> capabilities -> add -> ["NET_ADMIN"]        <-- note that the pod must run as "root user" then only it works

-------------------------------------------------------------------------------------
Network Policies:
----------------
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db                                     <-- this is the pod to which the other Pods trying to access, 
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock: 
            cidr: 172.17.0.0/16                    <-- IP of the pod that from which we trying to access the main-pod 
            except:
              - 172.17.1.0/24
        - namespaceSelector:                      <-- Namespace in which all the pods that can access the main-pod
            matchLabels:
              project: myproject
        - podSelector:                             <-- the pod form which we want to  access the main-pod. This Pod should be in the same namespace that 
            matchLabels:                               the main-pod is having
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:                           <-- this is the IP that the main-pod can send request to 
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978
    - to:          
        - namespaceSelector:                      <-- Namespace to which the main-pod can send request to 
            matchLabels:
              project: myproject
      ports:
        - protocol: TCP
          port: 5978              
    - to:                        
        - podSelector:                             <-- Pod that the main-pod can send the request
            matchLabels:                          
              role: fronten
      ports:
        - protocol: TCP
          port: 5978

> k get networkpolicies 
> k get netpol my-network-policy 


######################################################################################################################
Storage:
-------



====================================================================================================
Volume Examples:
----------------

1) Storing Data directly from Pod in the 

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - image: alpine
      name: alpine
      command: ["/bin/sh", "-c"]
      args: ["shuf -i 0-100 -n l >> /my-storage-path/endnumber.out;"]
      volumeMounts:
        - mountPath: /my-storage-path        <-- (1) we are creating a "Volume" with path "/my-storage-path" in the container. 
          name: my-data-volume                       The service running on the pod will send logs to /my-storage-path folder.
  volumes:                                           Note: we can have two different path names but the "Volume" name must be same
    - name: my-data-volume
      hotstPath:
        path: /data                <--(2) Any data that comes from Pod in the "/my-storage-path" that will be stored at the end in the 
        type: Directory                    "/data" folder on the Node, as we defined "type: Director" so it create a folder on the Node



2) Create a Storage with AWS-EBS

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata: 
  name: my-pod
spec:
  containers:
    - image: alpine
      name: alpine
      command: ["/bin/sh", "-c"]
      args: ["shuf -i 0-100 -n l >> /my-storage-path/endnumber.out;"]
      volumeMounts:
        - mountPath: /my-storage-path        <-- (1) we are creating a "Volume" with path "/my-storage-path" in the container. 
          name: my-data-volume                       The service running on the pod will send logs to /my-storage-path folder.
  volumes:                                           Note: we can have two different path names but the "Volume" name must be same
    - name: my-data-volume
      awsElasticBlockStore:
        volumeID: <ID-Nummber>                <--(2) Any data that comes from Pod in the "/my-storage-path" that will be stored at the end in the 
        fsType: ext4                          "/data" folder on the Node, as we defined "type: Director" so it create a folder on the Node


=====================================================================================================================
Persistent Volume:
-----------------

Commands:

> kubectl get pv
> kubectl get pvc
> kubectl delete pvc my-claim-name
> 


-) Their are three types of "PersistentVolume" modes
   - RWO--ReadWriteOnce        <-- the volume can be mounted as read-write from pods placed in a single node. ReadWriteOnce access mode still can    
                                    allow multiple pods to access the volume when the pods are running on the same node.
   - ROX--ReadOnlyMany         <-- the volume can be mounted as read-only from pods placed in many nodes.
   - RWX--ReadWriteMany        <-- the volume can be mounted as read-write from pods placed in many nodes.
     RWOP--ReadWriteOncePod    <-- 

-) The "accessModes" of both PV and PVC should be match, then only PVC can claim a PV 

>) persistentVolumeReclaimPolicy: Retain   <-- After we delete a PVC, the attached PV will be remain their till we manually delete it, 
                                               It can not be re-used by any other Claims.
>) persistentVolumeReclaimPolicy: Delete   <-- when we delete the PVC the attached PV will also be delete at the same time

>) persistentVolumeReclaimPolicy: Recycle   <-- In this case when a PVC is deleted, the data stored in the PV will also be deleted,
                                              But the PV remain their, and after this recycle process the PV can be claimed by any other PVC 
                                               

Step-1) Create "Persistent Volume"

pv.yaml
--------
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: my-per-volume
  labels:
    name: my-pv
spec:
  accessModes:
    - ReadWriteOnce        
  capacity:
    storage: 1Gi
  persistentVolumeReclaimPolicy: Retain  
  hostPath:
    path: /data     
    type: Directory                    <-- "type: Directory" it is optional, if we use "hostPath" then it is automatically taken



pv.yaml
--------
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: my-per-volume
  labels:
    name: my-pv                    <-- here we can define a lable in the PV
spec:
  accessModes:
    - ReadWriteOnce        
  capacity:
    storage: 1Gi
  persistentVolumeReclaimPolicy: Delete  
  awsElasticBlockStore:
    volumeID: <ID-Nummber>  
    fsType: ext4 


Step-2) Create "Presistent Volume Claim"

pvc.yaml
--------
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: my-claim-aaaa
spec:
  selector:
    matchLabels:
    name: my-pv                       <-- As we have define the "Selector" then it search for "PV" with this lable and attach to it
  accessModes:
    - ReadWriteOnce        
  resources:
    request:
      storage: 500Mi



Step-3) Attach the above created "PVC" in the Pod.yaml below

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim-aaaa

----------------------------------------------------------------------------------------------------------
StorageClass:
------------

> k get storageclass 
> k get sc 


StorageClass can use three type of Disks that we define under "Parameters"

Silver StorageClass:               Gold StorageClass:          Platinum StorageClass:
-------------------                -----------------           ----------------------
type: pd-standard                  type: pd-standard           type: pd-ssd
replication-type: none             replication-type: none      replication-type: none 


Note:
volumeBindingMode: WaitForFirstConsumer  or  Immediate

Step-1) Create a "StorageClass"

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: my-storage-class-bb
provisioner: ebs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer 
parameters:
  type: pd-standard 
  replication-type: none  


Step-2) Create PVC 

pvc.yaml
--------
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: my-claim-aaaa
spec:
  storageClassName: my-storage-class-bb
  accessModes:
    - ReadWriteOnce        
  resources:
    request:
      storage: 500Mi


Step-3) Attach the above created "PVC" in the Pod.yaml below

pod.yaml
--------
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim-bb


#################################################################################################################################
Networking:
----------

----------------------------------
> route  
> ip route           <-- both of these command we use to see the "routing-table" entries 
-----------------------------------

> ip link    <-- to list and modify interface on the host.

> ip addr    <-- to see the ip-addresses link to above interfaces.

> ip addr add 192.168.1.10/24 dev eth0         <-- like this we can set ip-address on a Interface. This command we have to execute on linux machine 
                                                  Note: all the changes made above are valid only till a new re-start. All will be deleted.
                                                  If we want to presist the changes, then we have to put then in the Network-interface file.

> i route add 192.168.2.0/24  via  192.168.1.1

> ip route add default via  192.168.2.1

> pin <any ip>                         <-- if the ip address is reachable then it return result,

Node-A             Node-B           Node-C      
Network-A  -->   Network-B  -->   Network-C          <-- these are the networks created on every Node.

> echo 1 > /proc/sys/net/ipv4/ip_forward        <-- Execute this command on the Node-B, after doing this we can send a packet from 
                                                    Node-A to Node-c  through the Node-B, As we have done the "ip_forward" 
                                                    this command will make in the file "/etc/sysctl.conf = 1" file 

> cat /proc/sys/net/ipv4/ip_forward             <-- if it return  "0" it means no ip_forward is set for this Node or linux machine.



-------------------------------------------------------------------------------------
DNS Server:
----------

> cat >> /etc/hosts    <-- in this file add   ->   192.168.1.11  db      <-- this is file is placed on every linux machine.


Assume we have a DNS service with ip - 192.168.1.100

> /etc/resolv.conf     <-- on every Linux machine have a file "resolv.conf" in it just add the "DNS" server ip, then when we do a ping then 
                           it just check the ip in DNS server and return the result.

> cat /etc/nsswitch.conf             <-- here we can define, which DNS server, the host first look, it should search in local-file with DNS entry 
                                       or the call first go to any extern DNS server 
--------------------------------------------------------------------------------------------
Domain Names 
-------------

below is example of "www.google.com" 

.                             <-- Root 
.com, .net, .edu, .de         <-- these are Top Level Domain Name
google                        <-- This is domain name assign to google
www                           <-- Subdomain



-----------------------------------------------------------------------------------------------
Record Types  in Route 53:
-------------------------

A-Records           <-- mapping ipv4  
AAAA                <-- mapping ipv6
CNAME               <-- mapping  food.web  to  my-web.com, It is name-to-name mapping


--------------------------------------------------------------------------------
> nslookup                        <-- if we put a entry in local "/etc/resolv.conf" file, "nslookup" does not check this file
                                     It directly check and send request to DNS server 

> dig                            <-- if we put a entry in local "/etc/resolv.conf" file, "nslookup" does not check this file
                                     It directly check and send request to DNS server 

> ps aux                                        <-- when we we run this command on the pod, then it will show the process running on the pod
> ps aus                                        <-- when we run this command on the node, then it show all the process running on the whole node.


> ip netns add my-interface-a                   <-- with this we can create a new "Network-Namespace" on a Linux-host or we can say linux-machine 
> ip netns                                      <-- like this we can list all the Network-namespace available on the linux-host 

> ip link                                       <-- to like all the interfaces on the linux-host.

> ip netns exec <network-namespace-name> ip link         <-- like this we can see which Network interface that is defined inside the "Network-namespace"
OR
> ip -n <namesapce-name> link                        <--both of these commands are same 

> arp                                           <-- we can see entried in the linux-host, this show the list of other hosts that we have made connection 

> ip netns exec <namespace-name> arp             <-- this command runs now inside the given "namespace-name"

> route                                        <-- like this we can get the entries from the routing-table on the linux-host 

> ip netns exec <namespace-name> route          <-- like this we will get the routing-table entrie from the given namespace 

> ip address                                     <-- we can get the "Network-interface" and "MAC-address" that is attached to pod or Node

> ip address show <interface-name>             <-- e.g  eth0  <-- it can be a interface name, then it will give us the info about it.

> ip address show type bridge                  <-- it can show us all the "Bridge-network-interface" on the Machine, Node or Pod.

> ip route                                      <-- it will give info about all the routes that are defined.

> ip route add 192.168.1.0/24  via 192.168.2.2

> netstat -plnt 

> route 

> netstat  --help 

> ip link add <any-cable-endpoint-name> type veth peer name <any-cable-endpoint-name>        <-- like this we create two endpoints with a attachment 

> ip link add <cable-endpoint-name> netns <namespace-name>                       <-- like this we can attach one side of the cable-endpoint to the namespace 

> ip link add <cable-endpoint-name> netns <namespace-name>                       <-- like this we can attach other side of the cable-endpoint to the other namespace 

> ip -n <namespace-name> addr add 192.168.15.1 dev <cable-endpoint-name>         <-- like this we can define a ip-address to a namespace 

> ip -n <namespace-name> link set <cable-endpoint-name> up                       <-- like this we are activating the links connection 

> ip link add <any-bridge-name> type bridge                                      <-- like this we can create a network-bridge on a linux-machine 

> ip netns                                                                   <-- to get all the namesapces from a "Docker-host"

 
> ip address show type bridge                                <-- it will show all the "bridge" interface on the machine 



#####################################################################################################################
CNI    (Container network interface)
----
-) The main purpose of the CNI is when a new "Pod" is created then it should assign a "IP" address and also create connections to "Network-bridge"
   that a pod can call any other po or service.

-) kubelet.service          <-- in this file we can see the 
                                "--network-plugin=cni"            <-- which type of newtork-plugin we have defined 
                                "--cni-bin-dir=/opt/cni/bin"      <-- in this directory we see all the supported cni-plugins as executable files e.g  "bridge" "dhcp" "flannel"
                                "--cni-conf-dir=/etc/cni/net.d"   <-- in this location we can find a "configuration" file, this files is used by "kubelet" to know which plugin to be used. 

> ps -aux | grep kubelet       <-- like this we can also see all the configuration about the "CNI" defined in the "kubelet"


> cd /etc/cni/net.d/<name-of-file>               <-- here is the file that is executed by the CNI every time when a new container is created. 


> kubectl logs <wave-net-ere> -n kube-system       <-- As in this example we have "wave" as CNI installed, in the logs we can see 
                                                      a attribute "ipalloc-range:10.x.x.x/x"  <-- from here we can findout the defined 
                                                      range for the  ip-address that will be allocated to every pod. 


--------------------------------------------------------------------------------------------------------------------
Kube-proxy: 
-----------
-) When we create a new "Service" then automatically "kube-proxy" will be executed and it prove a ip address to the newely created "Service" 

> ps aux | grep kube-api-server       <--    in this process or directly in the "kube-apiserver.yaml  file we can see 
                                             a attribute "--service-cluster-ip-range=x.x.x.x/x" from here we can find that 
                                             under which range our cluster is alloweed to provide automatically a ip address 
                                             to the "Service" when it is created 

> iptables -L -t nat | grep <service-name>        <-- like this we can find the "Ip address" that is assigned to the service 

Logs:
----
> /var/log/kube-proxy.log                    <-- from here we can see all the logs that are produced by the "kube-proxy" 





##########################################################################################################
Services & kube-api-server & Kube-proxy   (Vid  223: Service-Networking)
----------------------------------------

kube-api-server --service-cluste-ip-range ipNet (Default: 10.0.0.0/24)

> ps aux | grep kube-api-server               <-- with this we get a output that from-till which range "kube-proxy" alloweed
                                                 to set ip ranges for Services.
                                        Output: kube-apiserver --authorization-mode=Node,RBAC --service-cluster-ip-range=10.96.0.0/12   

> iptables -L -t nat | grep my-db-service        <-- this commmand outputs the ip-address defined for the service and it also shows
                                                    the ip address that is for the pod that is mapped to this service 

> cat /var/log/kube-proxy.log                 <-- it will give us all the logs, how the kube-proxy assign all the ip to service and to pod 
                                                  and mapping them.


########################################################################################################
CoreDNS
-------

-) When a Service is created, then a ip-address is assigned to it, After Service is created then a entry in the CoreDNS table
   is made with the Name of the "service-map" to ip of the service, so when any pod send a request to the Name of the service then
   the request is forwarded to the ip of the service.
   

##############################################################################################################
Ingress:
---------
> k create ingress --help

#################################################################################################################

Check on the Master Node:
------------------------
> service kube-apiserver status 
> service kube-controller-manager status
> service kube-scheduler status 

Logs on Master:
--------------
> k logs kube-apiserver-master -n kube-system 

> sudo journalctl -u kube-apiserver 

Check on the Worker Node:
-----------------------
> service kubelet status 
> service kube-proxy status 




#####################################################################################################################
Json Path queries
-----------------

> k get nodes -o=jsonpath='{.items[*].metadata.name}'                                         <-- it use "-o=jsonpath="


> k get nodes -o=custom-columns=ANY-NAME:.metadata.name                                       <-- like this we can create output with custom "Columna-name" 

> k get nodes -o=custom-columns=ANY-NAME:.metadata.name, ANY-NAME:.status.capacity.cpu 

> k get nodes --sort-by=.metadata.name 

> k get nodes --sort-by=.status.capacity.cpu  

> k get nodes -o=jsonpath='{.items[*].metadata.name}{.items[*].status.capacity.cpu}'   <-- master node 4 4        Assume we have two nodes 

> k get nodes -o=jsonpath='{.items[*].metadata.name}{"\n"}{.items[*].status.capacity.cpu}'       <-- "\n"  for "New-line"

> k get nodes -o=jsonpath='{.items[*].metadata.name}{"\t"}{.items[*].status.capacity.cpu}'       <-- "\t"  for one "tab" space 

> k get nodes -o=jsonpath='{range .items[*]}{.metadata,name}{"\t"}{.status.capacity.cpu}{"\n"}{"end"}'    

> k get nodes -o=custom-columns=NODE-A:.metadata.name

> k get nodes -o=custom-columns=NODE-A:.metadata.name,CPU-A:.status.capacity.cpu 


############################################################################################################
Helm
----

> cat /etc/*release*                                            <-- to get the information about the Operating-system installed on the machine e.g "ubuntu" 

> helm --help                                                   <-- it will print all the sub-commands that we can use with "helm"

> helm version 

> helm --debug                                                   <-- to enable "verbose" option 


------------------------------------------------------
> helm install  <my-chart-name> <chart-name-from-provider>       <-- it will pull the "helm-chart" and "install" it also 



> helm pull --untar <chart-name-from-provider>                    <-- it will only pull the "helm-cart" and store it locally.

> ls <chart-name-from-provider>

> helm install <my-chart-name>  ./path-to-chart                    <-- like this we can install the above "pulled" chart           
> helm uninstall <my-char-name>                                 <-- to uninstall the package 

OR

> helm repo add <my-name>  <link-for-repo> 
> helm repo add bitnami  https://charts.bitnami.com/bitnami 
 


> helm repo list                           <-- like this we can find all the manually "added" repos                                
---------------------------------------------------------

                "Check the difference between these two commands" 

> helm list                                <-- to list all the available packages. 



> helm search repo <name> 
> helm search repo bitnami                   <-- if we have added "repo" manually then like this we can search them. 
                               App-version   <-- with this "helm search repo" command we can check the "APP VERSION"
                               CHART VERSION <-- with this "helm search repo" command we can check the chart-version 


Search in the Artifact-hub:
----------------------------
> helm search hub <chart-name-from-provider>                         <-- to search the chart with provider-name  

> helm search hub <my-chart-name>                                   <-- to search the chart with <my-name> 




















###################################################################################################################
Killler.sh  Questions:
------------------------
1) Write all the contexts added in you .kube/config file, with "kubectl" command /config.txt 


> kubectl config get-contexts  > /config.txt       <-- to get all the contexts 

> kubectl config current-context                 <-- to get the current context 

> kubectl config use-context <context-name>       <-- Command to switch to a specific context 

> cat ./kube/config | grep current-context         <-- like this we can print current-context without using kuebctl 

> kubectl --context=<context-name> <command>     <-- like this we can also run a command by using a special context 

> k config user-context <any-name> --kubeconfig /abc/config                               <-- if the  "config" file is placed at any other location.

-----------------------



























































