###########################################################################################################
100 Questions


Q-1) Update the image of the deployment  ?

1)
> k get deploy -owide                                 <-- like this we will see all the deployment with the image-name defined on them 

2)
> k set image deployment <deploy-name> <containerName>=<image-name>   <-- like this we can update the image, main point is we have to give 
                                                                         "ContainerName" and new image 

----------------------------------------------------------------------------------------
Q-2) Change the static pod path ?

1) 
> ps -aus | grep kubelet 

2)
--config=/var/lib/kubelet/config.yaml            <-- then find this path 

3)
vi /var/lib/kubelet/config.yaml                 <-- open this file and see the option "staticPodPath: ......" and update the path 
                                                    after that just save the file and no need to do anything else 

----------------------------------------------------------------------------------------
Q-3)  Upgrade the cluster ?
                    -kubeadm: 1.19.0
                    -kubelet: 1.19.0
                    -kubectl: 1.19.0

1) k drain <node-name>
2) k cordon <node-name>
3) apt update 
4) apt install kubeadm=1.19.0-00
5) kubeadm upgrade apply v1.19.0
6) apt install kubelet=1.19.0-00
7) apt install kubectl=1.19.0-00
8) systemctl restart kubelet 
9) kubectl uncordon <node-name> 


------------------------------------------------------------------------------------------
Q-4) Create a new deployment and scale with 2 replicas 
            - name: httpd-deploy 
            - image: http:alpine 

1) k create deployment <deploy-name> --image=http:alpine 

2) k scale deployment <deploy-name> --replicas=2

------------------------------------------------------------------------------------------
Q-5) Create a pod with labels 
     -name: mypod
     -image: redis:alpine
     -labels: tier=redis 

1) k run my-pod --image=redis:alpine -l tier=redis 

-------------------------------------------------------------------------------------
Q-6) Creat a pod in a namespace 
     -namespace: tech
     -pod name: my-pod
     -image: redis:alpin

1) k create  namespace tech 
2) k run my-pod --image=redis:alpine -n tech 

----------------------------------------------------------------------------------------
Q-7) Create a pod and expose it
      -name: mypod
      -image: redis:alphine
      -service name: my-service
      -port: 8080
      -targe-port: 8080

1) k run mypod --image=redis:alpine 
2) k expose pod mypod --name=my-service --port=8080 --target-port:8080

-----------------------------------------------------------------------------------------
Q-8)  create a deployment with 3 replca and upgrade by using "rolling-update"
      -name: my-deploy 
      -image: nginx:1.16
      -upgrage: nginx:1.17

Note: As we need to show the rollingupdate so we will create a .yaml file first then create it 


1) k create deploy my-deploy --image=nginx:1.16 --replicas=3 --dry-run=client -oyaml > deploy.yaml 

2) k set image deployment/my-deploy nginx=nginx:1.17 --record        <-- the main point it we need to use the 
                                                                      name of the "image"  "nginx=nginx:1.17"
                                                                      and also "--record" that we can rollback 
                                             Note: try how to rollback to old version and again this version 

3) k rollout history deployment my-deploy 


-------------------------------------------------------------------------------------------
Q-9) create a static pod with a command  "sleep 1000"
       -name: my-pod 
       -image: redis:alpine 

1) 
> ps -aus | grep kubelet 

2)
--config=/var/lib/kubelet/config.yaml            <-- then find this path 


3) k get nodes                            

4) my-node  ssh                           <-- we have to first go inside the node 

5) sudo -i                               <-- we have to change to the "root" user 

6) vi /var/lib/kubelet/config.yaml                 <-- open this file and see the option "staticPodPath: ......"
                                                      in this attribute the path is defined, Just copy that path 

7) logout                                <-- just logout from the node 

8) k run my-pod --image=busybox --command sleep 1000 --dry-run=client -oyaml > pod.yaml 

9) k get nodes -o wide                  <-- here we are taking the ip of the node 

10) sudo scp pod.yaml <node-ip>:/root/        <-- here we are copying the file from worker-node to controleplane node 

11) minikube ssh                              <-- we ssh to main node again 

12) sudo -i                                    <-- get permissions as "root" user 

13) cp /root/pod.yaml    /etc/kubernetes/manifests/       <-- "/etc/kubernetes/manifests/"   <-- this is the static pod path on controleplane node 

14) logout                        <-- as "kubelet" is not installed on "master" node so we have to logout every time out from master-node 


--------------------------------------------------------------------------------------------------
Q-10)  Tain a node to be unschedulabel and test it by creating a pod on it ?
        -node-name: test-node 
        -Taint  
         -key:env_type
         -value: production
         operator: NoSchedule 

  Note: just for testing i will create two pods, one with same labels and one with no labels that it can not be placed on the node 

1) k taint node <node-name> env_type=production:NoSchedule  

2) k describe node <node-name> | grep -l taint               <-- like this we can check that the above "taint" command works or not 

3) k run test-pod-a --image=redis:alpine                    <-- after creating check if this pod is running on the tainted node or not 

4) k run test-pod-b --image=redis:alpine --dry-run=client -oyaml > pod.yaml 

5) 
spec:                                               <-- like this in  pod defination file, under "spec" we can create a tolerations
  tolerations:
  - key: env_type 
    effect: NoSchedule 
    operator: Equal 
    value: porduction  

6) k apply -f pod.yaml 


----------------------------------------------------------------------------------------------------
Q-11) Create a new Service-account, Cluster-role, Cluster-rolebinding ? 
      and make it possible to list the persistent volumes, And create a pod with the service-account 
      -service account name: my-sc-account 
      -clusterrole name: pv-role
      -clusterrolebinding: pv-binding 
      -pod name: pv-pod 
      -image: redis 


1) k create serviceaccount my-sc-account 

2) k create clusterrole pv-role --resource=persistentvolumes --verb=list 

3) k create clusterrolebinding pv-binding --clusterrole=pv-role --serviceaccount=default:my-sc-account        <-- "default" we need to put this name 
                                                                                                              as we want to make it as default 

4) k run pv-pod --image=redis --dry-run=client -oyaml > pod.yaml 

5) 
spec:
  serviceAccountName: my-sc-account                    <-- like this we can add the "ServiceAccount" to the pod 

6) k create -f pod.yaml 

----------------------------------------------------------------------------------------------------
Q-12) Create a NetworkPolicy that allows  all pods in the "tech-deploy" namespace to have communication only on a single port 
      -NetworkPolicy name: tech-policy 
      -Port: 80/TCP 

1) k label namespace tech-deploy app=tech-deploy            <-- first we add a "label" to the namespace 


2) vi network.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: tech-deploy
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              app: tech-deploy 
      ports:
        - protocol: TCP
          port: 80

> k apply -f network.yaml 

-------------------------------------------------------------------------------------------------------
Q-13) List all the internla IPs of all the nodes in the cluster and save it to file /doc/ip_nodes.txt 


> search in kubernetes documentation for "kubernetes cheat sheet"

> kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'   <-- this is the command we get from the  
                                                                                                    "kubernetes cheat sheet" page 

> kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /doc/ip_nodes.txt     <-- here i have changed 
                                                                                                                          "ExternalIP"  to "InternalIP"

Note: just for double check, check the internal ip of the node and then check what is written on the file .


----------------------------------------------------------------------------------------------------------
Q-14) Create a multipod with two containers. And add the command "sleep 3600" to container 2
      -name container 1: micro 
      -image: nginx 

      -name container 2: mega 
      -image: busybox 


1) k run multi-pod --image=busybox --command sleep 3600 --dry-run=client -oyaml > multi.yaml 

2) 
spec:
  containers:
  - command:
    - sleep 
    - "3600"
    image: buxybox
    name: mega
  - name: micro
    image: nginx   

3) k apply -f multi.yaml 

--------------------------------------------------------------------------------------------------
Q-15) A new colleague "Jan" has joined your team. Create a new user and grant him access to the cluster.
      He should have the permission to create, list, get, update and delete pods in the "tech" namespace. 


1) Search in kubernetes-documentation "Certificate signing request" 

2) openssl genrsa -out jan.key 2048

3) openssl req -new -key jan.key -out jan.csr -subj "/CN=jan"        <-- after executing this command it ask many yes-no   just press enter 


Note: on the same kubernetes-documentation page search at the end "CertificateSigningRequest" 

4) vi certificatesigningrequest.yaml 

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: jan 
spec:
  request: xxxxxxxxxxx paste the certificate here xxxxxxxxxxxx
  signerName: kubernetes.io/kube-apiserver-client
  expirationSeconds: 86400  # one day
  usages:
  - client auth

5) cat jan.csr | base64 | tr -d "\n"           <-- it will return a "Certificate", just copy that certificate and past in the above certificatesigningrequest.yaml  file 
                                                    this command also given on the same page 

6) k apply -f certificatesigningrequest.yaml   

7) k get csr                                   <-- the certificate-signing-request  should be in a pending start, we should approve it 

8) k certificate approve jan                   <-- like this we can approve the Certificate-signing-request 

9) k create role my-role  --verb=get,list,watch,delete --resource=pods  --namespace=my-namespace

OR

9) vi role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: tech                                     <-- as it is a "role" not a cluster-role and above defined permissions for "tech" namespace 
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "create", "list", "update"]

10) k create rolebinding <any-name> --role=pod-reader --user=john --namespace=my-namespace

10) vi rolebinding.yaml 

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: tech
subjects:
- kind: User
  name: jan                               # "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role                 #this must be Role or ClusterRole
  name: pod-reader                # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io

11) k apply -f role.yaml
12) k apply -f rolebinding.yaml 

13) k auth can-i delete pods -n tech --as jan             <-- here i am checking if "jan" user is alloweed to delete a pod in the "tech" namespace 


-----------------------------------------------------------------------------------------------------------
Q-16) Create a service from the "green" pod and run a DNS lookup to check the service and write to file /doc/lookup.txt 
      -service name: green-service
      -port: 80

Note: As per given, "green" pod is their, i have to create a new Service for it and after that i create a new pod and run inside this new 
       pod "nslookup <service-name>"

1) k expose pod green --name=green-service --port=80

2) k run my-pod --image=busybox:1.28 --command sleep 3600         <--here we are creating a new pod, just for running "nslookup" command as pre given in the question 

3) k exec -it my-pod -- nslookup green-service               <-- first check if the output is correct 

4) k exec -it my-pod -- nslookup green-service > /doc/lookup.txt         <-- now put the output in a .txt file  

5) cat /doc/lookup.txt                             <-- at the end always check the content of the file if everything is ok 


----------------------------------------------------------------------------------------------------------
Q-17) Create a secret and mount it to the pod "yellow"
      -secret name: yellow-secret 
      -secret content: password=kube1234


1) k create secret generic yellow-secret --from-literal=password=kube1234

2) k get pod yellow -oyaml > yellow.yaml 

3) 
apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
    - name: dotfile-test-container
      image: registry.k8s.io/busybox
      volumeMounts:
        - mountPath: "/secret"      
          name: yellow-secret-a 
  volumes:
    - name: yellow-secret-a 
      secret:
        secretName: yellow-secret

4) k delete pod yello                     <-- as we want to create a new pod with the .yaml file, so we first delete the old pod. 

5) k apply -f yellow.yaml 

6) k describe pod yellow                  <-- check it the secret is attached to the pod 

Note: just check in the real how the secret is mounted, it has created a folder for volume or how it has done 

---------------------------------------------------------------------------------------------------------------
Q-18) List all the persistent volumes sorted by capacity and write to file /doc/pervol.txt ?

Note: just for testing create 3 different pv with different capacity 

Note: first go to kubernetes documentation "cheat sheet" and search for  "sorted by"

1) k get pv --sort-by=.spec.capacity.storage            <-- check if the order is sorted or not 

2) k get pv --sort-by=.spec.capacity.storage > /doc/pervol.txt          

3) cat /doc/pervol.txt                                      <-- always check the output 

------------------------------------------------------------------------------------------------------------------
Q-19) Find the pod label  environment=process, find all the pods running high CPU workloads and write the name of which is 
      consuming the most CPU to the file /doc/cpu.txt 


1) k get pods -l environment=process           <-- first check how many pods are having a lable "environment=process" 

2) k top pod --sort-by cpu -l environment=process                 <--  the cpu usages of all the pods 

3) k top pod --sort-by cpu -l environment=process | head -2        <-- "head -2" means return only first two lines, first line is header, second is with the name of pod with higherst usage 

4) k top pod --sort-by cpu -l environment=process | head -2 > /doc/cpu.txt    

------------------------------------------------------------------------------------------------------------------
Q-20) Use JSON path to get all the node names and store them in the file /doc/nameofnodes.txt 

Note: go to kubernetes documentation "cheat sheet" search for "get nodes"

1) kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'         <-- copy this from the documentation page 

2) kubectl get nodes -o jsonpath='{.items[*].metadata.name}'      <-- modify the command and check the output 

3) kubectl get nodes -o jsonpath='{.items[*].metadata.name}' > /doc/nameofnodes.txt        


--------------------------------------------------------------------------------------------------------------------
Q-21) Show the logs from the container and save it to /doc/nginx.log 
       -pod name: direct-pod 
       -Container: nginx 
       -Namespace: dev-net 

1) k get pod direct-pod -n dev-net                   <-- first check the pod is running 

2) k logs direct-pod -c nginx -n dev-net              <-- check if the logs can be displayed 

3) k logs direct-pod -c nginx -n dev-net  /doc/nginx.log 

OR
> k logs direct-pod nginx -n dev-net                 <-- with "-c" we can also get the logs

--------------------------------------------------------------------------------------------------------------------
Q-22) Create a new ingress resource and exposes service "hello" on path /hello by using service port 5678
       -name: ingress-connect 
       -namespace: host-dev 

1)  vi ingress.yaml                         <-- just copy from the documentation the example ingress, this is easy to solve this question 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-connect 
  namespace: host-dev
spec:
  rules:
  - http:
      paths:
      - path: /hello 
        pathType: Prefix
        backend:
          service:
            name: hello 
            port:
              number: 5678

2) k apply -f ingress.yaml 

------------------------------------------------------------------------------------------------------------------------
Q-23) Overwrite the label of the dev-nginx pod with the value "env=true"

Note: their is already a label "env=green" defined on the pod, we have to overwrite this label 


1)  k label pod/dev-nginx env=true --overwrite 


----------------------------------------------------------------------------------------------------------------------
Q-24) Upgrade the image in the deployment "green" to buxybox:1.28, check the history and roll back 

1) k set image deployment green busybox=busybox:1.28 --record

2) k get deployment green                                       <-- check the image version is updated or not 

3) k rollout undo deployment green                              <-- rollback the updation 

4) k get deployment green                                       <-- check if the old version is again their or not .


--------------------------------------------------------------------------------------------------------------------------
Q-25) Find out how many pods are available with the label env=green in the cluster and write them to the file /doc/podsavailable.txt 

1) k get pods -l env=green > /doc/podsavailable.txt 



----------------------------------------------------------------------------------------------------------------------------
Q-26) The pod "red" is failing, Find out why and fix the issue ?

1) k describe pod red                 <-- here we see "sleeeep" command is written wrong through the error logs"

2) k get pod red -oyaml  > red.yaml    <-- in the red.yaml file just update the "sleep" 

3) k apply -f red.yaml                

---------------------------------------------------------------------------------------------------------------------------
Q-27) Create a pod that will only be scheduled on a node with a specific label 
      -pod name: blue
      -image: nginx 

1) k label nodes <node-name> disk=green                               <-- to label the pod, first we have to label the node 

2) k describe nodes <node-name>                                       <-- check if the label is created 

3) k run blue --image=nginx --dry-run=client -oyaml > blue.yaml  

4) vi pod.yaml 

spec:
  nodeSelector:
    disk: green 

5) k apply/create -f pod.yaml                      


------------------------------------------------------------------------------------------------------------------------
Q-28) Create a pod which uses a persistent volume for storage
      -pod name: yellow
      -image:    busybox 
      -persistent volume name:       yellow-pv-volume 
      -persistent volume claim:      pvc-yellow 
      -persistent volume claim size: 100mi

Note their is already pv volume "yellow-pv-volume"


1) vi persistentvolumeclaim.yaml 

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-yellow 
spec:
  accessModes:
    - ReadWriteOnce 
  resources:
    requests:
      storage: 100mi

2) k apply -f persistentvolumeclaim.yaml 

3) k run yellow --image=busybox --dry-run=client -oyaml > yellow.yaml 

4) vi yellow.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: yellow 
spec:
  containers:
    - name: busybox
      image: busybox 
      volumeMounts:
      - mountPath: "/data"
        name: pv-storage 
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: pvc-yellow

5) k apply -f yellow.yaml 

--------------------------------------------------------------------------------------------------------------
Q-29) Remove the taint added to the node "my-node-a",
      Taint: key=red:NoSchedule 

> k describe node my-node-a | grep Taint                   <-- it will show the taint value 

> k taint nodes my-node-a key=red:NoSchedule-                <-- just put "-" at the end of the Taint then it will be removed

> k describe node my-node-a | grep Taint                 <-- now after removing it has no value. 

--------------------------------------------------------------------------------------------------------------
Q-30) Take a backup and restore ETCD ?

> cat /etc/kubernetes/manifests/etcd.yaml | grep file               <-- like this we will get all the info that we need for backup

Restoring from a backup simple way:
--------------------------------

1) ETCDCTL_API=3 etcdctl --data-dir <new-location-which-is-used-by-etcd-db> snapshot restore  <path-of-backup>/snapshot.db

This line is optional  "2) vi etcd.yaml          "--data-dir=<new-location-which-is-used-by-etcd-db>"   <-- update the location in this file" (see 133-last question to understand)

2b) volumes:
      path: var/lib/etcd            <-- chage this path to the  <new-location-which-is-used-by-etcd-db>


3) systemctl restart etcd 

OR

Restoring from a backup:
----------------------
1) ls <path-where "snaposhot.db" is stored> 

2) service kube-apiserver stop

3) ETCDCTL_API=3 etcdctl --data-dir <new-location-which-is-used-by-etcd-db> snapshot restore  <path-of-backup>/snapshot.db

This line is optional  "4) vi etcd.yaml          "--data-dir=<new-location-which-is-used-by-etcd-db>"   <-- update the location in this file" (see 133-last question to understand)

4b) volumes:
      path: var/lib/etcd            <-- chage this path to the  <new-location-which-is-used-by-etcd-db>


5) systemctl daemon-reload 

6) service etcd restart 

7) service kube-apiserver start 

-------------------------------------------------------------------------------------------------------------
Q-31) Schedule a pod on the node "my-node-a" by using tolerations 
      -pod name: blue-pod 
      -image: nginx 

Note: Node has already a Taint   Taint: app=frontend:NoSchedule 

1) k run blue-pod --image=nginx --dry-run=client -oyaml > blue-pod.yaml 

2) vi blue-pod.yaml 

spec:
  tolerations:
  - effect: NoSchedule 
    key: app 
    operator: Equal 
    value: frontend 


3) k apply -f blue-pod.yaml 

4) k get pod blue-pod -owide            <-- with this we can see on which node this pod is running on 


----------------------------------------------------------------------------------------------------------------
Q-32) Apply autoscaling to the "green-deployment" with a minimum of 5 and maximum of 10 replicas and a target CPU of 75%

Note: their is a deployment name "green-deployment"

1) k autoscale deployment green-deployment --min=5 --max=10 --cpu-percent=75  

2) k get hpa      <-- with this we can see the values of "green-deployment"

3) k get pods                   <-- check if we are having 5 replicas of the pods 


-------------------------------------------------------------------------------------------------------------------
Q-33) Check how many nodes are in ready state and write it to the file /doc/readynodes.txt 

> k describe nodes | grep ready | wc -l             <-- it returns the number count of ready nodes 
                                               Note: just check if this command also counts header also 
                                                   "wc -l"  <-- it just count how many lines are their, so be carefull
                                                              as some time it can also cout the header line also, so always recheck

> k describe nodes | grep ready | wc -l   > /doc/readynodes.txt 

--------------------------------------------------------------------------------------------------------------------
Q-34) Create a pod and set the environment variable "dev=dev10"
       -pod name: grey 
       -image: nginx 

1) k run grey --image=nginx --restart=Never --env=dev=dev10 --env=qa=bbb

2) k exec -it grey -- sh -c 'echo $dev'                               <-- like this we can check that env variable is set or not in the pod 
OR                                                                        it should return us the value 
2) k exec grey -- sh -c 'echo $dev'                            <-- this command works without "-it" also 

3) k describe pod grey              <-- like this we can also see the configured env variable 


-------------------------------------------------------------------------------------------------------------------
Q-35) Create a configmap and add it to the pod 
        -pod name: blue
        -configmap name: my-config 
        -Data: user=root,password=pass1234


1) k create configmap my-config --from-literal=user=root --from-literal=password=pass1234  

2) k describe configmap my-config                        <-- like this we can re-check the created configmap 

3) k get pod blue -oyaml > blue-pod.yaml 

4) vi blue-pod.yaml 

spec:
  containers:
  - envFrom:
    - configMapRef:
        name: my-config 

5) k delete pod blue                     <-- first delete the old pod 

6) k apply -f blue-pod.yaml               

7) k exec -it blue -- env                   <-- like this we can directly on the pod if the env variable is set or not 

-----------------------------------------------------------------------------------------------------------------
Q-36) List all the events sorted by the timestamp and write the result to file /doc/events.log 

Note: search in kubernetes-documentation "cheat sheet" after that in page search "get events"

1)  kubectl get events --sort-by=.metadata.creationTimestamp         <-- check the output 

2)  kubectl get events --sort-by=.metadata.creationTimestamp   /doc/events.log 



----------------------------------------------------------------------------------------------------------------
Q-37) Create a pod with a non-persistent volume 
      -pod name: non-per-pod 
      -image: redis 
      -mount path: /data/per-redis 


Note: As it ask for "non-persistent" so first we search in the kubernetes-documentation "emptyDir: {}" 

1) vi non-persistent-pod.yaml 

apiVersion: v1           
kind: Pod
metadata:
  name: non-pre-pod 
spec:
  containers:
  - image: redis 
    name: test-container
    volumeMounts:
    - mountPath: /data/per-redis 
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:                          <-- this example is from documentation but in video he shows  "emptyDir: {}"
      sizeLimit: 500Mi


------

apiVersion: v1           
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}

2) k apply -f non-persistent-pod.yaml 

----------------------------------------------------------------------------------------------
Q-38) The node "minikube" is in "NotReady" state, Investigate and bring the node back to ready state.


1) k get nodes                                 <-- first check the "STATUS" of the node .

2) k describe node minikube                    <-- in the "Events" below it shows also same "NodeNotReady"

3) <name-of-node> ssh                          <-- first go inside the node .

4) sudo -i                                     <-- like this we will get "root" priveleges 

5) systemctl status kubelet                    <-- check the status of the "kubelet"
                                                  If in the third or fourth line "Acticve: inactive (dead)"
                                                  like this we found thats mean the problem is with "kubelet"

6) systemctl start kubelet 

7) systemctl enable kubelet 

8) systemctl status kubelet                       <-- here it should show the "active (running)

9) logout 

10) k get nodes                                <-- now it should show the status "READY" 

-------------------------------------------------------------------------------------------------
Q-39) Make the node "my-node" unavailable and reschedule all the pods on it.


1) k get pods -owide                                    <-- check how many pods are their and on which node they are running 

2) k get nodes                                          <-- it should show two or more nodes, so that we can "cordon" one node 

3) k cordon <my-node>                                   <-- now we have make it unschduable 

4) k drain <my-node>  --ignore-daemonsets --force       <-- now we are releasing the pods from this node.

5) k get pods -owide                                    <- now we can see the pods are running on another node .

6) k get nodes                                           <-- on the node "STATUS = Ready,SchedulingDisabled" 
                                                            it means we can not schedule any pod on it 


-----------------------------------------------------------------------------------------------------
Q-40) Create a pod that echo's "goggggg" and then exists, The pod should be deleted automatically when it is completed 
     -pod name: tech-pod 
     -image: busybox

1) k run tech-pod --image=busybox -it --rm --restart=Never -- /bin/sh -c 'echo goggggg'   <-- After executing this command in the output it should 
                                                                                            print the message and also a seperate message, 
                                                                                            that pod is deleted            
                                                                                       

-----------------------------------------------------------------------------------------------------
Q-41) Annotate the existing pod "yellow" and use the value "name=yellow-pod" 

Note: we use "Annotations" when we do want the metadata to be identified, like with labels we can identify the pods 
      and rearrange it with a category.


1) k annotate pod yellow name=yellow-pod                        <-- just check in the .yaml formate if it is "Annotated" or not 

2) k describe yellow | grep -i annotations                      <-- "-i" if we do not use "-i" then we will not get answer.

-----------------------------------------------------------------------------------------------------
Q-42) Get list of all pods in all namespaces and write it to the file /doc/podsnamespaces.txt 

1) k get pods --all-namespaces                <-- first check the output 

2) k get pods --all-namespaces   > /doc/podsnamespaces.txt            <-- after that check the file also .



----------------------------------------------------------------------------------------------------
Q-43) Update the password in the existing configmap "config-green" to "NewPass1234" 

1) k describe configmap config-green            <-- first check which password is currenty written in the configmap 

2) k get configmap config-green -oyaml > config-green.yaml    <-- here update the password in the .yaml file 

3) k replace --force -f config-green.yaml 

4) k get pods                                  <-- at the end we must check, if the pods is still running or not,
                                               if the configmap is not their then pod stops running 

5) k describe pod green                       <-- check under "Environment" that config-map is defined or not.

6) k exec -it green -- env                   <-- it should also print "password=NewPass1234"



----------------------------------------------------------------------------------------------------------
Q-44) You just created the pod "blue", but it is not scheduling on the node. Troubleshoot and fix the issue 

1) k describe pod blue               <-- check the "Events" thier it shows some issue with "nodeselector" 

2) k describe node minikube         <-- check the label defined on it  "app=blue"

3) k get pod blue -oyaml > blue-pod.yaml    

spec: 
  nodeSelector:
    app: blue                          <-- make the correction in the name 

4) k replace --force -f blue-pod.yaml 

5) k get pods                                    <-- at the end check if the pod "blue" is running 

6) k describe pod blue                            <-- also check in the ein, if nodeselector is correctly updated 

---------------------------------------------------------------------------------------------------------
Q-45) Create a network policy and allow traffic from the "green" pod to the "finance-service" and the "data-service"
      -policy name:  internal-policy 
      -policy type:  egress 
      -label pod:    role=post 
      -egress allow: finance 
      -finance port: 8080
      -egress allow: data 
      -data port:    5432



1) network-policy.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy 
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: post
  policyTypes:
    - Egress
  egress:
    - to:  
        - podSelector:
            matchLabels:
              name: frnance
    - to:  
        - podSelector:
            matchLabels:
              name: data 

------------------------------------------------------------------------------------------------------------
Q-46) Create a pod and set "SYS_TIME" + sleep 3600 seconds 
       -pod name: grey 
       -image: busybox 

1) k run grey --image=busybox --command sleep 3600 --dry-run=client -oyaml > sys-time-pod.yaml 

2) go to kuberentes documentation search for "sys_time"

3) vi sys-time-pod.yaml 

spec:
  containers:
  - name: grey 
    image: busybox 
    securityContext:
      capabilities:
        add: ["SYS_TIME"]

4) k apply -f sys-time-pod.yaml 

5) k describe pod grey                        <-- just check if commands and SYS_TIME is correctly configured or not 



-----------------------------------------------------------------------------------------------------
1-30--Q-5) Create a pod called admin-pod, image=busybox, Allow the pod to be able to set system_time 
         command sleep 3200

Note: system_time       means  "SYS_TIME"      <--this i must remember 


######################################################################################################
Q-47) Create a clusterrole and a clusterrolebinding which provides get,watch,list access to the pods 
       
       -clusterrole name:       cluster-administrator 
       -cluserrolebinding name: clusterbinding-administrator 
       -serviceaccount:         admin-sa 

1) k create clusterrole cluster-administrator --verb=get,watch,list --resource=pods 

2) k create clusterrolebinding clusterbinding-administrator --clusterrole=cluster-administrator --serviceaccount=admin-sa 

3) k auth can-i list pods --as system:serviceaccount:default:admin-sa         <-- like this we can recheck if it works 


-----------------------------------------------------------------------------------------------------------
Q-48) Get the IP address of the "blue" pod and write it to the file /doc/ip.txt 

1) k get pods -l run=blue -A -o jsonpath='{range.items[*]}{@.status.podIP}{""}{"\n"}{end}'

Note: check how he has done and how we get the output of a pod as a json 

or alternatively, just copy the ip address of the blue pod replicas and paste them to the .txt file 


---------------------------------------------------------------------------------------------------------
Q-49) Find out the version of the cluster and write it to the file  /doc/versioncluster.txt 


Note: he did not show the answer, but told that use the same above command to print the answer on the .txt file 

1) k get --raw "/version" | jq -f '.gitVersion'     <-- just check if this command works 



----------------------------------------------------------------------------------------------------------
Q-50) Change the mountpath of the nginx container in the "online" statefulset to  /usr/share/nginx/updated-html"


1) k get statefulset online                  <-- first check the state for the statefulset if it is in "READY" state 

2) k get statefulset online -oyaml > online-statefulset.yaml 

3) vi online-statefulset.yaml       

spec:
  containers:
  - name: nginx 
    volumeMounts:
      mountPath: /usr/share/nginx/updated-html        <-- change the "mountPath" value under "nginx" container 


4) k replace -force -f online-statefulset.yaml       

5) k get statefulset online            <-- check if it is in ready state

6) k describe statefulset online           <-- check if the mount path is updated 


-----------------------------------------------------------------------------------------------------------
Q-51)  Create a cronjob which prnts the date and "Running" every minute
       -pod name: show-date-job 
       -image:    busybox 

1) vi cronjob.yaml 

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Running               <-- "- date;"   it is a DATE command in linux and it print 
          restartPolicy: OnFailure                 "echo Running"  it prints "Running" every min 


2) k apply -f cronjob.yaml 


--------------------------------------------------------------------------------------------------------------
Q-52) User JSONPATH and get a list of all the pods with name and namespaces, and write to the file /doc/name-namespace.txt 

1) kubectl get pods -A -o jsonpath='{range.items[*]}{.metadata.name}{"\t"}{.metadata.namespace}{"\n"}{end}'    <-- this command he made by himself 

2) kubectl get pods -A -o jsonpath='{range.items[*]}{.metadata.name}{"\t"}{.metadata.namespace}{"\n"}{end}' > /doc/name-namespace.txt 



---------------------------------------------------------------------------------------------------------------
Q-53) Create a networkpolicy and allow traffic from all the pods in the "dev-tech" namespace  and from pods with 
      the label "type=review" to the pods matching the label "app=postgres" 

Note: to understand the question, In the question he define "allow traffic from" it means we have to put this policy on the 
      target port so every thing that comes to target port shoul be filtered.

1) k label namespace/dev-tech app=dev-tech              <-- first label the namespace 

       
1) vi network-policy.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: postgres 
  policyTypes:
    - Ingress 
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              app: dev-tech 
        - podSelector:
            matchLabels:
              type: review

Note: if in the question they do not define any port, then we have also not to define a port in the networkpolicy 


---------------------------------------------------------------------------------------------------------------
Q-54) Create a pod with container port  80. It should check the pod running at endpoint /healthz on port 80 + verify + delete the pod 
      -pod name: health-pod 
      -Image: nginx 

> k run health-pod --image=nginx --port=80 --restart=Always --dry-run=client -oyaml > liveness-pod.yaml 

1) vi liveness-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: health-pod 
spec:
  containers:
  - name: nginx 
    image: nginx
    livenessProbe:                           <-- From the kubernetes-documentation page i have taken this "livenessProbe" option 
      httpGet:
        path: /healthz
        port: 80
      initialDelaySeconds: 300              
  restartPolicy: Always        

3) k apply -f liveness-pod.yaml 

4) k descirbe pod health-pod               <-- check if the pod works fine 

5) k delete pod health-pod                  <-- As asked in the question at the end delete the pod .


-------------------------------------------------------------------------------------------------------------
Q-55) Monoitor the logs of the pod "yellow", extract all the log lines matching with "not found" and write to the 
      file /doc/failed.log 

1) k get logs yellow | grep "not found" 

2) k get logs yellow | grep "not found" > /doc/failed.log   



--------------------------------------------------------------------------------------------------------------7
Q-56) Rollback the deployment "blue-deplyoment" to revision 1 

1) k rollout history deployment blue-deploy                      <-- here it will show some revision 1,2 or maybe 3 

2) k rollout history deployment blue-deploy --revision=1         <-- it will rollback to the "revision 1"


------------------------------------------------------------------------------------------------------------------
Q-57)  List the orange pod with the custom columns POD_STATIS and POD_NAME and write to the file  /doc/status.txt 

1) k get pod orange -o=custom-columns="POD_NAME:.metadata.name,POD_STATUS:.status.phase"      <-- this command is made by him  


--------------------------------------------------------------------------------------------------------------
Q-58) For the orange pod , set the CPU memory requests and limits 
      Reqiests: CPU=20m, memory=40Mi
      Limits: CPU=160m, memory=200Mi

Note: first check it may be a case that the requests and limits are already their and we need just to update them,
        In this case we assume that these are their 

1) k get pod  orange -oyaml > orange-pod.yaml 

2) vi orange-pod.yaml                             <-- just change the resources as given in the question 

3) k replace --force -f orange-pod.yaml 


------------------------------------------------------------------------------------------------------------
Q-59) Create a pod with a non-persistent storage 
      -pod name: redis-pod 
      -Image: redis

Note: i have created this pod from kubernetes document search "emptyDir"

apiVersion: v1           
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}


------------------------------------------------------------------------------------------------------------
Q-60) Troubleshoot the failed pod "dev-blue" and make it running again 


Note: In this case the image name was wrong, So first he creates a .yaml file from pod then use "k replace -f " to create new 



--------------------------------------------------------------------------------------------------------------
Q-61) Expose the yellow-tech pod internally and create a test-pod for look-up 
      -port: 80
      -service name: yellow-tech-service 
      -test pod name: test-yellow-tech 
      -Type: ClusterIP 


1) k expose pod yellow-tech-service --name=yellow-tech-service --port=80 --target-port=80 --type=ClusterIP 

2) k run test-yellow-tech --image=busybox:1.28  --it --restart=Never -- nslookup yellow-tech-service


----------------------------------------------------------------------------------------------------------
Q-62) Create a DaemonSet 
       -name:  blue-daemon 
       -image: httpd:alpine  

> k get nodes                <-- check how many different nodes are running, at the end on ever Node one "DaemonSet" should run 

1) k create deployment blue-daemon --image=httpd:alpine --dry-run=client -oyaml > blue-daemon.yaml 

2) vi blue-daemon.yaml 

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: blue-daemon 
  labels:
    app: blue-daemon 
spec:
  selector:
    matchLabels:
      app: blue-daemon 
  template:
    metadata:
      labels:
        app: blue-daemon 
    spec:
      containers:
      - name: blue-daemon 
        image: httpd:alpine 

4) k apply -f blue-daemon.yaml 

5) k get pods  -A             <-- it will return all the DaemonSets, check if all the DaemonSets are running on different Nodes 


-------------------------------------------------------------------------------------------------------------
Q-63) There'S an issue with the node "my-node". The admin is not able to schedule any pods on the node.
      Fix the issue and deploy pod-1.yaml on the node. 


1) k get node my-node -owide             <-- As per in this question the STATUS= Ready,SchedulingDisabled
                                            As it is "SchedulingDisabled" it means we can not Schedule any pod on it 


2) k describe node my-node              <-- In the Events we can check, their should also be the same message showing 

3) k uncordone my-node                  

4) k get node my-node -owide             <-- now STATUS=Ready state 

5) k apply -f pod-1.yaml             <-- First chceck the pod-1.yaml file if the "nodeName" is correct, after that execute the "apply" command 



---------------------------------------------------------------------------------------------------------------
Q-64) Get all the objects in all the namespaces and write to file /doc/all.txt 

1) k get all --all-namespaces > /doc/all.txt 




--------------------------------------------------------------------------------------------------------------
Q-65) Create a pod and assign it to the node my-node 
      -pod name: dev-grey
      -image:    nginx 

1) first check if the node with name "my-node" exists 

2) k run dev-grey --image=nginx --dry-run=client -oyaml > dev-grey.yaml 

3) vi dev-grey.yaml 

spec:
  nodeName: my-node 

4) k replace --force -f dev-grey.yaml 


-------------------------------------------------------------------------------------------------------------
Q-66) Create all the pods with the label "env=tech-dev" and write to the file  /doc/label.txt 

1) k get pods -l env=tech-dev                           <-- first check the output of this command 

2) k get pods -l env=tech-dev  > /doc/label.txt            




-------------------------------------------------------------------------------------------------------------
Q-67) Create a taint on the node "my-node" with the key "spray" value of "red" and effect of "NoSchedule" 

Note: first check if the node "my-node" is in READY state and also their is no "SchedulingDisabled" option is written in STATUS 

1) k taint node my-node spray=red:NoSchedule 

2) k describe node my-node | grep Taints          <-- like this we can see if Taint was successfull or not 


----------------------------------------------------------------------------------------------------------
Q-68) Create a pod and set tolerations 
      -pod name: dev-green 
      -Image: nginx 
      -Tolerations: spray=red:NoSchedule 

1) k describe node  my-node | grep Taints            <-- like this we can check if the Taints on the Node matches the given Tolerations 

2) k run dev-green --image=nginx --dry-run=client -oyaml > dev-green.yaml 

3) vi dev-green.yaml 

spec:
  tolerations:
  - effect: NoSchedule 
    key: spray 
    opersator: Equal
    value: red 
  

4) k apply -f dev-green.yaml 

5) k describe pod dev-green | grep Node     <-- like this we can check on which node this pod is placed 


--------------------------------------------------------------------------------------------------------------
Q-69) Check the image version of the "grey" pod without using the describe command and write to file /doc/grey-image.txt 

1) k get pod grey -o jsonpath='{.spec.containers[*].image}{"/n"}'          <-- i am not sure ".containers[*]  <-- "*" star should be their or not 

2) k get pod grey -o jsonpath='{.spec.containers[*].image}{"/n"}'  > /doc/grey-image.txt 



----------------------------------------------------------------------------------------------------------
Q-70) Create a pod with a sidecar container for logging 
       -pod name: pod-logging
       -image: busy-box 

Note: first go to kubernetes-documentation search for  "sidecar logging", then it opens a page under "Logging Architecture"
      just copy the pod manifest file, and delete the last container from it change the name of the pod 


1) vi logging-pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: pod-logging 
spec:
  containers:
  - name: count
    image: busybox:1.28
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done      
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-1
    image: busybox:1.28
    args: [/bin/sh, -c, 'tail -n+1 -F /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}

2) k apply -f logging-pod.yaml 



--------------------------------------------------------------------------------------------------------
Q-71) Find out where the kubernetes master and KubeDNS are running at and write to file  /doc/info.txt 
      
Note: "Kubernetes master" means  master node 
      "KubeDNS"  or they can also say "CoreDNS" 

1) k cluster-info                       <-- see the output of this commands 

2) k cluster-info > /doc/info.txt 



------------------------------------------------------------------------------------------------------
Q-72) Print the pod names and start times to the file /doc/start.txt 

1) k get pods -o jsonpath='{range.items[*]}{.metadata.name}{"\t"}{.status.startTime}{"\n"}{end}'  





----------------------------------------------------------------------------------------------------
Q-73) Create a pod and run the command which shows "Welcome from tech" and sleep for 100 seconds 
      -pod name: tech-blue
      -image: busybox 


1) k run tech-blue --image=busybox  > tech-blue-pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox
    command:
    - "/bin/sh"
    - "-c"
    - "echo 'This is my best pod' && sleep 1000"
    image: busybox 


--------------------------------------------------------------------------------------------------
Q-74) Create the pod "green" and specify a CPU request of "1" and CPU limit of "2"
      -image: nginx 


Note: First i go to kubernetes-documentation and search for  "pod cpu"

apiVersion: v1
kind: Pod
metadata:
  name: green 
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        cpu: "2"
      requests:
        cpu: "1"


-----------------------------------------------------------------------------------------------
Q-75) Scale the "blue" deployment to 5 replicas 

1) k scale deployment blue --replicas=5 



--------------------------------------------------------------------------------------------
Q-76) List al the secrets and configmaps in the cluster in all namespaces and write to file /doc/config-secret.txt 


1) k get configmaps,secrets --all-namespaces       <-- check th eoutput 

2) k get configmaps,secrets /doc/config-secret.txt 



--------------------------------------------------------------------------------------------
Q-77) Create a NetworkPolicy which denies all the ingress traffic 


Note: In the kubernetes documentation i searched for "default deny ingress" then in page i search for "default deny ingress"
      their we can also check "default allow all ingress", and both the same rules for "Egress" traffic 


1) vi network-policy.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress


-----------------------------------------------------------------------------------------------
Q-78) Create an init-container in a pod which creates the file "check.txt" in the "tech-dir" directory.
      Use the main container to check if the "check.txt" file exists and execute the "sleep 300" command when it exists 
      -pod name: blue-check 
      -image initcontainer: busybox:1.28 
      -image container: alpine 



apiVersion: v1
kind: Pod
metadata:
  name: green 
spec:
  volumes:
  - name: blue-volume
    emptyDir: {}
  initContainers:
  - name: init-busybox 
    image: busybox:1.28
    command: ['sh', '-c', 'mkdir /tech-dir; echo > /tech-dir/check.txt']
    volumeMounts:
    - name: blue-vol 
      mountPath: /tech-dir 
  containers:
  - name: alpine 
    image: alpine 
    command: ['sh','-c','if [ -f /tech-dir/check.txt ]; then sleep 300; fi']


------------------------------------------------------------------------------------------------------
Q-79) List the logs of the pod names "dev-green" and search for the pattern "start" and write it to the 
     file /doc/start.txt 



1) k get pods     <-- check the name of the pod, it may be it not directly "dev-green" it can be a part of deployment 

2) k logs dev-green-erere-eeer | grep start            <-- check if logs are showing 

3) k logs dev-green-erere-eeer | grep start > /doc/start.txt 



-------------------------------------------------------------------------------------------------------
Q-80) Expose the deployment  "blue-deploy" 
       -name service: blue-service 
       -port: 6379 
       -type: NodePort 

1) k get deployment blue-deploy       <-- first check if deployment is running 

2) k expose deployment blue-deploy --name blue-service --type=NodePort --port=6379 --target-port=6379

3) k describe service blue-service 


-----------------------------------------------------------------------------------------------------
Q-81) Create two pods with different labels 
      -pod 1 name: pod-1
      -image: nginx
      -label: env=green 

      -pod 2 name: pod-2
      -image: nginx
      -label: env=red 

1) k run pod-1 --image=nginx -l env=green 

2) k run pod-2 --image=nginx -l env=red 

3) k get pod -l env=green 

3) k get pod -l env=red 


-------------------------------------------------------------------------------------------------
Q-82) Create a new clusterrole named "green-clusterrole" which allows you to create deployments 
      After create a new servivceaccount named "green-sa" in the "tech" namespace.
      And finally, bind the clusterrole to the serviceaccount by creating a rolebinding named "green-rb"


1) k create clusterrole green-clusterrole --verb=create --resource=deployment 

2) k create serviceaccount green-sa --namespace=tech 

3) k create rolebinding green-rb --clusterrole=green-clusterrole --serviceaccount=default:green-sa --namesapce=tech 


-------------------------------------------------------------------------------------------------
Q-83) Find the static pod path and copy the location to /doc/staticpath.txt 

1) k get nodes                  <-- first check all the node 

2) minikube ssh                 <-- as "minikube" is node their so we need to first "ssh" in it 

3) sudo -i                      <-- change the login as "root"

4) ps -aux | grep kubelet         <-- with this we can find the location of the config file "--config=/var/lib/kubelet/config.yaml" 

5) cat /var/lib/kubelet/config.yaml | grep staticPodPath 

Output:
staticPodPath: /etc/kubernetes/manifests              <-- we copy this path with mouse right-click 

6) logout                                         <-- we logout from the "minikube" node 

7) echo /etc/kubernetes/manifests > /doc/staticpodpath.txt 



-------------------------------------------------------------------------------------------------
Q-84) Delete a pod "white-shark" without any delay 

1) k get pods                 <-- check if "white-shark" pod is running 

2) k delete pod white-shark --grace-period=0 --force 


----------------------------------------------------------------------------------------------
Q-85) Grep the current context and write it to the file /doc/current.txt 

1) k config current-context                    <-- first check the current context 

2) cat ~/.kube/config                       <-- it will print the config file 

3) cat ~/.kube/config | grep current               <-- check if it prints the current context same as the output in command 1

4) 3) cat ~/.kube/config | grep current > /doc/current.txt 



---------------------------------------------------------------------------------------------
Q-86) Get a list of all the pods which were recently deleted 
      Write the list to the file /doc/recentdelete.txt 


> k get events -o jsonpath='{range.items[?(@.reason=="Killing")]}{.involvedObject.name}{"\n"}{end}'




--------------------------------------------------------------------------------------------
Q-87) There is something wrong with the "dark-blue" pod. Troubleshoot and fix the issue 

1) k describe pod dark-blue 

-) in this question their is a type in the "Args"     "d0ne"  it should be "done"


--------------------------------------------------------------------------------------------
Q-88) Create a pod "yellow" with the image "redis:alpine" and a storage which lasts as long as the lifetime of the pod 
      
Note: in the indirect way they are asking to create a "non-persistent" storage with "emptyDir"      


1) vi non-persist.yaml
 
apiVersion: v1           
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - image: redis:alpine 
    name: redis 
    volumeMounts:
    - mountPath: /data/redis                           <-- normally we can use any path, but "/data/redis/" is used by him in video 
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}


-----------------------------------------------------------------------------------------
Q-89) Create a pod and add "runAsUser:2000" and "fsGroup:5000"
       -Pod name: sec-pod 
       -image:    nginx 


1) k run sec-pod --image=nginx --dry-run=client -oyaml > sec-pod.yaml 

2) vi sec-pod.yaml 


apiVersion: v1           
kind: Pod
metadata:
  name: sec-pod 
spec:
  securityContext:
    runAsUser: 2000
    fsGroup: 5000
  containers:
  - image: nginx 
    name: nginx 


--------------------------------------------------------------------------------------------
Q-90) There's an issue with the kubeconfig file located in the folder ~/.kube/config 
      Troubleshoot and fix the issue 

clusters:
  - cluster:
      server: https://192.168.49.2:8443                  <-- in the video here was "9000" written, change it to 8443
                                                       Note: in some case we use 6443 and in some 8443 so just check with both,      




----------------------------------------------------------------------------------------------------
1-30--Q-6) A kubeconfig file called test.kubeconfig has been created in /root/TEST, Their is something wrong with the configuration 

> kubectl config view                    <-- with this we can view the current config 
                                            After comparing with the "/root/TEST/config" file the server:https://controlplane:8000
                                            But normally the port is  6443   "server:https://controlplane:6443"







#####################################################################################################
Q-91) Create a pod named "sec-blue" with the image "nginx" and set "NET_ADMIN"

Note: i just go to kubernetes documentation and search for  "net_admin"

apiVersion: v1
kind: Pod
metadata:
  name: sec-blue 
spec:
  containers:
  - name: nginx
    image: nginx 
    securityContext:
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]



-------------------------------------------------------------------------------------------
Q-92) Delete all the pods with the label "environment: orange" 

1) k get pod -l environment=orange 

2) k delete pod -l environment=orange 

2) k get pod -l environment=orange                     <-- check if the pods are still their 



---------------------------------------------------------------------------------------------
Q-93) Create a multipod with three containers:
      -Name container 1: container-1
      -image:            nginx 

      -Name container 2: container-2
      -image:            redis 

      -Name container 3: container-3
      -image:            alpine 



apiVersion: v1
kind: Pod
metadata:
  name: multi-pod 
spec:
  containers:
  - name: container-1
    image: nginx
  - name: container-2
    image: redis 
  - name: container-3
    image: alpine 

----------------------------------------------------------------------------------------------
Q-94) Replace the "grey" pod with the existing yaml file "pod-replace.yaml" and verify after 

Note: Their is a existing pod name "grey", and their is pod-replace.yaml  file and it has also a pod with same name "grey"

1) k get pod grey -oyaml                      <-- in this case existing pod has a image: nginx:1.17

1) k replace --force -f pod-replace.yaml 

2) k get pod grey -oyaml                      <-- after replacing it has a new image: nginx:1.18



---------------------------------------------------------------------------------------------
Q-95) Change the requested storage size of the PersistentVolumeClaim "storage-pvc" to 800Mi

1) k get pvc storage-pvc            <-- their is already a pvc with the current size of "2Gi"

2) k get pvc storage-pvc > pvc.yaml 

3) vi pvc.yaml     

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 800Mi                   <-- Here change the storage from "2Gi" to "800Mi"


4) k replace --force -f pvc.yaml           <-- like this we can replace the older with the current.

---------------------------------------------------------------------------------------------
Q-96) Edit and existing pod "green-nginx" and add the command "sleep 3600"

1) k get pod green-nginx 

2) k get pod green-nginx > green-nginx.yaml 

3) vi green-nginx.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: sec-blue 
spec:
  containers:
  - name: nginx
    image: nginx 
    command:
    - "sleep"
    - "3600" 

4) k replace --force -f green-nginx.yaml 


----------------------------------------------------------------------------------------------
Q-97) Add a readiness probe to the existing deployment "ready-deployment"
       -path: /ready
       -port: 80

Note: Readiness probe checks that, the this pod is ready to recieve traffic.

1) k get deploy ready-deployment -oyaml > ready-deployment.yaml 

2) vi ready-deployment.yaml 


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:                <-- "httpGet" <-- this example i did not saw in the kubernetes-documentation so i have to remember it 
            path: /ready
            port: 80
          successThreshold: 3  

3) k replace --force -f ready-deployment.yaml          <-- try and check this if it works 

--------------------------------------------------------------------------------------------
Q-98) Get all contexts and write it to the file /doc/contexts.txt 

Note: With "contexts" he means "contexts" in the  "~/.kube/config" file 

1) k config get-contexts           <-- see the output 

2) k config get-contexts > /doc/contexts.txt 


-----------------------------------------------------------------------------------------
Q-99) Create a replicaset "front-replicaset" with the image "nginx" which has 3 replicas 

Note: As with imperative commands we can not create directly replicaset, so in this case 
      First we will create a Deployment, After that we need to change the "kind"

1) k create deployment front-replicaset --image=nginx --dry-run=client -oyaml > replicaset.yaml 

2) vi replicaset.yaml 

apiVersion: apps/v1
kind: ReplicaSet                       <-- change "Deployment" to "ReplicaSet"
metadata:
  name: front-replicaset 
  labels:
    app: front-replicaset 
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front-replicaset
  strategy: {}                              <-- we have to remove this field "strategy"     
  template:
    metadata:
      labels:
        app: front-replicaset 
    spec:
      containers:
      - name: nginx
        image: nginx


3) k apply -f replicaset.yaml 

4) k delete pod <take-name-of-pod>           <-- just delete one pod and it should create new pod again as part of ReplicaSet 

--------------------------------------------------------------------------------------------
Q-100) List all the control plane components and write to the file /doc/controlcomp.txt 

1) k get pods -n kube-system                      <-- see the output 

2) k get pods -n kube-system > /doc/controlcomp.txt 





